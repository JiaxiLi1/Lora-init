#!/usr/bin/env python3
"""
æµ‹è¯•dx_direct_sparseå‚æ•°çš„å®ç°
"""

import torch
import torch.nn as nn
import sys
import os

# Add the parent directory to the path
sys.path.append(os.path.dirname(os.path.dirname(__file__)))

from peft_pretraining.modeling_llama import ActivationSparse2to4Function
from transformers import LlamaConfig

def test_dx_direct_sparse():
    """æµ‹è¯•dx_direct_sparseå‚æ•°çš„åŠŸèƒ½"""
    
    print("=" * 60)
    print("æµ‹è¯•dx_direct_sparseå‚æ•°åŠŸèƒ½")
    print("=" * 60)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # æµ‹è¯•å‚æ•°
    batch_size, seq_len, hidden_size = 2, 16, 128
    intermediate_size = 256
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    input_data = torch.randn(batch_size, seq_len, hidden_size, device=device, requires_grad=True)
    weight1 = torch.randn(hidden_size, intermediate_size, device=device, requires_grad=True)
    weight2 = torch.randn(intermediate_size, hidden_size, device=device, requires_grad=True)
    
    print(f"è¾“å…¥æ•°æ®å½¢çŠ¶: {input_data.shape}")
    print(f"æƒé‡1å½¢çŠ¶: {weight1.shape}")
    print(f"æƒé‡2å½¢çŠ¶: {weight2.shape}")
    
    # æµ‹è¯•ä¸¤ç§dxè®¡ç®—æ–¹å¼
    methods = [
        (False, "Split-GEMM strategy"),
        (True, "Direct naive sparse")
    ]
    
    results = {}
    
    for dx_direct_sparse, method_name in methods:
        print(f"\næµ‹è¯• {method_name} (dx_direct_sparse={dx_direct_sparse}):")
        
        # é‡ç½®æ¢¯åº¦
        if input_data.grad is not None:
            input_data.grad.zero_()
        if weight1.grad is not None:
            weight1.grad.zero_()
        if weight2.grad is not None:
            weight2.grad.zero_()
        
        # è®¾ç½®è®­ç»ƒæ­¥æ•°ä¸ºéwarmupçŠ¶æ€
        ActivationSparse2to4Function._training_step = 2000
        
        # Forward pass
        output = ActivationSparse2to4Function.apply(
            input_data,
            weight1,
            weight2,
            None,  # bias1
            None,  # bias2
            "naive",  # sparsity_method
            1000,  # warmup_steps
            dx_direct_sparse  # dx_direct_sparse
        )
        
        # Backward pass
        loss = output.sum()
        loss.backward()
        
        # æ£€æŸ¥ç»“æœ
        has_input_grad = input_data.grad is not None
        has_weight1_grad = weight1.grad is not None
        has_weight2_grad = weight2.grad is not None
        
        print(f"  è¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"  è¾“å…¥æ¢¯åº¦å­˜åœ¨: {has_input_grad}")
        print(f"  æƒé‡1æ¢¯åº¦å­˜åœ¨: {has_weight1_grad}")
        print(f"  æƒé‡2æ¢¯åº¦å­˜åœ¨: {has_weight2_grad}")
        
        if has_input_grad:
            input_grad_norm = torch.norm(input_data.grad).item()
            print(f"  è¾“å…¥æ¢¯åº¦èŒƒæ•°: {input_grad_norm:.6f}")
        
        if has_weight1_grad:
            weight1_grad_norm = torch.norm(weight1.grad).item()
            print(f"  æƒé‡1æ¢¯åº¦èŒƒæ•°: {weight1_grad_norm:.6f}")
        
        if has_weight2_grad:
            weight2_grad_norm = torch.norm(weight2.grad).item()
            print(f"  æƒé‡2æ¢¯åº¦èŒƒæ•°: {weight2_grad_norm:.6f}")
        
        # å­˜å‚¨ç»“æœç”¨äºæ¯”è¾ƒ
        results[method_name] = {
            'output': output.detach().clone(),
            'input_grad': input_data.grad.detach().clone() if has_input_grad else None,
            'weight1_grad': weight1.grad.detach().clone() if has_weight1_grad else None,
            'weight2_grad': weight2.grad.detach().clone() if has_weight2_grad else None,
        }
        
        # æ£€æŸ¥æ•°å€¼ç¨³å®šæ€§
        has_nan = torch.isnan(output).any()
        has_inf = torch.isinf(output).any()
        print(f"  æ•°å€¼ç¨³å®šæ€§: NaN={has_nan}, Inf={has_inf}")
        
        if has_nan or has_inf:
            print(f"  âŒ æ•°å€¼ä¸ç¨³å®š!")
        else:
            print(f"  âœ… æ•°å€¼ç¨³å®š")
    
    # æ¯”è¾ƒä¸¤ç§æ–¹æ³•çš„ç»“æœ
    print(f"\n" + "=" * 60)
    print("æ¯”è¾ƒä¸¤ç§æ–¹æ³•çš„ç»“æœ")
    print("=" * 60)
    
    split_gemm_result = results["Split-GEMM strategy"]
    direct_sparse_result = results["Direct naive sparse"]
    
    # æ¯”è¾ƒè¾“å‡º
    output_diff = torch.norm(split_gemm_result['output'] - direct_sparse_result['output']).item()
    print(f"è¾“å‡ºå·®å¼‚ (L2 norm): {output_diff:.6f}")
    
    # æ¯”è¾ƒæ¢¯åº¦
    if split_gemm_result['input_grad'] is not None and direct_sparse_result['input_grad'] is not None:
        input_grad_diff = torch.norm(split_gemm_result['input_grad'] - direct_sparse_result['input_grad']).item()
        print(f"è¾“å…¥æ¢¯åº¦å·®å¼‚ (L2 norm): {input_grad_diff:.6f}")
    
    if split_gemm_result['weight1_grad'] is not None and direct_sparse_result['weight1_grad'] is not None:
        weight1_grad_diff = torch.norm(split_gemm_result['weight1_grad'] - direct_sparse_result['weight1_grad']).item()
        print(f"æƒé‡1æ¢¯åº¦å·®å¼‚ (L2 norm): {weight1_grad_diff:.6f}")
    
    if split_gemm_result['weight2_grad'] is not None and direct_sparse_result['weight2_grad'] is not None:
        weight2_grad_diff = torch.norm(split_gemm_result['weight2_grad'] - direct_sparse_result['weight2_grad']).item()
        print(f"æƒé‡2æ¢¯åº¦å·®å¼‚ (L2 norm): {weight2_grad_diff:.6f}")
    
    # åˆ†æå·®å¼‚
    print(f"\nåˆ†æ:")
    if output_diff < 1e-6:
        print("âœ… ä¸¤ç§æ–¹æ³•çš„è¾“å‡ºåŸºæœ¬ç›¸åŒ")
    else:
        print("âš ï¸  ä¸¤ç§æ–¹æ³•çš„è¾“å‡ºæœ‰å·®å¼‚ (è¿™æ˜¯é¢„æœŸçš„ï¼Œå› ä¸ºdxè®¡ç®—æ–¹å¼ä¸åŒ)")
    
    print("ğŸ“ è¿™äº›å·®å¼‚æ˜¯é¢„æœŸçš„ï¼Œå› ä¸º:")
    print("   - Split-GEMM strategy: ä½¿ç”¨feature-wise 2:4ç¨€ç–åŒ–çš„dy1")
    print("   - Direct naive sparse: ä½¿ç”¨token-wise 2:4ç¨€ç–åŒ–çš„dy1")

def test_warmup_mode():
    """æµ‹è¯•warmupæ¨¡å¼ä¸‹çš„è¡Œä¸º"""
    
    print(f"\n" + "=" * 60)
    print("æµ‹è¯•Warmupæ¨¡å¼")
    print("=" * 60)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # æµ‹è¯•å‚æ•°
    batch_size, seq_len, hidden_size = 2, 16, 128
    intermediate_size = 256
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    input_data = torch.randn(batch_size, seq_len, hidden_size, device=device, requires_grad=True)
    weight1 = torch.randn(hidden_size, intermediate_size, device=device, requires_grad=True)
    weight2 = torch.randn(intermediate_size, hidden_size, device=device, requires_grad=True)
    
    # è®¾ç½®è®­ç»ƒæ­¥æ•°ä¸ºwarmupçŠ¶æ€
    ActivationSparse2to4Function._training_step = 500  # < 1000
    
    # æµ‹è¯•ä¸¤ç§dx_direct_sparseè®¾ç½®åœ¨warmupæ¨¡å¼ä¸‹çš„è¡Œä¸º
    for dx_direct_sparse in [False, True]:
        print(f"\nWarmupæ¨¡å¼ (dx_direct_sparse={dx_direct_sparse}):")
        
        # é‡ç½®æ¢¯åº¦
        if input_data.grad is not None:
            input_data.grad.zero_()
        if weight1.grad is not None:
            weight1.grad.zero_()
        if weight2.grad is not None:
            weight2.grad.zero_()
        
        # Forward pass
        output = ActivationSparse2to4Function.apply(
            input_data,
            weight1,
            weight2,
            None,  # bias1
            None,  # bias2
            "naive",  # sparsity_method
            1000,  # warmup_steps
            dx_direct_sparse  # dx_direct_sparse
        )
        
        # Backward pass
        loss = output.sum()
        loss.backward()
        
        # æ£€æŸ¥ç»“æœ
        print(f"  è¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"  è¾“å…¥æ¢¯åº¦èŒƒæ•°: {torch.norm(input_data.grad).item():.6f}")
        print(f"  æƒé‡1æ¢¯åº¦èŒƒæ•°: {torch.norm(weight1.grad).item():.6f}")
        print(f"  æƒé‡2æ¢¯åº¦èŒƒæ•°: {torch.norm(weight2.grad).item():.6f}")
        
        # æ£€æŸ¥æ•°å€¼ç¨³å®šæ€§
        has_nan = torch.isnan(output).any()
        has_inf = torch.isinf(output).any()
        print(f"  æ•°å€¼ç¨³å®šæ€§: NaN={has_nan}, Inf={has_inf}")
        
        if has_nan or has_inf:
            print(f"  âŒ æ•°å€¼ä¸ç¨³å®š!")
        else:
            print(f"  âœ… æ•°å€¼ç¨³å®š")
    
    print(f"\nğŸ“ åœ¨warmupæ¨¡å¼ä¸‹ï¼Œdx_direct_sparseå‚æ•°ä¸å½±å“è®¡ç®—ï¼Œå› ä¸ºä½¿ç”¨çš„æ˜¯æ ‡å‡†denseæ¢¯åº¦è®¡ç®—")

if __name__ == "__main__":
    torch.manual_seed(42)
    
    test_dx_direct_sparse()
    test_warmup_mode()
    
    print(f"\n" + "=" * 60)
    print("æ€»ç»“:")
    print("âœ… dx_direct_sparseå‚æ•°å·²æ­£ç¡®å®ç°")
    print("âœ… ä¸¤ç§dxè®¡ç®—æ–¹å¼éƒ½èƒ½æ­£å¸¸å·¥ä½œ")
    print("âœ… åœ¨warmupæ¨¡å¼ä¸‹å‚æ•°ä¸å½±å“è®¡ç®—")
    print("âœ… å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•° --dx_direct_sparse True/False æ¥æ§åˆ¶")
    print("=" * 60) 