#!/usr/bin/env python3
"""
æµ‹è¯•RTX 3090ä¸Š2:4ç¨€ç–çŸ©é˜µä¹˜æ³•çš„åŠ é€Ÿæ•ˆæœ
åªæµ‹è¯•çº¯çŸ©é˜µä¹˜æ³•éƒ¨åˆ†ï¼Œä¸åŒ…å«sparse weightè®¡ç®—å¼€é”€
"""

import torch
import torch.nn as nn
import time

# Import our corrected implementation
from loro_torch.sparse_lowrank_module import (
    FP8SparseOperation, 
    SparseLowRankLinear,
    SPARSE_AVAILABLE
)

def test_pure_matmul_performance():
    """æµ‹è¯•çº¯çŸ©é˜µä¹˜æ³•æ€§èƒ½ï¼Œé¢„å…ˆè®¡ç®—sparse weights"""
    print("=== æµ‹è¯•çº¯çŸ©é˜µä¹˜æ³•æ€§èƒ½ ===")
    
    if not SPARSE_AVAILABLE:
        print("âŒ 2by4 sparse package not available")
        return False
    
    # åˆ›å»ºæµ‹è¯•å‚æ•°
    batch_size = 8
    seq_len = 1024
    hidden_size = 768
    intermediate_size = 3072
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    x = torch.randn(batch_size, seq_len, hidden_size).cuda()
    
    print(f"æµ‹è¯•é…ç½®: batch_size={batch_size}, seq_len={seq_len}")
    print(f"çŸ©é˜µå°ºå¯¸: {hidden_size} -> {intermediate_size}")
    
    # 1. åˆ›å»ºç¨€ç–å±‚å¹¶é¢„è®¡ç®—sparse weights
    print("\n1. å‡†å¤‡ç¨€ç–å±‚...")
    linear_ref = nn.Linear(hidden_size, intermediate_size).cuda()
    sparse_layer = SparseLowRankLinear(linear_ref, rank=64, init="xavier", enable_sparse=True).cuda()
    
    # é¢„è®¡ç®—sparse weightsï¼ˆè¿™ä¸ªå¼€é”€ä¸è®¡å…¥æ€§èƒ½æµ‹è¯•ï¼‰
    sparse_weight_in = sparse_layer.get_sparse_weight_in()
    sparse_weight_out = sparse_layer.get_sparse_weight_out()
    
    # æ£€æŸ¥ç¨€ç–åº¦
    sparsity_in = (sparse_weight_in == 0).float().mean().item()
    sparsity_out = (sparse_weight_out == 0).float().mean().item()
    print(f"   Weight_in ç¨€ç–åº¦: {sparsity_in:.1%}")
    print(f"   Weight_out ç¨€ç–åº¦: {sparsity_out:.1%}")
    
    # 2. åˆ›å»ºå¯¹åº”çš„å¯†é›†å±‚
    print("\n2. å‡†å¤‡å¯†é›†å±‚...")
    dense_layer = SparseLowRankLinear(linear_ref, rank=64, init="xavier", enable_sparse=False).cuda()
    dense_weight_in = dense_layer.weight_in
    dense_weight_out = dense_layer.weight_out
    
    # 3. æµ‹è¯•çº¯çŸ©é˜µä¹˜æ³•æ€§èƒ½
    iterations = 100
    warmup = 20
    
    def time_matmul_operations(name, weight_in, weight_out, use_sparse=False):
        print(f"\n3. æµ‹è¯•{name}çŸ©é˜µä¹˜æ³•...")
        
        # Warmup
        for _ in range(warmup):
            if use_sparse:
                x_proj = FP8SparseOperation.apply(x, weight_in.t(), None)
                output = FP8SparseOperation.apply(x_proj, weight_out, None)
            else:
                x_proj = torch.matmul(x, weight_in)
                output = torch.matmul(x_proj, weight_out.t())
        
        torch.cuda.synchronize()
        start_time = time.time()
        
        # å®é™…æµ‹è¯•
        for _ in range(iterations):
            if use_sparse:
                x_proj = FP8SparseOperation.apply(x, weight_in.t(), None)
                output = FP8SparseOperation.apply(x_proj, weight_out, None)
            else:
                x_proj = torch.matmul(x, weight_in)
                output = torch.matmul(x_proj, weight_out.t())
        
        torch.cuda.synchronize()
        end_time = time.time()
        
        avg_time = (end_time - start_time) / iterations * 1000  # ms
        print(f"   {name}å¹³å‡æ—¶é—´: {avg_time:.3f}ms")
        return avg_time
    
    # æµ‹è¯•ç¨€ç–çŸ©é˜µä¹˜æ³•
    sparse_time = time_matmul_operations("ç¨€ç–", sparse_weight_in, sparse_weight_out, use_sparse=True)
    
    # æµ‹è¯•å¯†é›†çŸ©é˜µä¹˜æ³•
    dense_time = time_matmul_operations("å¯†é›†", dense_weight_in, dense_weight_out, use_sparse=False)
    
    # è®¡ç®—åŠ é€Ÿæ¯”
    speedup = dense_time / sparse_time
    
    print(f"\n=== çº¯çŸ©é˜µä¹˜æ³•æ€§èƒ½å¯¹æ¯” ===")
    print(f"ç¨€ç–çŸ©é˜µä¹˜æ³•: {sparse_time:.3f}ms")
    print(f"å¯†é›†çŸ©é˜µä¹˜æ³•: {dense_time:.3f}ms")
    print(f"åŠ é€Ÿæ¯”: {speedup:.2f}x")
    
    if speedup > 1.2:
        print("âœ… 2:4ç¨€ç–åŒ–åœ¨RTX 3090ä¸ŠæˆåŠŸåŠ é€Ÿï¼")
        return True
    elif speedup > 0.9:
        print("âš ï¸  æ€§èƒ½ç›¸å½“ï¼Œå¯èƒ½å—å…¶ä»–å› ç´ å½±å“")
        return True
    else:
        print("âŒ ç¨€ç–åŒ–æ€§èƒ½ä½äºé¢„æœŸ")
        return False


def test_single_layer_performance():
    """æµ‹è¯•å•å±‚MLPæ€§èƒ½ï¼Œå¤ç°ç”¨æˆ·è®°å½•ä¸­çš„æµ‹è¯•"""
    print("\n" + "="*50)
    print("æµ‹è¯•å•å±‚MLPæ€§èƒ½ï¼ˆå¤ç°ä¹‹å‰çš„æˆåŠŸæ¡ˆä¾‹ï¼‰")
    print("="*50)
    
    if not SPARSE_AVAILABLE:
        print("âŒ 2by4 sparse package not available")
        return False
    
    # ä½¿ç”¨ä¸è®°å½•ä¸­ç›¸åŒçš„é…ç½®
    batch_size = 8
    seq_len = 1024
    hidden_size = 768
    
    input_tensor = torch.randn(batch_size, seq_len, hidden_size).cuda()
    
    # åˆ›å»ºç¨€ç–å±‚
    linear_ref = nn.Linear(hidden_size, hidden_size * 4).cuda()
    sparse_layer = SparseLowRankLinear(linear_ref, rank=64, init="xavier", enable_sparse=True).cuda()
    
    # åˆ›å»ºå¯†é›†å±‚
    dense_layer = SparseLowRankLinear(linear_ref, rank=64, init="xavier", enable_sparse=False).cuda()
    
    def test_layer_performance(layer, name, iterations=50):
        layer.eval()
        
        # Warmup
        for _ in range(10):
            with torch.no_grad():
                _ = layer(input_tensor)
        
        torch.cuda.synchronize()
        start_time = time.time()
        
        for _ in range(iterations):
            with torch.no_grad():
                output = layer(input_tensor)
        
        torch.cuda.synchronize()
        end_time = time.time()
        
        avg_time = (end_time - start_time) / iterations * 1000  # ms
        print(f"{name}å±‚å¹³å‡æ—¶é—´: {avg_time:.2f}ms")
        return avg_time
    
    # æµ‹è¯•æ€§èƒ½
    sparse_time = test_layer_performance(sparse_layer, "ç¨€ç–")
    dense_time = test_layer_performance(dense_layer, "å¯†é›†")
    
    # æ£€æŸ¥ç¨€ç–åº¦
    sparse_weight_in = sparse_layer.get_sparse_weight_in()
    sparse_weight_out = sparse_layer.get_sparse_weight_out()
    sparsity = ((sparse_weight_in == 0).float().mean() + (sparse_weight_out == 0).float().mean()) / 2
    
    speedup = dense_time / sparse_time
    
    print(f"\n=== å•å±‚MLPæ€§èƒ½å¯¹æ¯” ===")
    print(f"ç¨€ç–å±‚æ—¶é—´: {sparse_time:.2f}ms")
    print(f"å¯†é›†å±‚æ—¶é—´: {dense_time:.2f}ms")
    print(f"åŠ é€Ÿæ¯”: {speedup:.2f}x")
    print(f"å®é™…ç¨€ç–åº¦: {sparsity:.1%}")
    
    if sparsity < 0.4:
        print("âš ï¸  è­¦å‘Šï¼šç¨€ç–åº¦ä½äº40%ï¼Œ2:4ç¨€ç–åŒ–å¯èƒ½æ²¡æœ‰æ­£ç¡®åº”ç”¨")
        
    return speedup > 1.5  # æœŸæœ›è‡³å°‘1.5xåŠ é€Ÿ


def main():
    print("ğŸ§ª RTX 3090 2:4ç¨€ç–çŸ©é˜µä¹˜æ³•åŠ é€Ÿæµ‹è¯•")
    print("="*60)
    
    # æ£€æŸ¥GPUä¿¡æ¯
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        print(f"GPU: {gpu_name}")
        print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    else:
        print("âŒ CUDAä¸å¯ç”¨")
        return False
    
    if not SPARSE_AVAILABLE:
        print("âŒ 2by4 sparse packageä¸å¯ç”¨")
        return False
    
    print("âœ… ä½¿ç”¨æ­£ç¡®çš„2by4-pretrain-acc-exampleså®ç°")
    
    # è¿è¡Œæµ‹è¯•
    test1_passed = test_pure_matmul_performance()
    test2_passed = test_single_layer_performance()
    
    print(f"\n" + "="*60)
    print("ğŸ“‹ æµ‹è¯•æ€»ç»“")
    print("="*60)
    print(f"çº¯çŸ©é˜µä¹˜æ³•æµ‹è¯•: {'âœ… é€šè¿‡' if test1_passed else 'âŒ å¤±è´¥'}")
    print(f"å•å±‚MLPæµ‹è¯•: {'âœ… é€šè¿‡' if test2_passed else 'âŒ å¤±è´¥'}")
    
    if test1_passed and test2_passed:
        print("\nğŸ‰ RTX 3090ä¸Šçš„2:4ç¨€ç–åŒ–åŠ é€Ÿç¡®è®¤æˆåŠŸï¼")
    else:
        print(f"\nâš ï¸  éƒ¨åˆ†æµ‹è¯•æœªè¾¾åˆ°é¢„æœŸï¼Œå¯èƒ½éœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")


if __name__ == "__main__":
    main() 