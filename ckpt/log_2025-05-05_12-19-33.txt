

Experiment = loro_adamw_loro_freq_500_r_attn256_mlp256_all_init_lrk_xavier_rs_-1_rfsh_all_500_lr_0.01_gc0.0_cosine_restart_cyc500_wp5_adj-6000_2000_0.1


2025-05-05 12:19:33.614 | INFO     | __main__:main:304 - Global rank 0, local rank 0, device: 0
2025-05-05 12:19:33.615 | INFO     | __main__:main:310 - Process group initialized
Rank 0 using device cuda:0


# CUDA visible devices: 1


2025-05-05 12:19:33.615 | INFO     | __main__:main:341 - Using dist with rank 0 (only rank 0 will log)
2025-05-05 12:19:33.615 | INFO     | __main__:main:342 - ****************************************
2025-05-05 12:19:33.615 | INFO     | __main__:main:343 - Starting training with the arguments
2025-05-05 12:19:33.615 | INFO     | __main__:main:345 - c4_local                       False
2025-05-05 12:19:33.615 | INFO     | __main__:main:345 - train_data_path                en/c4-train.*.json.gz
2025-05-05 12:19:33.615 | INFO     | __main__:main:345 - val_data_path                  en/c4-validation.*.json.gz
2025-05-05 12:19:33.615 | INFO     | __main__:main:345 - model_config                   configs/llama_60m.json
2025-05-05 12:19:33.615 | INFO     | __main__:main:345 - use_hf_model                   False
2025-05-05 12:19:33.615 | INFO     | __main__:main:345 - continue_from                  None
2025-05-05 12:19:33.615 | INFO     | __main__:main:345 - batch_size                     16
2025-05-05 12:19:33.615 | INFO     | __main__:main:345 - gradient_accumulation          2
2025-05-05 12:19:33.616 | INFO     | __main__:main:345 - total_batch_size               32
2025-05-05 12:19:33.616 | INFO     | __main__:main:345 - max_length                     256
2025-05-05 12:19:33.616 | INFO     | __main__:main:345 - optimizer                      loro_adamw
2025-05-05 12:19:33.616 | INFO     | __main__:main:345 - lr                             0.01
2025-05-05 12:19:33.616 | INFO     | __main__:main:345 - scheduler                      cosine_restart
2025-05-05 12:19:33.616 | INFO     | __main__:main:345 - cosine_restart_freq            500
2025-05-05 12:19:33.616 | INFO     | __main__:main:345 - cosine_restart_warmup          5
2025-05-05 12:19:33.616 | INFO     | __main__:main:345 - lr_jag_after_warmup            False
2025-05-05 12:19:33.616 | INFO     | __main__:main:345 - lr_adjust_steps                -6000
2025-05-05 12:19:33.616 | INFO     | __main__:main:345 - min_lr_ratio                   0.1
2025-05-05 12:19:33.616 | INFO     | __main__:main:345 - activation_checkpointing       False
2025-05-05 12:19:33.616 | INFO     | __main__:main:345 - weight_decay                   0.0
2025-05-05 12:19:33.616 | INFO     | __main__:main:345 - warmup_steps                   2000
2025-05-05 12:19:33.616 | INFO     | __main__:main:345 - eval_every                     300
2025-05-05 12:19:33.616 | INFO     | __main__:main:345 - num_training_steps             6
2025-05-05 12:19:33.616 | INFO     | __main__:main:345 - max_train_tokens               None
2025-05-05 12:19:33.617 | INFO     | __main__:main:345 - save_every                     10000
2025-05-05 12:19:33.617 | INFO     | __main__:main:345 - save_ckpt                      False
2025-05-05 12:19:33.617 | INFO     | __main__:main:345 - save_dir                       /home/rtx3090/code_jiaxi/LORO-main/ckpt
2025-05-05 12:19:33.617 | INFO     | __main__:main:345 - tags                           None
2025-05-05 12:19:33.617 | INFO     | __main__:main:345 - dtype                          bfloat16
2025-05-05 12:19:33.617 | INFO     | __main__:main:345 - workers                        8
2025-05-05 12:19:33.617 | INFO     | __main__:main:345 - seed                           0
2025-05-05 12:19:33.617 | INFO     | __main__:main:345 - name                           test
2025-05-05 12:19:33.617 | INFO     | __main__:main:345 - grad_clipping                  0.0
2025-05-05 12:19:33.617 | INFO     | __main__:main:345 - beta1                          0.0
2025-05-05 12:19:33.617 | INFO     | __main__:main:345 - rank                           128
2025-05-05 12:19:33.617 | INFO     | __main__:main:345 - update_proj_gap                50
2025-05-05 12:19:33.617 | INFO     | __main__:main:345 - galore_scale                   1.0
2025-05-05 12:19:33.617 | INFO     | __main__:main:345 - proj_type                      std
2025-05-05 12:19:33.618 | INFO     | __main__:main:345 - loro_type                      loro
2025-05-05 12:19:33.618 | INFO     | __main__:main:345 - loro_freq                      500
2025-05-05 12:19:33.618 | INFO     | __main__:main:345 - loro_refresh                   all
2025-05-05 12:19:33.618 | INFO     | __main__:main:345 - loro_refresh_freq              500
2025-05-05 12:19:33.618 | INFO     | __main__:main:345 - loro_attn_rank                 256
2025-05-05 12:19:33.618 | INFO     | __main__:main:345 - loro_mlp_rank                  256
2025-05-05 12:19:33.618 | INFO     | __main__:main:345 - loro_init                      xavier
2025-05-05 12:19:33.618 | INFO     | __main__:main:345 - loro_scope                     all
2025-05-05 12:19:33.618 | INFO     | __main__:main:345 - loro_lr_scaler                 -1
2025-05-05 12:19:33.618 | INFO     | __main__:main:345 - loro_mlp_dense                 False
2025-05-05 12:19:33.618 | INFO     | __main__:main:345 - single_gpu                     False
2025-05-05 12:19:33.618 | INFO     | __main__:main:345 - desc                           loro_adamw_loro_freq_500_r_attn256_mlp256_all_init_lrk_xavier_rs_-1_rfsh_all_500_lr_0.01_gc0.0_cosine_restart_cyc500_wp5_adj-6000_2000_0.1
2025-05-05 12:19:33.618 | INFO     | __main__:main:345 - timestamp                      2025-05-05_12-19-33
2025-05-05 12:19:33.618 | INFO     | __main__:main:345 - log_path                       /home/rtx3090/code_jiaxi/LORO-main/ckpt/log_2025-05-05_12-19-33.txt
2025-05-05 12:19:33.618 | INFO     | __main__:main:345 - num_cuda                       1
2025-05-05 12:19:33.618 | INFO     | __main__:main:346 - ****************************************
2025-05-05 12:19:40.557 | INFO     | __main__:main:362 - Shuffling data with seed 42
layer.0.self_attn.q_proj: Linear(in_features=512, out_features=512, bias=False) --> LowRankLinear(in_dim=512, out_dim=512, rank=256, init=xavier)
layer.0.self_attn.k_proj: Linear(in_features=512, out_features=512, bias=False) --> LowRankLinear(in_dim=512, out_dim=512, rank=256, init=xavier)
layer.0.self_attn.v_proj: Linear(in_features=512, out_features=512, bias=False) --> LowRankLinear(in_dim=512, out_dim=512, rank=256, init=xavier)
layer.0.self_attn.o_proj: Linear(in_features=512, out_features=512, bias=False) --> LowRankLinear(in_dim=512, out_dim=512, rank=256, init=xavier)
layer.0.mlp.gate_proj: Linear(in_features=512, out_features=1376, bias=False) --> LowRankLinear(in_dim=512, out_dim=1376, rank=256, init=xavier)
layer.0.mlp.down_proj: Linear(in_features=1376, out_features=512, bias=False) --> LowRankLinear(in_dim=1376, out_dim=512, rank=256, init=xavier)
layer.0.mlp.up_proj: Linear(in_features=512, out_features=1376, bias=False) --> LowRankLinear(in_dim=512, out_dim=1376, rank=256, init=xavier)
layer.1.self_attn.q_proj: Linear(in_features=512, out_features=512, bias=False) --> LowRankLinear(in_dim=512, out_dim=512, rank=256, init=xavier)
layer.1.self_attn.k_proj: Linear(in_features=512, out_features=512, bias=False) --> LowRankLinear(in_dim=512, out_dim=512, rank=256, init=xavier)
layer.1.self_attn.v_proj: Linear(in_features=512, out_features=512, bias=False) --> LowRankLinear(in_dim=512, out_dim=512, rank=256, init=xavier)
layer.1.self_attn.o_proj: Linear(in_features=512, out_features=512, bias=False) --> LowRankLinear(in_dim=512, out_dim=512, rank=256, init=xavier)
layer.1.mlp.gate_proj: Linear(in_features=512, out_features=1376, bias=False) --> LowRankLinear(in_dim=512, out_dim=1376, rank=256, init=xavier)
layer.1.mlp.down_proj: Linear(in_features=1376, out_features=512, bias=False) --> LowRankLinear(in_dim=1376, out_dim=512, rank=256, init=xavier)
layer.1.mlp.up_proj: Linear(in_features=512, out_features=1376, bias=False) --> LowRankLinear(in_dim=512, out_dim=1376, rank=256, init=xavier)
layer.2.self_attn.q_proj: Linear(in_features=512, out_features=512, bias=False) --> LowRankLinear(in_dim=512, out_dim=512, rank=256, init=xavier)
layer.2.self_attn.k_proj: Linear(in_features=512, out_features=512, bias=False) --> LowRankLinear(in_dim=512, out_dim=512, rank=256, init=xavier)
layer.2.self_attn.v_proj: Linear(in_features=512, out_features=512, bias=False) --> LowRankLinear(in_dim=512, out_dim=512, rank=256, init=xavier)
layer.2.self_attn.o_proj: Linear(in_features=512, out_features=512, bias=False) --> LowRankLinear(in_dim=512, out_dim=512, rank=256, init=xavier)
layer.2.mlp.gate_proj: Linear(in_features=512, out_features=1376, bias=False) --> LowRankLinear(in_dim=512, out_dim=1376, rank=256, init=xavier)
layer.2.mlp.down_proj: Linear(in_features=1376, out_features=512, bias=False) --> LowRankLinear(in_dim=1376, out_dim=512, rank=256, init=xavier)
layer.2.mlp.up_proj: Linear(in_features=512, out_features=1376, bias=False) --> LowRankLinear(in_dim=512, out_dim=1376, rank=256, init=xavier)
layer.3.self_attn.q_proj: Linear(in_features=512, out_features=512, bias=False) --> LowRankLinear(in_dim=512, out_dim=512, rank=256, init=xavier)
layer.3.self_attn.k_proj: Linear(in_features=512, out_features=512, bias=False) --> LowRankLinear(in_dim=512, out_dim=512, rank=256, init=xavier)
layer.3.self_attn.v_proj: Linear(in_features=512, out_features=512, bias=False) --> LowRankLinear(in_dim=512, out_dim=512, rank=256, init=xavier)
layer.3.self_attn.o_proj: Linear(in_features=512, out_features=512, bias=False) --> LowRankLinear(in_dim=512, out_dim=512, rank=256, init=xavier)
layer.3.mlp.gate_proj: Linear(in_features=512, out_features=1376, bias=False) --> LowRankLinear(in_dim=512, out_dim=1376, rank=256, init=xavier)
layer.3.mlp.down_proj: Linear(in_features=1376, out_features=512, bias=False) --> LowRankLinear(in_dim=1376, out_dim=512, rank=256, init=xavier)
layer.3.mlp.up_proj: Linear(in_features=512, out_features=1376, bias=False) --> LowRankLinear(in_dim=512, out_dim=1376, rank=256, init=xavier)
layer.4.self_attn.q_proj: Linear(in_features=512, out_features=512, bias=False) --> LowRankLinear(in_dim=512, out_dim=512, rank=256, init=xavier)
layer.4.self_attn.k_proj: Linear(in_features=512, out_features=512, bias=False) --> LowRankLinear(in_dim=512, out_dim=512, rank=256, init=xavier)
layer.4.self_attn.v_proj: Linear(in_features=512, out_features=512, bias=False) --> LowRankLinear(in_dim=512, out_dim=512, rank=256, init=xavier)
layer.4.self_attn.o_proj: Linear(in_features=512, out_features=512, bias=False) --> LowRankLinear(in_dim=512, out_dim=512, rank=256, init=xavier)
layer.4.mlp.gate_proj: Linear(in_features=512, out_features=1376, bias=False) --> LowRankLinear(in_dim=512, out_dim=1376, rank=256, init=xavier)
layer.4.mlp.down_proj: Linear(in_features=1376, out_features=512, bias=False) --> LowRankLinear(in_dim=1376, out_dim=512, rank=256, init=xavier)
layer.4.mlp.up_proj: Linear(in_features=512, out_features=1376, bias=False) --> LowRankLinear(in_dim=512, out_dim=1376, rank=256, init=xavier)
layer.5.self_attn.q_proj: Linear(in_features=512, out_features=512, bias=False) --> LowRankLinear(in_dim=512, out_dim=512, rank=256, init=xavier)
layer.5.self_attn.k_proj: Linear(in_features=512, out_features=512, bias=False) --> LowRankLinear(in_dim=512, out_dim=512, rank=256, init=xavier)
layer.5.self_attn.v_proj: Linear(in_features=512, out_features=512, bias=False) --> LowRankLinear(in_dim=512, out_dim=512, rank=256, init=xavier)
layer.5.self_attn.o_proj: Linear(in_features=512, out_features=512, bias=False) --> LowRankLinear(in_dim=512, out_dim=512, rank=256, init=xavier)
layer.5.mlp.gate_proj: Linear(in_features=512, out_features=1376, bias=False) --> LowRankLinear(in_dim=512, out_dim=1376, rank=256, init=xavier)
layer.5.mlp.down_proj: Linear(in_features=1376, out_features=512, bias=False) --> LowRankLinear(in_dim=1376, out_dim=512, rank=256, init=xavier)
layer.5.mlp.up_proj: Linear(in_features=512, out_features=1376, bias=False) --> LowRankLinear(in_dim=512, out_dim=1376, rank=256, init=xavier)
layer.6.self_attn.q_proj: Linear(in_features=512, out_features=512, bias=False) --> LowRankLinear(in_dim=512, out_dim=512, rank=256, init=xavier)
layer.6.self_attn.k_proj: Linear(in_features=512, out_features=512, bias=False) --> LowRankLinear(in_dim=512, out_dim=512, rank=256, init=xavier)
layer.6.self_attn.v_proj: Linear(in_features=512, out_features=512, bias=False) --> LowRankLinear(in_dim=512, out_dim=512, rank=256, init=xavier)
layer.6.self_attn.o_proj: Linear(in_features=512, out_features=512, bias=False) --> LowRankLinear(in_dim=512, out_dim=512, rank=256, init=xavier)
layer.6.mlp.gate_proj: Linear(in_features=512, out_features=1376, bias=False) --> LowRankLinear(in_dim=512, out_dim=1376, rank=256, init=xavier)
layer.6.mlp.down_proj: Linear(in_features=1376, out_features=512, bias=False) --> LowRankLinear(in_dim=1376, out_dim=512, rank=256, init=xavier)
layer.6.mlp.up_proj: Linear(in_features=512, out_features=1376, bias=False) --> LowRankLinear(in_dim=512, out_dim=1376, rank=256, init=xavier)
layer.7.self_attn.q_proj: Linear(in_features=512, out_features=512, bias=False) --> LowRankLinear(in_dim=512, out_dim=512, rank=256, init=xavier)
layer.7.self_attn.k_proj: Linear(in_features=512, out_features=512, bias=False) --> LowRankLinear(in_dim=512, out_dim=512, rank=256, init=xavier)
layer.7.self_attn.v_proj: Linear(in_features=512, out_features=512, bias=False) --> LowRankLinear(in_dim=512, out_dim=512, rank=256, init=xavier)
layer.7.self_attn.o_proj: Linear(in_features=512, out_features=512, bias=False) --> LowRankLinear(in_dim=512, out_dim=512, rank=256, init=xavier)
layer.7.mlp.gate_proj: Linear(in_features=512, out_features=1376, bias=False) --> LowRankLinear(in_dim=512, out_dim=1376, rank=256, init=xavier)
layer.7.mlp.down_proj: Linear(in_features=1376, out_features=512, bias=False) --> LowRankLinear(in_dim=1376, out_dim=512, rank=256, init=xavier)
layer.7.mlp.up_proj: Linear(in_features=512, out_features=1376, bias=False) --> LowRankLinear(in_dim=512, out_dim=1376, rank=256, init=xavier)

LowRankLinear modules are set successfully!
Self-attention rank: 256, MLP rank: 256.
Full param: 58.0736 M = 0.0580736 G
Low-rank param: 52.765184 M = 0.052765184 G
Cprs rate: 90.85916%

2025-05-05 12:19:46.752 | INFO     | __main__:main:652 - 
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 512, padding_idx=31999)
    (layers): ModuleList(
      (0-7): 8 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): LowRankLinear(in_dim=512, out_dim=512, rank=256, init=xavier)
          (k_proj): LowRankLinear(in_dim=512, out_dim=512, rank=256, init=xavier)
          (v_proj): LowRankLinear(in_dim=512, out_dim=512, rank=256, init=xavier)
          (o_proj): LowRankLinear(in_dim=512, out_dim=512, rank=256, init=xavier)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): LowRankLinear(in_dim=512, out_dim=1376, rank=256, init=xavier)
          (down_proj): LowRankLinear(in_dim=1376, out_dim=512, rank=256, init=xavier)
          (up_proj): LowRankLinear(in_dim=512, out_dim=1376, rank=256, init=xavier)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=512, out_features=32000, bias=False)
)

2025-05-05 12:19:46.753 | INFO     | __main__:main:653 - Total params: 52.77M
2025-05-05 12:19:46.753 | INFO     | __main__:main:656 - Trainable params: 52.77M
2025-05-05 12:19:46.753 | INFO     | __main__:main:659 - Saving model to /home/rtx3090/code_jiaxi/LORO-main/ckpt every 10000 update steps
2025-05-05 12:19:56.032 | INFO     | __main__:main:753 - Reached max number of update steps (f6). Stopping training.
Rank 0 stopping training.
2025-05-05 12:19:56.084 | INFO     | __main__:main:957 - Training finished
2025-05-05 12:19:56.085 | INFO     | __main__:main:1000 - Running final evaluation
2025-05-05 12:19:57.422 | INFO     | __main__:evaluate_model:240 - Loaded validation dataset in 1.21 seconds
2025-05-05 12:19:57.425 | INFO     | __main__:evaluate_model:260 - Eval set prepared in 1.21 seconds
