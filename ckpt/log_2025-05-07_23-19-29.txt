

Experiment = loro_adamw_loro_freq_5_r_attn256_mlp256_all_init_lrk_xavier_rs_-1_rfsh_all_5_lr_0.01_gc0.0_cosine_restart_cyc500_wp5_adj-6000_2000_0.1


2025-05-07 23:19:29.955 | INFO     | __main__:main:304 - Global rank 0, local rank 0, device: 0
2025-05-07 23:19:29.956 | INFO     | __main__:main:310 - Process group initialized
Rank 0 using device cuda:0


# CUDA visible devices: 1


2025-05-07 23:19:29.956 | INFO     | __main__:main:341 - Using dist with rank 0 (only rank 0 will log)
2025-05-07 23:19:29.956 | INFO     | __main__:main:342 - ****************************************
2025-05-07 23:19:29.956 | INFO     | __main__:main:343 - Starting training with the arguments
2025-05-07 23:19:29.956 | INFO     | __main__:main:345 - c4_local                       False
2025-05-07 23:19:29.956 | INFO     | __main__:main:345 - train_data_path                en/c4-train.*.json.gz
2025-05-07 23:19:29.956 | INFO     | __main__:main:345 - val_data_path                  en/c4-validation.*.json.gz
2025-05-07 23:19:29.956 | INFO     | __main__:main:345 - model_config                   configs/llama_350m.json
2025-05-07 23:19:29.956 | INFO     | __main__:main:345 - use_hf_model                   False
2025-05-07 23:19:29.956 | INFO     | __main__:main:345 - continue_from                  None
2025-05-07 23:19:29.957 | INFO     | __main__:main:345 - batch_size                     16
2025-05-07 23:19:29.957 | INFO     | __main__:main:345 - gradient_accumulation          32
2025-05-07 23:19:29.957 | INFO     | __main__:main:345 - total_batch_size               512
2025-05-07 23:19:29.957 | INFO     | __main__:main:345 - max_length                     256
2025-05-07 23:19:29.957 | INFO     | __main__:main:345 - optimizer                      loro_adamw
2025-05-07 23:19:29.957 | INFO     | __main__:main:345 - lr                             0.01
2025-05-07 23:19:29.957 | INFO     | __main__:main:345 - scheduler                      cosine_restart
2025-05-07 23:19:29.957 | INFO     | __main__:main:345 - cosine_restart_freq            500
2025-05-07 23:19:29.957 | INFO     | __main__:main:345 - cosine_restart_warmup          5
2025-05-07 23:19:29.957 | INFO     | __main__:main:345 - lr_jag_after_warmup            False
2025-05-07 23:19:29.957 | INFO     | __main__:main:345 - lr_adjust_steps                -6000
2025-05-07 23:19:29.957 | INFO     | __main__:main:345 - min_lr_ratio                   0.1
2025-05-07 23:19:29.957 | INFO     | __main__:main:345 - activation_checkpointing       False
2025-05-07 23:19:29.957 | INFO     | __main__:main:345 - weight_decay                   0.0
2025-05-07 23:19:29.957 | INFO     | __main__:main:345 - warmup_steps                   2000
2025-05-07 23:19:29.957 | INFO     | __main__:main:345 - eval_every                     3000
2025-05-07 23:19:29.958 | INFO     | __main__:main:345 - num_training_steps             10000
2025-05-07 23:19:29.958 | INFO     | __main__:main:345 - max_train_tokens               None
2025-05-07 23:19:29.958 | INFO     | __main__:main:345 - save_every                     10000
2025-05-07 23:19:29.958 | INFO     | __main__:main:345 - save_ckpt                      False
2025-05-07 23:19:29.958 | INFO     | __main__:main:345 - save_dir                       /home/rtx3090/code_jiaxi/LORO-main/ckpt
2025-05-07 23:19:29.958 | INFO     | __main__:main:345 - tags                           None
2025-05-07 23:19:29.958 | INFO     | __main__:main:345 - dtype                          bfloat16
2025-05-07 23:19:29.958 | INFO     | __main__:main:345 - workers                        8
2025-05-07 23:19:29.958 | INFO     | __main__:main:345 - seed                           0
2025-05-07 23:19:29.958 | INFO     | __main__:main:345 - name                           test
2025-05-07 23:19:29.958 | INFO     | __main__:main:345 - grad_clipping                  0.0
2025-05-07 23:19:29.958 | INFO     | __main__:main:345 - beta1                          0.0
2025-05-07 23:19:29.958 | INFO     | __main__:main:345 - rank                           128
2025-05-07 23:19:29.958 | INFO     | __main__:main:345 - update_proj_gap                50
2025-05-07 23:19:29.958 | INFO     | __main__:main:345 - galore_scale                   1.0
2025-05-07 23:19:29.958 | INFO     | __main__:main:345 - proj_type                      std
2025-05-07 23:19:29.959 | INFO     | __main__:main:345 - loro_type                      loro
2025-05-07 23:19:29.959 | INFO     | __main__:main:345 - loro_freq                      5
2025-05-07 23:19:29.959 | INFO     | __main__:main:345 - loro_refresh                   all
2025-05-07 23:19:29.959 | INFO     | __main__:main:345 - loro_refresh_freq              5
2025-05-07 23:19:29.959 | INFO     | __main__:main:345 - loro_attn_rank                 256
2025-05-07 23:19:29.959 | INFO     | __main__:main:345 - loro_mlp_rank                  256
2025-05-07 23:19:29.959 | INFO     | __main__:main:345 - loro_init                      xavier
2025-05-07 23:19:29.959 | INFO     | __main__:main:345 - loro_scope                     all
2025-05-07 23:19:29.959 | INFO     | __main__:main:345 - loro_lr_scaler                 -1
2025-05-07 23:19:29.959 | INFO     | __main__:main:345 - loro_mlp_dense                 False
2025-05-07 23:19:29.959 | INFO     | __main__:main:345 - single_gpu                     False
2025-05-07 23:19:29.959 | INFO     | __main__:main:345 - desc                           loro_adamw_loro_freq_5_r_attn256_mlp256_all_init_lrk_xavier_rs_-1_rfsh_all_5_lr_0.01_gc0.0_cosine_restart_cyc500_wp5_adj-6000_2000_0.1
2025-05-07 23:19:29.959 | INFO     | __main__:main:345 - timestamp                      2025-05-07_23-19-29
2025-05-07 23:19:29.959 | INFO     | __main__:main:345 - log_path                       /home/rtx3090/code_jiaxi/LORO-main/ckpt/log_2025-05-07_23-19-29.txt
2025-05-07 23:19:29.959 | INFO     | __main__:main:345 - num_cuda                       1
2025-05-07 23:19:29.959 | INFO     | __main__:main:346 - ****************************************
2025-05-07 23:19:35.998 | INFO     | __main__:main:362 - Shuffling data with seed 42
layer.0.self_attn.q_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.0.self_attn.k_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.0.self_attn.v_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.0.self_attn.o_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.0.mlp.gate_proj: Linear(in_features=1024, out_features=2736, bias=False) --> LowRankLinear(in_dim=1024, out_dim=2736, rank=256, init=xavier)
layer.0.mlp.down_proj: Linear(in_features=2736, out_features=1024, bias=False) --> LowRankLinear(in_dim=2736, out_dim=1024, rank=256, init=xavier)
layer.0.mlp.up_proj: Linear(in_features=1024, out_features=2736, bias=False) --> LowRankLinear(in_dim=1024, out_dim=2736, rank=256, init=xavier)
layer.1.self_attn.q_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.1.self_attn.k_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.1.self_attn.v_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.1.self_attn.o_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.1.mlp.gate_proj: Linear(in_features=1024, out_features=2736, bias=False) --> LowRankLinear(in_dim=1024, out_dim=2736, rank=256, init=xavier)
layer.1.mlp.down_proj: Linear(in_features=2736, out_features=1024, bias=False) --> LowRankLinear(in_dim=2736, out_dim=1024, rank=256, init=xavier)
layer.1.mlp.up_proj: Linear(in_features=1024, out_features=2736, bias=False) --> LowRankLinear(in_dim=1024, out_dim=2736, rank=256, init=xavier)
layer.2.self_attn.q_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.2.self_attn.k_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.2.self_attn.v_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.2.self_attn.o_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.2.mlp.gate_proj: Linear(in_features=1024, out_features=2736, bias=False) --> LowRankLinear(in_dim=1024, out_dim=2736, rank=256, init=xavier)
layer.2.mlp.down_proj: Linear(in_features=2736, out_features=1024, bias=False) --> LowRankLinear(in_dim=2736, out_dim=1024, rank=256, init=xavier)
layer.2.mlp.up_proj: Linear(in_features=1024, out_features=2736, bias=False) --> LowRankLinear(in_dim=1024, out_dim=2736, rank=256, init=xavier)
layer.3.self_attn.q_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.3.self_attn.k_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.3.self_attn.v_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.3.self_attn.o_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.3.mlp.gate_proj: Linear(in_features=1024, out_features=2736, bias=False) --> LowRankLinear(in_dim=1024, out_dim=2736, rank=256, init=xavier)
layer.3.mlp.down_proj: Linear(in_features=2736, out_features=1024, bias=False) --> LowRankLinear(in_dim=2736, out_dim=1024, rank=256, init=xavier)
layer.3.mlp.up_proj: Linear(in_features=1024, out_features=2736, bias=False) --> LowRankLinear(in_dim=1024, out_dim=2736, rank=256, init=xavier)
layer.4.self_attn.q_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.4.self_attn.k_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.4.self_attn.v_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.4.self_attn.o_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.4.mlp.gate_proj: Linear(in_features=1024, out_features=2736, bias=False) --> LowRankLinear(in_dim=1024, out_dim=2736, rank=256, init=xavier)
layer.4.mlp.down_proj: Linear(in_features=2736, out_features=1024, bias=False) --> LowRankLinear(in_dim=2736, out_dim=1024, rank=256, init=xavier)
layer.4.mlp.up_proj: Linear(in_features=1024, out_features=2736, bias=False) --> LowRankLinear(in_dim=1024, out_dim=2736, rank=256, init=xavier)
layer.5.self_attn.q_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.5.self_attn.k_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.5.self_attn.v_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.5.self_attn.o_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.5.mlp.gate_proj: Linear(in_features=1024, out_features=2736, bias=False) --> LowRankLinear(in_dim=1024, out_dim=2736, rank=256, init=xavier)
layer.5.mlp.down_proj: Linear(in_features=2736, out_features=1024, bias=False) --> LowRankLinear(in_dim=2736, out_dim=1024, rank=256, init=xavier)
layer.5.mlp.up_proj: Linear(in_features=1024, out_features=2736, bias=False) --> LowRankLinear(in_dim=1024, out_dim=2736, rank=256, init=xavier)
layer.6.self_attn.q_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.6.self_attn.k_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.6.self_attn.v_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.6.self_attn.o_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.6.mlp.gate_proj: Linear(in_features=1024, out_features=2736, bias=False) --> LowRankLinear(in_dim=1024, out_dim=2736, rank=256, init=xavier)
layer.6.mlp.down_proj: Linear(in_features=2736, out_features=1024, bias=False) --> LowRankLinear(in_dim=2736, out_dim=1024, rank=256, init=xavier)
layer.6.mlp.up_proj: Linear(in_features=1024, out_features=2736, bias=False) --> LowRankLinear(in_dim=1024, out_dim=2736, rank=256, init=xavier)
layer.7.self_attn.q_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.7.self_attn.k_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.7.self_attn.v_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.7.self_attn.o_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.7.mlp.gate_proj: Linear(in_features=1024, out_features=2736, bias=False) --> LowRankLinear(in_dim=1024, out_dim=2736, rank=256, init=xavier)
layer.7.mlp.down_proj: Linear(in_features=2736, out_features=1024, bias=False) --> LowRankLinear(in_dim=2736, out_dim=1024, rank=256, init=xavier)
layer.7.mlp.up_proj: Linear(in_features=1024, out_features=2736, bias=False) --> LowRankLinear(in_dim=1024, out_dim=2736, rank=256, init=xavier)
layer.8.self_attn.q_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.8.self_attn.k_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.8.self_attn.v_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.8.self_attn.o_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.8.mlp.gate_proj: Linear(in_features=1024, out_features=2736, bias=False) --> LowRankLinear(in_dim=1024, out_dim=2736, rank=256, init=xavier)
layer.8.mlp.down_proj: Linear(in_features=2736, out_features=1024, bias=False) --> LowRankLinear(in_dim=2736, out_dim=1024, rank=256, init=xavier)
layer.8.mlp.up_proj: Linear(in_features=1024, out_features=2736, bias=False) --> LowRankLinear(in_dim=1024, out_dim=2736, rank=256, init=xavier)
layer.9.self_attn.q_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.9.self_attn.k_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.9.self_attn.v_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.9.self_attn.o_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.9.mlp.gate_proj: Linear(in_features=1024, out_features=2736, bias=False) --> LowRankLinear(in_dim=1024, out_dim=2736, rank=256, init=xavier)
layer.9.mlp.down_proj: Linear(in_features=2736, out_features=1024, bias=False) --> LowRankLinear(in_dim=2736, out_dim=1024, rank=256, init=xavier)
layer.9.mlp.up_proj: Linear(in_features=1024, out_features=2736, bias=False) --> LowRankLinear(in_dim=1024, out_dim=2736, rank=256, init=xavier)
layer.10.self_attn.q_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.10.self_attn.k_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.10.self_attn.v_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.10.self_attn.o_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.10.mlp.gate_proj: Linear(in_features=1024, out_features=2736, bias=False) --> LowRankLinear(in_dim=1024, out_dim=2736, rank=256, init=xavier)
layer.10.mlp.down_proj: Linear(in_features=2736, out_features=1024, bias=False) --> LowRankLinear(in_dim=2736, out_dim=1024, rank=256, init=xavier)
layer.10.mlp.up_proj: Linear(in_features=1024, out_features=2736, bias=False) --> LowRankLinear(in_dim=1024, out_dim=2736, rank=256, init=xavier)
layer.11.self_attn.q_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.11.self_attn.k_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.11.self_attn.v_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.11.self_attn.o_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.11.mlp.gate_proj: Linear(in_features=1024, out_features=2736, bias=False) --> LowRankLinear(in_dim=1024, out_dim=2736, rank=256, init=xavier)
layer.11.mlp.down_proj: Linear(in_features=2736, out_features=1024, bias=False) --> LowRankLinear(in_dim=2736, out_dim=1024, rank=256, init=xavier)
layer.11.mlp.up_proj: Linear(in_features=1024, out_features=2736, bias=False) --> LowRankLinear(in_dim=1024, out_dim=2736, rank=256, init=xavier)
layer.12.self_attn.q_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.12.self_attn.k_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.12.self_attn.v_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.12.self_attn.o_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.12.mlp.gate_proj: Linear(in_features=1024, out_features=2736, bias=False) --> LowRankLinear(in_dim=1024, out_dim=2736, rank=256, init=xavier)
layer.12.mlp.down_proj: Linear(in_features=2736, out_features=1024, bias=False) --> LowRankLinear(in_dim=2736, out_dim=1024, rank=256, init=xavier)
layer.12.mlp.up_proj: Linear(in_features=1024, out_features=2736, bias=False) --> LowRankLinear(in_dim=1024, out_dim=2736, rank=256, init=xavier)
layer.13.self_attn.q_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.13.self_attn.k_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.13.self_attn.v_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.13.self_attn.o_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.13.mlp.gate_proj: Linear(in_features=1024, out_features=2736, bias=False) --> LowRankLinear(in_dim=1024, out_dim=2736, rank=256, init=xavier)
layer.13.mlp.down_proj: Linear(in_features=2736, out_features=1024, bias=False) --> LowRankLinear(in_dim=2736, out_dim=1024, rank=256, init=xavier)
layer.13.mlp.up_proj: Linear(in_features=1024, out_features=2736, bias=False) --> LowRankLinear(in_dim=1024, out_dim=2736, rank=256, init=xavier)
layer.14.self_attn.q_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.14.self_attn.k_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.14.self_attn.v_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.14.self_attn.o_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.14.mlp.gate_proj: Linear(in_features=1024, out_features=2736, bias=False) --> LowRankLinear(in_dim=1024, out_dim=2736, rank=256, init=xavier)
layer.14.mlp.down_proj: Linear(in_features=2736, out_features=1024, bias=False) --> LowRankLinear(in_dim=2736, out_dim=1024, rank=256, init=xavier)
layer.14.mlp.up_proj: Linear(in_features=1024, out_features=2736, bias=False) --> LowRankLinear(in_dim=1024, out_dim=2736, rank=256, init=xavier)
layer.15.self_attn.q_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.15.self_attn.k_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.15.self_attn.v_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.15.self_attn.o_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.15.mlp.gate_proj: Linear(in_features=1024, out_features=2736, bias=False) --> LowRankLinear(in_dim=1024, out_dim=2736, rank=256, init=xavier)
layer.15.mlp.down_proj: Linear(in_features=2736, out_features=1024, bias=False) --> LowRankLinear(in_dim=2736, out_dim=1024, rank=256, init=xavier)
layer.15.mlp.up_proj: Linear(in_features=1024, out_features=2736, bias=False) --> LowRankLinear(in_dim=1024, out_dim=2736, rank=256, init=xavier)
layer.16.self_attn.q_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.16.self_attn.k_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.16.self_attn.v_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.16.self_attn.o_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.16.mlp.gate_proj: Linear(in_features=1024, out_features=2736, bias=False) --> LowRankLinear(in_dim=1024, out_dim=2736, rank=256, init=xavier)
layer.16.mlp.down_proj: Linear(in_features=2736, out_features=1024, bias=False) --> LowRankLinear(in_dim=2736, out_dim=1024, rank=256, init=xavier)
layer.16.mlp.up_proj: Linear(in_features=1024, out_features=2736, bias=False) --> LowRankLinear(in_dim=1024, out_dim=2736, rank=256, init=xavier)
layer.17.self_attn.q_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.17.self_attn.k_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.17.self_attn.v_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.17.self_attn.o_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.17.mlp.gate_proj: Linear(in_features=1024, out_features=2736, bias=False) --> LowRankLinear(in_dim=1024, out_dim=2736, rank=256, init=xavier)
layer.17.mlp.down_proj: Linear(in_features=2736, out_features=1024, bias=False) --> LowRankLinear(in_dim=2736, out_dim=1024, rank=256, init=xavier)
layer.17.mlp.up_proj: Linear(in_features=1024, out_features=2736, bias=False) --> LowRankLinear(in_dim=1024, out_dim=2736, rank=256, init=xavier)
layer.18.self_attn.q_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.18.self_attn.k_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.18.self_attn.v_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.18.self_attn.o_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.18.mlp.gate_proj: Linear(in_features=1024, out_features=2736, bias=False) --> LowRankLinear(in_dim=1024, out_dim=2736, rank=256, init=xavier)
layer.18.mlp.down_proj: Linear(in_features=2736, out_features=1024, bias=False) --> LowRankLinear(in_dim=2736, out_dim=1024, rank=256, init=xavier)
layer.18.mlp.up_proj: Linear(in_features=1024, out_features=2736, bias=False) --> LowRankLinear(in_dim=1024, out_dim=2736, rank=256, init=xavier)
layer.19.self_attn.q_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.19.self_attn.k_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.19.self_attn.v_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.19.self_attn.o_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.19.mlp.gate_proj: Linear(in_features=1024, out_features=2736, bias=False) --> LowRankLinear(in_dim=1024, out_dim=2736, rank=256, init=xavier)
layer.19.mlp.down_proj: Linear(in_features=2736, out_features=1024, bias=False) --> LowRankLinear(in_dim=2736, out_dim=1024, rank=256, init=xavier)
layer.19.mlp.up_proj: Linear(in_features=1024, out_features=2736, bias=False) --> LowRankLinear(in_dim=1024, out_dim=2736, rank=256, init=xavier)
layer.20.self_attn.q_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.20.self_attn.k_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.20.self_attn.v_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.20.self_attn.o_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.20.mlp.gate_proj: Linear(in_features=1024, out_features=2736, bias=False) --> LowRankLinear(in_dim=1024, out_dim=2736, rank=256, init=xavier)
layer.20.mlp.down_proj: Linear(in_features=2736, out_features=1024, bias=False) --> LowRankLinear(in_dim=2736, out_dim=1024, rank=256, init=xavier)
layer.20.mlp.up_proj: Linear(in_features=1024, out_features=2736, bias=False) --> LowRankLinear(in_dim=1024, out_dim=2736, rank=256, init=xavier)
layer.21.self_attn.q_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.21.self_attn.k_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.21.self_attn.v_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.21.self_attn.o_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.21.mlp.gate_proj: Linear(in_features=1024, out_features=2736, bias=False) --> LowRankLinear(in_dim=1024, out_dim=2736, rank=256, init=xavier)
layer.21.mlp.down_proj: Linear(in_features=2736, out_features=1024, bias=False) --> LowRankLinear(in_dim=2736, out_dim=1024, rank=256, init=xavier)
layer.21.mlp.up_proj: Linear(in_features=1024, out_features=2736, bias=False) --> LowRankLinear(in_dim=1024, out_dim=2736, rank=256, init=xavier)
layer.22.self_attn.q_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.22.self_attn.k_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.22.self_attn.v_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.22.self_attn.o_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.22.mlp.gate_proj: Linear(in_features=1024, out_features=2736, bias=False) --> LowRankLinear(in_dim=1024, out_dim=2736, rank=256, init=xavier)
layer.22.mlp.down_proj: Linear(in_features=2736, out_features=1024, bias=False) --> LowRankLinear(in_dim=2736, out_dim=1024, rank=256, init=xavier)
layer.22.mlp.up_proj: Linear(in_features=1024, out_features=2736, bias=False) --> LowRankLinear(in_dim=1024, out_dim=2736, rank=256, init=xavier)
layer.23.self_attn.q_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.23.self_attn.k_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.23.self_attn.v_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.23.self_attn.o_proj: Linear(in_features=1024, out_features=1024, bias=False) --> LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
layer.23.mlp.gate_proj: Linear(in_features=1024, out_features=2736, bias=False) --> LowRankLinear(in_dim=1024, out_dim=2736, rank=256, init=xavier)
layer.23.mlp.down_proj: Linear(in_features=2736, out_features=1024, bias=False) --> LowRankLinear(in_dim=2736, out_dim=1024, rank=256, init=xavier)
layer.23.mlp.up_proj: Linear(in_features=1024, out_features=2736, bias=False) --> LowRankLinear(in_dim=1024, out_dim=2736, rank=256, init=xavier)

LowRankLinear modules are set successfully!
Self-attention rank: 256, MLP rank: 256.
Full param: 367.96928 M = 0.36796928 G
Low-rank param: 185.222144 M = 0.185222144 G
Cprs rate: 50.33631%

2025-05-07 23:19:52.846 | INFO     | __main__:main:652 - 
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 1024, padding_idx=31999)
    (layers): ModuleList(
      (0-23): 24 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
          (k_proj): LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
          (v_proj): LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
          (o_proj): LowRankLinear(in_dim=1024, out_dim=1024, rank=256, init=xavier)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): LowRankLinear(in_dim=1024, out_dim=2736, rank=256, init=xavier)
          (down_proj): LowRankLinear(in_dim=2736, out_dim=1024, rank=256, init=xavier)
          (up_proj): LowRankLinear(in_dim=1024, out_dim=2736, rank=256, init=xavier)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=1024, out_features=32000, bias=False)
)

2025-05-07 23:19:52.847 | INFO     | __main__:main:653 - Total params: 185.22M
2025-05-07 23:19:52.847 | INFO     | __main__:main:656 - Trainable params: 185.22M
2025-05-07 23:19:52.847 | INFO     | __main__:main:659 - Saving model to /home/rtx3090/code_jiaxi/LORO-main/ckpt every 10000 update steps
LORO optim states reset successfully.
LORO optim states reset successfully.
LORO optim states reset successfully.
LORO optim states reset successfully.
LORO optim states reset successfully.
