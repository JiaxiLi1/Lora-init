#!/usr/bin/env python3
"""
æµ‹è¯•ä¸åŒçŸ©é˜µå°ºå¯¸çš„2:4åŠ é€Ÿæ•ˆæœ
ä½¿ç”¨ç›¸åŒçš„ç¨€ç–å‡½æ•°è¿›è¡Œå…¬å¹³å¯¹æ¯”
"""

import sys
import os
sys.path.insert(0, '/home/rtx3090/code_jiaxi/2by4-pretrain-acc-examples/v2/nanoGPT')

import torch
import time
from sparse_ops import fp8_linear

def create_24_sparse_weight(shape):
    """åˆ›å»ºçœŸæ­£çš„2:4ç¨€ç–æƒé‡"""
    weight = torch.randn(shape).cuda()
    # æ¯4ä¸ªå…ƒç´ ä¿ç•™æœ€å¤§çš„2ä¸ª
    weight_flat = weight.view(-1, 4)
    _, indices = torch.topk(torch.abs(weight_flat), 2, dim=1)
    mask = torch.zeros_like(weight_flat)
    mask.scatter_(1, indices, 1)
    sparse_weight = weight_flat * mask
    return sparse_weight.view(shape)

def test_matrix_size_performance(in_features, out_features, name):
    """æµ‹è¯•æŒ‡å®šå°ºå¯¸çŸ©é˜µçš„æ€§èƒ½"""
    print(f"\n=== æµ‹è¯•{name} {in_features}Ã—{out_features} ===")
    
    batch_size = 8
    seq_len = 1024
    x = torch.randn(batch_size, seq_len, in_features).cuda()
    
    # åˆ›å»ºç¨€ç–æƒé‡
    sparse_weight = create_24_sparse_weight((out_features, in_features))
    sparsity = (sparse_weight == 0).float().mean().item()
    
    # åˆ›å»ºå¯†é›†æƒé‡
    dense_weight = torch.randn(out_features, in_features).cuda()
    
    iterations = 100
    warmup = 20
    
    # æµ‹è¯•ç¨€ç–çŸ©é˜µä¹˜æ³•
    print("   æµ‹è¯•ç¨€ç–çŸ©é˜µä¹˜æ³•...")
    for _ in range(warmup):
        _ = fp8_linear.apply(x, sparse_weight, None)
    
    torch.cuda.synchronize()
    start_time = time.time()
    for _ in range(iterations):
        _ = fp8_linear.apply(x, sparse_weight, None)
    torch.cuda.synchronize()
    sparse_time = (time.time() - start_time) / iterations * 1000
    
    # æµ‹è¯•å¯†é›†çŸ©é˜µä¹˜æ³•
    print("   æµ‹è¯•å¯†é›†çŸ©é˜µä¹˜æ³•...")
    for _ in range(warmup):
        _ = torch.matmul(x, dense_weight.t())
    
    torch.cuda.synchronize()
    start_time = time.time()
    for _ in range(iterations):
        _ = torch.matmul(x, dense_weight.t())
    torch.cuda.synchronize()
    dense_time = (time.time() - start_time) / iterations * 1000
    
    speedup = dense_time / sparse_time
    
    print(f"   ç¨€ç–çŸ©é˜µ: {sparse_time:.3f}ms")
    print(f"   å¯†é›†çŸ©é˜µ: {dense_time:.3f}ms") 
    print(f"   åŠ é€Ÿæ¯”: {speedup:.2f}x")
    print(f"   ç¨€ç–åº¦: {sparsity:.1%}")
    
    return sparse_time, dense_time, speedup

def test_loro_vs_original_comparison():
    """å¯¹æ¯”LOROçš„ä¸¤æ­¥ä¹˜æ³• vs åŸå§‹çš„å•æ­¥ä¹˜æ³•"""
    print("\n" + "="*60)
    print("LOROåˆ†è§£ vs åŸå§‹çŸ©é˜µ æ€§èƒ½å¯¹æ¯”")
    print("="*60)
    
    batch_size = 8
    seq_len = 1024
    hidden_size = 768
    intermediate_size = 3072
    rank = 64
    
    x = torch.randn(batch_size, seq_len, hidden_size).cuda()
    
    # 1. åŸå§‹å¤§çŸ©é˜µ 768Ã—3072
    print("\n1. åŸå§‹çŸ©é˜µ 768Ã—3072")
    original_sparse_weight = create_24_sparse_weight((intermediate_size, hidden_size))
    original_dense_weight = torch.randn(intermediate_size, hidden_size).cuda()
    
    iterations = 50
    warmup = 10
    
    # åŸå§‹ç¨€ç–
    for _ in range(warmup):
        _ = fp8_linear.apply(x, original_sparse_weight, None)
    torch.cuda.synchronize()
    start = time.time()
    for _ in range(iterations):
        _ = fp8_linear.apply(x, original_sparse_weight, None)
    torch.cuda.synchronize()
    original_sparse_time = (time.time() - start) / iterations * 1000
    
    # åŸå§‹å¯†é›†
    for _ in range(warmup):
        _ = torch.matmul(x, original_dense_weight.t())
    torch.cuda.synchronize()
    start = time.time()
    for _ in range(iterations):
        _ = torch.matmul(x, original_dense_weight.t())
    torch.cuda.synchronize()
    original_dense_time = (time.time() - start) / iterations * 1000
    
    print(f"   åŸå§‹ç¨€ç–: {original_sparse_time:.3f}ms")
    print(f"   åŸå§‹å¯†é›†: {original_dense_time:.3f}ms")
    print(f"   åŸå§‹åŠ é€Ÿæ¯”: {original_dense_time/original_sparse_time:.2f}x")
    
    # 2. LOROåˆ†è§£ï¼š768Ã—64 + 64Ã—3072
    print(f"\n2. LOROåˆ†è§£ 768Ã—{rank} + {rank}Ã—3072")
    
    # åˆ›å»ºLOROæƒé‡
    weight_in_sparse = create_24_sparse_weight((rank, hidden_size))  # 768Ã—64
    weight_out_sparse = create_24_sparse_weight((intermediate_size, rank))  # 64Ã—3072
    
    weight_in_dense = torch.randn(rank, hidden_size).cuda()
    weight_out_dense = torch.randn(intermediate_size, rank).cuda()
    
    # LOROç¨€ç–ä¸¤æ­¥
    for _ in range(warmup):
        x_proj = fp8_linear.apply(x, weight_in_sparse, None)
        _ = fp8_linear.apply(x_proj, weight_out_sparse, None)
    torch.cuda.synchronize()
    start = time.time()
    for _ in range(iterations):
        x_proj = fp8_linear.apply(x, weight_in_sparse, None)
        _ = fp8_linear.apply(x_proj, weight_out_sparse, None)
    torch.cuda.synchronize()
    loro_sparse_time = (time.time() - start) / iterations * 1000
    
    # LOROå¯†é›†ä¸¤æ­¥
    for _ in range(warmup):
        x_proj = torch.matmul(x, weight_in_dense.t())
        _ = torch.matmul(x_proj, weight_out_dense.t())
    torch.cuda.synchronize()
    start = time.time()
    for _ in range(iterations):
        x_proj = torch.matmul(x, weight_in_dense.t())
        _ = torch.matmul(x_proj, weight_out_dense.t())
    torch.cuda.synchronize()
    loro_dense_time = (time.time() - start) / iterations * 1000
    
    print(f"   LOROç¨€ç–: {loro_sparse_time:.3f}ms")
    print(f"   LOROå¯†é›†: {loro_dense_time:.3f}ms")
    print(f"   LOROåŠ é€Ÿæ¯”: {loro_dense_time/loro_sparse_time:.2f}x")
    
    # 3. æ€»ç»“å¯¹æ¯”
    print(f"\n=== æ€»ç»“å¯¹æ¯” ===")
    print(f"åŸå§‹ç¨€ç– vs åŸå§‹å¯†é›†: {original_dense_time/original_sparse_time:.2f}x åŠ é€Ÿ")
    print(f"LOROç¨€ç– vs LOROå¯†é›†: {loro_dense_time/loro_sparse_time:.2f}x åŠ é€Ÿ")
    print(f"LOROç¨€ç– vs åŸå§‹ç¨€ç–: {original_sparse_time/loro_sparse_time:.2f}x ({'å¿«' if original_sparse_time < loro_sparse_time else 'æ…¢'})")
    print(f"LOROå¯†é›† vs åŸå§‹å¯†é›†: {original_dense_time/loro_dense_time:.2f}x ({'å¿«' if original_dense_time < loro_dense_time else 'æ…¢'})")

def main():
    print("ğŸ§ª æµ‹è¯•ä¸åŒçŸ©é˜µå°ºå¯¸çš„2:4åŠ é€Ÿæ•ˆæœ")
    print("ä½¿ç”¨ç›¸åŒçš„fp8_linearå‡½æ•°è¿›è¡Œå…¬å¹³å¯¹æ¯”")
    print("="*60)
    
    # æ£€æŸ¥GPUä¿¡æ¯
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        print(f"GPU: {gpu_name}")
    else:
        print("âŒ CUDAä¸å¯ç”¨")
        return
    
    # æµ‹è¯•ä¸åŒå°ºå¯¸çš„çŸ©é˜µ
    test_results = []
    
    # å°çŸ©é˜µ (LOROçš„ç¬¬ä¸€æ­¥)
    sparse_time, dense_time, speedup = test_matrix_size_performance(768, 64, "å°çŸ©é˜µ")
    test_results.append(("768Ã—64 (LOROç¬¬ä¸€æ­¥)", speedup))
    
    # ä¸­å°çŸ©é˜µ (LOROçš„ç¬¬äºŒæ­¥)  
    sparse_time, dense_time, speedup = test_matrix_size_performance(64, 3072, "ä¸­å°çŸ©é˜µ")
    test_results.append(("64Ã—3072 (LOROç¬¬äºŒæ­¥)", speedup))
    
    # ä¸­ç­‰çŸ©é˜µ
    sparse_time, dense_time, speedup = test_matrix_size_performance(768, 256, "ä¸­ç­‰çŸ©é˜µ")
    test_results.append(("768Ã—256", speedup))
    
    # å¤§çŸ©é˜µ (åŸå§‹)
    sparse_time, dense_time, speedup = test_matrix_size_performance(768, 3072, "å¤§çŸ©é˜µ")
    test_results.append(("768Ã—3072 (åŸå§‹)", speedup))
    
    # å¯¹æ¯”LORO vs åŸå§‹
    test_loro_vs_original_comparison()
    
    # æ±‡æ€»ç»“æœ
    print(f"\n" + "="*60)
    print("ğŸ“‹ ä¸åŒçŸ©é˜µå°ºå¯¸åŠ é€Ÿæ•ˆæœæ±‡æ€»")
    print("="*60)
    for name, speedup in test_results:
        status = "âœ…" if speedup > 1.2 else "âš ï¸ " if speedup > 0.9 else "âŒ"
        print(f"{status} {name:20} {speedup:.2f}x")

if __name__ == "__main__":
    main() 