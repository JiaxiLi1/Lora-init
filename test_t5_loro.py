#!/usr/bin/env python3
"""
ç®€å•æµ‹è¯•è„šæœ¬ï¼ŒéªŒè¯LORO-mainä¸­çš„T5æ”¯æŒ
"""

import torch
from transformers import AutoTokenizer
from transformers.models.t5.configuration_t5 import T5Config
from peft_pretraining.modeling_t5 import T5ForConditionalGeneration
from t5_data_utils import create_t5_denoising_batch

def test_t5_model_loading():
    """æµ‹è¯•T5æ¨¡å‹åŠ è½½"""
    print("ğŸ”§ Testing T5 model loading...")
    
    try:
        # åŠ è½½T5-smallé…ç½®
        config = T5Config.from_pretrained('configs/t5_small_normal.json')
        
        # åˆ›å»ºT5æ¨¡å‹
        model = T5ForConditionalGeneration(config)
        print(f'âœ… T5 model created successfully with {sum(p.numel() for p in model.parameters())/1e6:.1f}M parameters')
        
        return model
    except Exception as e:
        print(f"âŒ T5 model loading failed: {e}")
        return None

def test_t5_data_processing():
    """æµ‹è¯•T5æ•°æ®å¤„ç†"""
    print("ğŸ”§ Testing T5 data processing...")
    
    try:
        tokenizer = AutoTokenizer.from_pretrained('t5-base')
        
        # åˆ›å»ºæ ·æœ¬batch
        sample_text = 'Today is a beautiful day and we should go to the park to play'
        tokenized = tokenizer(sample_text, return_tensors='pt', max_length=64, truncation=True, padding='max_length')
        
        batch = {
            'input_ids': tokenized['input_ids'],
            'attention_mask': tokenized['attention_mask']
        }
        
        # å¤„ç†ä¸ºT5å»å™ªbatch
        t5_batch = create_t5_denoising_batch(batch, tokenizer, max_length=64)
        
        print(f'âœ… T5 data processing successful!')
        print(f'   Original text: {sample_text}')
        print(f'   Encoder input: {tokenizer.decode(t5_batch["input_ids"][0], skip_special_tokens=False)}')
        print(f'   Decoder input: {tokenizer.decode(t5_batch["decoder_input_ids"][0], skip_special_tokens=False)}')
        
        return tokenizer, t5_batch
    except Exception as e:
        print(f"âŒ T5 data processing failed: {e}")
        return None, None

def test_t5_forward_pass():
    """æµ‹è¯•T5å‰å‘ä¼ æ’­"""
    print("ğŸ”§ Testing T5 forward pass...")
    
    model = test_t5_model_loading()
    tokenizer, t5_batch = test_t5_data_processing()
    
    if model is None or tokenizer is None or t5_batch is None:
        print("âŒ Cannot test forward pass due to previous failures")
        return False
    
    try:
        with torch.no_grad():
            output = model(
                input_ids=t5_batch["input_ids"],
                attention_mask=t5_batch["attention_mask"],
                decoder_input_ids=t5_batch["decoder_input_ids"],
                decoder_attention_mask=t5_batch["decoder_attention_mask"],
                labels=t5_batch["labels"]
            )
        
        print(f'âœ… T5 forward pass successful! Loss: {output.loss.item():.4f}')
        return True
    except Exception as e:
        print(f"âŒ T5 forward pass failed: {e}")
        return False

if __name__ == "__main__":
    print("ğŸš€ Starting T5 LORO integration tests...")
    print("=" * 60)
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    success = test_t5_forward_pass()
    
    print("=" * 60)
    if success:
        print("ğŸ‰ All T5 LORO tests passed!")
        print("âœ… You can now use --model_type t5 with --optimizer loro_adamw")
    else:
        print("âŒ Some T5 LORO tests failed!")
        print("ğŸ”§ Please check the error messages above")