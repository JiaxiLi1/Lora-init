ðŸš€ Simple MLP Forward Speed Benchmark
Device: cuda, dtype: torch.bfloat16

================================================================================
ðŸ“¦ Testing Batch Size: 1x512 (total tokens: 512)
================================================================================

ðŸ”§ Model: 60M

======================================================================
Testing MLP: hidden=512, intermediate=1376, rank=128
Batch*Seq size: 512
======================================================================
2:4 mask sparsity: 50.0%

1. Fullrank SwiGLU:
   Time: 0.081 ms

2. Lowrank SwiGLU:
   Time: 0.073 ms
   Speedup vs fullrank: 1.11x

3. Lowrank ReLU2 (no 2:4):
   Time: 0.060 ms
   Speedup vs fullrank: 1.35x

4. Lowrank ReLU2 + 2:4 sparse:
   (Mask application time excluded from measurement)
   Time: 0.163 ms
   Speedup vs fullrank: 0.49x
   Speedup vs lowrank ReLU2: 0.37x
   Mask application time: 0.006 ms (excluded from above)

ðŸ”§ Model: 130M

======================================================================
Testing MLP: hidden=768, intermediate=2048, rank=256
Batch*Seq size: 512
======================================================================
2:4 mask sparsity: 50.0%

1. Fullrank SwiGLU:
   Time: 0.119 ms

2. Lowrank SwiGLU:
   Time: 0.095 ms
   Speedup vs fullrank: 1.25x

3. Lowrank ReLU2 (no 2:4):
   Time: 0.084 ms
   Speedup vs fullrank: 1.41x

4. Lowrank ReLU2 + 2:4 sparse:
   (Mask application time excluded from measurement)
   Time: 0.163 ms
   Speedup vs fullrank: 0.73x
   Speedup vs lowrank ReLU2: 0.52x
   Mask application time: 0.008 ms (excluded from above)

ðŸ”§ Model: 350M

======================================================================
Testing MLP: hidden=1024, intermediate=2736, rank=256
Batch*Seq size: 512
======================================================================
2:4 mask sparsity: 50.0%

1. Fullrank SwiGLU:
   Time: 0.225 ms

2. Lowrank SwiGLU:
   Time: 0.122 ms
   Speedup vs fullrank: 1.85x

3. Lowrank ReLU2 (no 2:4):
   Time: 0.085 ms
   Speedup vs fullrank: 2.65x

4. Lowrank ReLU2 + 2:4 sparse:
   (Mask application time excluded from measurement)
   Time: 0.175 ms
   Speedup vs fullrank: 1.28x
   Speedup vs lowrank ReLU2: 0.48x
   Mask application time: 0.014 ms (excluded from above)

ðŸ”§ Model: 1B

======================================================================
Testing MLP: hidden=2048, intermediate=5461, rank=512
Batch*Seq size: 512
======================================================================
2:4 mask sparsity: 50.0%

1. Fullrank SwiGLU:
   Time: 0.851 ms

2. Lowrank SwiGLU:
   Time: 0.385 ms
   Speedup vs fullrank: 2.21x

3. Lowrank ReLU2 (no 2:4):
   Time: 0.292 ms
   Speedup vs fullrank: 2.91x

4. Lowrank ReLU2 + 2:4 sparse:
   (Mask application time excluded from measurement)
   Time: 0.415 ms
   Speedup vs fullrank: 2.05x
   Speedup vs lowrank ReLU2: 0.70x
   Mask application time: 0.025 ms (excluded from above)

ðŸ”§ Model: 7B

======================================================================
Testing MLP: hidden=4096, intermediate=11008, rank=1024
Batch*Seq size: 512
======================================================================
2:4 mask sparsity: 50.0%

1. Fullrank SwiGLU:
   Time: 2.491 ms

2. Lowrank SwiGLU:
   Time: 0.952 ms
   Speedup vs fullrank: 2.62x

3. Lowrank ReLU2 (no 2:4):
   Time: 0.794 ms
   Speedup vs fullrank: 3.14x

4. Lowrank ReLU2 + 2:4 sparse:
   (Mask application time excluded from measurement)
   Time: 0.842 ms
   Speedup vs fullrank: 2.96x
   Speedup vs lowrank ReLU2: 0.94x
   Mask application time: 0.047 ms (excluded from above)

ðŸ”§ Model: 13B

======================================================================
Testing MLP: hidden=5120, intermediate=13824, rank=1280
Batch*Seq size: 512
======================================================================
2:4 mask sparsity: 50.0%

1. Fullrank SwiGLU:
   Time: 3.212 ms

2. Lowrank SwiGLU:
   Time: 1.213 ms
   Speedup vs fullrank: 2.65x

3. Lowrank ReLU2 (no 2:4):
   Time: 1.097 ms
   Speedup vs fullrank: 2.93x

4. Lowrank ReLU2 + 2:4 sparse:
   (Mask application time excluded from measurement)
   Time: 1.127 ms
   Speedup vs fullrank: 2.85x
   Speedup vs lowrank ReLU2: 0.97x
   Mask application time: 0.058 ms (excluded from above)

ðŸ”§ Model: 33B

======================================================================
Testing MLP: hidden=6656, intermediate=17920, rank=1664
Batch*Seq size: 512
======================================================================
2:4 mask sparsity: 50.0%

1. Fullrank SwiGLU:
   Time: 5.218 ms

2. Lowrank SwiGLU:
   Time: 1.983 ms
   Speedup vs fullrank: 2.63x

3. Lowrank ReLU2 (no 2:4):
   Time: 1.901 ms
   Speedup vs fullrank: 2.74x

4. Lowrank ReLU2 + 2:4 sparse:
   (Mask application time excluded from measurement)
   Time: 2.179 ms
   Speedup vs fullrank: 2.39x
   Speedup vs lowrank ReLU2: 0.87x
   Mask application time: 0.075 ms (excluded from above)

ðŸ”§ Model: 65B

======================================================================
Testing MLP: hidden=8192, intermediate=22016, rank=2048
Batch*Seq size: 512
======================================================================
2:4 mask sparsity: 50.0%

1. Fullrank SwiGLU:
   Time: 8.585 ms

2. Lowrank SwiGLU:
   Time: 3.079 ms
   Speedup vs fullrank: 2.79x

3. Lowrank ReLU2 (no 2:4):
   Time: 2.716 ms
   Speedup vs fullrank: 3.16x

4. Lowrank ReLU2 + 2:4 sparse:
   (Mask application time excluded from measurement)
   Time: 3.059 ms
   Speedup vs fullrank: 2.81x
   Speedup vs lowrank ReLU2: 0.89x
   Mask application time: 0.090 ms (excluded from above)

================================================================================
ðŸ“¦ Testing Batch Size: 4x512 (total tokens: 2048)
================================================================================

ðŸ”§ Model: 60M

======================================================================
Testing MLP: hidden=512, intermediate=1376, rank=128
Batch*Seq size: 2048
======================================================================
2:4 mask sparsity: 50.0%

1. Fullrank SwiGLU:
   Time: 0.259 ms

2. Lowrank SwiGLU:
   Time: 0.144 ms
   Speedup vs fullrank: 1.80x

3. Lowrank ReLU2 (no 2:4):
   Time: 0.134 ms
   Speedup vs fullrank: 1.93x

4. Lowrank ReLU2 + 2:4 sparse:
   (Mask application time excluded from measurement)
   Time: 0.155 ms
   Speedup vs fullrank: 1.67x
   Speedup vs lowrank ReLU2: 0.86x
   Mask application time: 0.025 ms (excluded from above)

ðŸ”§ Model: 130M

======================================================================
Testing MLP: hidden=768, intermediate=2048, rank=256
Batch*Seq size: 2048
======================================================================
2:4 mask sparsity: 50.0%

1. Fullrank SwiGLU:
   Time: 0.405 ms

2. Lowrank SwiGLU:
   Time: 0.243 ms
   Speedup vs fullrank: 1.67x

3. Lowrank ReLU2 (no 2:4):
   Time: 0.222 ms
   Speedup vs fullrank: 1.83x

4. Lowrank ReLU2 + 2:4 sparse:
   (Mask application time excluded from measurement)
   Time: 0.257 ms
   Speedup vs fullrank: 1.58x
   Speedup vs lowrank ReLU2: 0.86x
   Mask application time: 0.035 ms (excluded from above)

ðŸ”§ Model: 350M

======================================================================
Testing MLP: hidden=1024, intermediate=2736, rank=256
Batch*Seq size: 2048
======================================================================
2:4 mask sparsity: 50.0%

1. Fullrank SwiGLU:
   Time: 0.709 ms

2. Lowrank SwiGLU:
   Time: 0.306 ms
   Speedup vs fullrank: 2.32x

3. Lowrank ReLU2 (no 2:4):
   Time: 0.288 ms
   Speedup vs fullrank: 2.47x

4. Lowrank ReLU2 + 2:4 sparse:
   (Mask application time excluded from measurement)
   Time: 0.376 ms
   Speedup vs fullrank: 1.89x
   Speedup vs lowrank ReLU2: 0.77x
   Mask application time: 0.046 ms (excluded from above)

ðŸ”§ Model: 1B

======================================================================
Testing MLP: hidden=2048, intermediate=5461, rank=512
Batch*Seq size: 2048
======================================================================
2:4 mask sparsity: 50.0%

1. Fullrank SwiGLU:
   Time: 2.952 ms

2. Lowrank SwiGLU:
   Time: 1.159 ms
   Speedup vs fullrank: 2.55x

3. Lowrank ReLU2 (no 2:4):
   Time: 1.049 ms
   Speedup vs fullrank: 2.81x

4. Lowrank ReLU2 + 2:4 sparse:
   (Mask application time excluded from measurement)
   Time: 1.193 ms
   Speedup vs fullrank: 2.47x
   Speedup vs lowrank ReLU2: 0.88x
   Mask application time: 0.088 ms (excluded from above)

ðŸ”§ Model: 7B

======================================================================
Testing MLP: hidden=4096, intermediate=11008, rank=1024
Batch*Seq size: 2048
======================================================================
2:4 mask sparsity: 50.0%

1. Fullrank SwiGLU:
   Time: 8.160 ms

2. Lowrank SwiGLU:
   Time: 3.270 ms
   Speedup vs fullrank: 2.50x

3. Lowrank ReLU2 (no 2:4):
   Time: 3.008 ms
   Speedup vs fullrank: 2.71x

4. Lowrank ReLU2 + 2:4 sparse:
   (Mask application time excluded from measurement)
   Time: 3.229 ms
   Speedup vs fullrank: 2.53x
   Speedup vs lowrank ReLU2: 0.93x
   Mask application time: 0.173 ms (excluded from above)

ðŸ”§ Model: 13B

======================================================================
Testing MLP: hidden=5120, intermediate=13824, rank=1280
Batch*Seq size: 2048
======================================================================
2:4 mask sparsity: 50.0%

1. Fullrank SwiGLU:
   Time: 12.317 ms

2. Lowrank SwiGLU:
   Time: 4.567 ms
   Speedup vs fullrank: 2.70x

3. Lowrank ReLU2 (no 2:4):
   Time: 4.240 ms
   Speedup vs fullrank: 2.91x

4. Lowrank ReLU2 + 2:4 sparse:
   (Mask application time excluded from measurement)
   Time: 4.416 ms
   Speedup vs fullrank: 2.79x
   Speedup vs lowrank ReLU2: 0.96x
   Mask application time: 0.217 ms (excluded from above)

ðŸ”§ Model: 33B

======================================================================
Testing MLP: hidden=6656, intermediate=17920, rank=1664
Batch*Seq size: 2048
======================================================================
2:4 mask sparsity: 50.0%

1. Fullrank SwiGLU:
   Time: 20.688 ms

2. Lowrank SwiGLU:
   Time: 7.780 ms
   Speedup vs fullrank: 2.66x

3. Lowrank ReLU2 (no 2:4):
   Time: 7.196 ms
   Speedup vs fullrank: 2.88x

4. Lowrank ReLU2 + 2:4 sparse:
   (Mask application time excluded from measurement)
   Time: 7.722 ms
   Speedup vs fullrank: 2.68x
   Speedup vs lowrank ReLU2: 0.93x
   Mask application time: 0.282 ms (excluded from above)

ðŸ”§ Model: 65B

======================================================================
Testing MLP: hidden=8192, intermediate=22016, rank=2048
Batch*Seq size: 2048
======================================================================
2:4 mask sparsity: 50.0%

1. Fullrank SwiGLU:
   Time: 30.667 ms

2. Lowrank SwiGLU:
   Time: 11.441 ms
   Speedup vs fullrank: 2.68x

3. Lowrank ReLU2 (no 2:4):
   Time: 10.527 ms
   Speedup vs fullrank: 2.91x

4. Lowrank ReLU2 + 2:4 sparse:
   (Mask application time excluded from measurement)
   Time: 11.526 ms
   Speedup vs fullrank: 2.66x
   Speedup vs lowrank ReLU2: 0.91x
   Mask application time: 0.345 ms (excluded from above)

================================================================================
ðŸ“¦ Testing Batch Size: 8x512 (total tokens: 4096)
================================================================================

ðŸ”§ Model: 60M

======================================================================
Testing MLP: hidden=512, intermediate=1376, rank=128
Batch*Seq size: 4096
======================================================================
2:4 mask sparsity: 50.0%

1. Fullrank SwiGLU:
   Time: 0.452 ms

2. Lowrank SwiGLU:
   Time: 0.236 ms
   Speedup vs fullrank: 1.91x

3. Lowrank ReLU2 (no 2:4):
   Time: 0.216 ms
   Speedup vs fullrank: 2.09x

4. Lowrank ReLU2 + 2:4 sparse:
   (Mask application time excluded from measurement)
   Time: 0.247 ms
   Speedup vs fullrank: 1.83x
   Speedup vs lowrank ReLU2: 0.88x
   Mask application time: 0.047 ms (excluded from above)

ðŸ”§ Model: 130M

======================================================================
Testing MLP: hidden=768, intermediate=2048, rank=256
Batch*Seq size: 4096
======================================================================
2:4 mask sparsity: 50.0%

1. Fullrank SwiGLU:
   Time: 0.881 ms

2. Lowrank SwiGLU:
   Time: 0.432 ms
   Speedup vs fullrank: 2.04x

3. Lowrank ReLU2 (no 2:4):
   Time: 0.416 ms
   Speedup vs fullrank: 2.12x

4. Lowrank ReLU2 + 2:4 sparse:
   (Mask application time excluded from measurement)
   Time: 0.486 ms
   Speedup vs fullrank: 1.81x
   Speedup vs lowrank ReLU2: 0.86x
   Mask application time: 0.068 ms (excluded from above)

ðŸ”§ Model: 350M

======================================================================
Testing MLP: hidden=1024, intermediate=2736, rank=256
Batch*Seq size: 4096
======================================================================
2:4 mask sparsity: 50.0%

1. Fullrank SwiGLU:
   Time: 1.280 ms

2. Lowrank SwiGLU:
   Time: 0.569 ms
   Speedup vs fullrank: 2.25x

3. Lowrank ReLU2 (no 2:4):
   Time: 0.548 ms
   Speedup vs fullrank: 2.34x

4. Lowrank ReLU2 + 2:4 sparse:
   (Mask application time excluded from measurement)
   Time: 0.679 ms
   Speedup vs fullrank: 1.89x
   Speedup vs lowrank ReLU2: 0.81x
   Mask application time: 0.089 ms (excluded from above)

ðŸ”§ Model: 1B

======================================================================
Testing MLP: hidden=2048, intermediate=5461, rank=512
Batch*Seq size: 4096
======================================================================
2:4 mask sparsity: 50.0%

1. Fullrank SwiGLU:
   Time: 5.654 ms

2. Lowrank SwiGLU:
   Time: 2.242 ms
   Speedup vs fullrank: 2.52x

3. Lowrank ReLU2 (no 2:4):
   Time: 2.035 ms
   Speedup vs fullrank: 2.78x

4. Lowrank ReLU2 + 2:4 sparse:
   (Mask application time excluded from measurement)
   Time: 2.272 ms
   Speedup vs fullrank: 2.49x
   Speedup vs lowrank ReLU2: 0.90x
   Mask application time: 0.173 ms (excluded from above)

ðŸ”§ Model: 7B

======================================================================
Testing MLP: hidden=4096, intermediate=11008, rank=1024
Batch*Seq size: 4096
======================================================================
2:4 mask sparsity: 50.0%

1. Fullrank SwiGLU:
   Time: 16.077 ms

2. Lowrank SwiGLU:
   Time: 6.545 ms
   Speedup vs fullrank: 2.46x

3. Lowrank ReLU2 (no 2:4):
   Time: 6.214 ms
   Speedup vs fullrank: 2.59x

4. Lowrank ReLU2 + 2:4 sparse:
   (Mask application time excluded from measurement)
   Time: 6.358 ms
   Speedup vs fullrank: 2.53x
   Speedup vs lowrank ReLU2: 0.98x
   Mask application time: 0.345 ms (excluded from above)

ðŸ”§ Model: 13B

======================================================================
Testing MLP: hidden=5120, intermediate=13824, rank=1280
Batch*Seq size: 4096
======================================================================
2:4 mask sparsity: 50.0%

1. Fullrank SwiGLU:
   Time: 24.800 ms

2. Lowrank SwiGLU:
   Time: 9.371 ms
   Speedup vs fullrank: 2.65x

3. Lowrank ReLU2 (no 2:4):
   Time: 8.463 ms
   Speedup vs fullrank: 2.93x

4. Lowrank ReLU2 + 2:4 sparse:
   (Mask application time excluded from measurement)
   Time: 8.895 ms
   Speedup vs fullrank: 2.79x
   Speedup vs lowrank ReLU2: 0.95x
   Mask application time: 0.434 ms (excluded from above)

ðŸ”§ Model: 33B

======================================================================
Testing MLP: hidden=6656, intermediate=17920, rank=1664
Batch*Seq size: 4096
======================================================================
2:4 mask sparsity: 50.0%

1. Fullrank SwiGLU:
   Time: 41.817 ms

2. Lowrank SwiGLU:
   Time: 15.906 ms
   Speedup vs fullrank: 2.63x

3. Lowrank ReLU2 (no 2:4):
   Time: 14.091 ms
   Speedup vs fullrank: 2.97x

4. Lowrank ReLU2 + 2:4 sparse:
   (Mask application time excluded from measurement)
   Time: 15.373 ms
   Speedup vs fullrank: 2.72x
   Speedup vs lowrank ReLU2: 0.92x
   Mask application time: 0.557 ms (excluded from above)

ðŸ”§ Model: 65B

======================================================================
Testing MLP: hidden=8192, intermediate=22016, rank=2048
Batch*Seq size: 4096
======================================================================
2:4 mask sparsity: 50.0%

1. Fullrank SwiGLU:
   Time: 61.920 ms

2. Lowrank SwiGLU:
   Time: 22.866 ms
   Speedup vs fullrank: 2.71x

3. Lowrank ReLU2 (no 2:4):
   Time: 20.935 ms
   Speedup vs fullrank: 2.96x

4. Lowrank ReLU2 + 2:4 sparse:
   (Mask application time excluded from measurement)
   Time: 22.245 ms
   Speedup vs fullrank: 2.78x
   Speedup vs lowrank ReLU2: 0.94x
   Mask application time: 0.684 ms (excluded from above)

================================================================================
ðŸ“¦ Testing Batch Size: 16x512 (total tokens: 8192)
================================================================================

ðŸ”§ Model: 60M

======================================================================
Testing MLP: hidden=512, intermediate=1376, rank=128
Batch*Seq size: 8192
======================================================================
2:4 mask sparsity: 50.0%

1. Fullrank SwiGLU:
   Time: 0.740 ms

2. Lowrank SwiGLU:
   Time: 0.391 ms
   Speedup vs fullrank: 1.89x

3. Lowrank ReLU2 (no 2:4):
   Time: 0.387 ms
   Speedup vs fullrank: 1.91x

4. Lowrank ReLU2 + 2:4 sparse:
   (Mask application time excluded from measurement)
   Time: 0.477 ms
   Speedup vs fullrank: 1.55x
   Speedup vs lowrank ReLU2: 0.81x
   Mask application time: 0.090 ms (excluded from above)

ðŸ”§ Model: 130M

======================================================================
Testing MLP: hidden=768, intermediate=2048, rank=256
Batch*Seq size: 8192
======================================================================
2:4 mask sparsity: 50.0%

1. Fullrank SwiGLU:
   Time: 1.435 ms

2. Lowrank SwiGLU:
   Time: 0.828 ms
   Speedup vs fullrank: 1.73x

3. Lowrank ReLU2 (no 2:4):
   Time: 0.805 ms
   Speedup vs fullrank: 1.78x

4. Lowrank ReLU2 + 2:4 sparse:
   (Mask application time excluded from measurement)
   Time: 0.940 ms
   Speedup vs fullrank: 1.53x
   Speedup vs lowrank ReLU2: 0.86x
   Mask application time: 0.131 ms (excluded from above)

ðŸ”§ Model: 350M

======================================================================
Testing MLP: hidden=1024, intermediate=2736, rank=256
Batch*Seq size: 8192
======================================================================
2:4 mask sparsity: 50.0%

1. Fullrank SwiGLU:
   Time: 2.414 ms

2. Lowrank SwiGLU:
   Time: 1.109 ms
   Speedup vs fullrank: 2.18x

3. Lowrank ReLU2 (no 2:4):
   Time: 1.087 ms
   Speedup vs fullrank: 2.22x

4. Lowrank ReLU2 + 2:4 sparse:
   (Mask application time excluded from measurement)
   Time: 1.318 ms
   Speedup vs fullrank: 1.83x
   Speedup vs lowrank ReLU2: 0.82x
   Mask application time: 0.173 ms (excluded from above)

ðŸ”§ Model: 1B

======================================================================
Testing MLP: hidden=2048, intermediate=5461, rank=512
Batch*Seq size: 8192
======================================================================
2:4 mask sparsity: 50.0%

1. Fullrank SwiGLU:
   Time: 10.925 ms

2. Lowrank SwiGLU:
   Time: 4.122 ms
   Speedup vs fullrank: 2.65x

3. Lowrank ReLU2 (no 2:4):
   Time: 3.783 ms
   Speedup vs fullrank: 2.89x

4. Lowrank ReLU2 + 2:4 sparse:
   (Mask application time excluded from measurement)
   Time: 4.271 ms
   Speedup vs fullrank: 2.56x
   Speedup vs lowrank ReLU2: 0.89x
   Mask application time: 0.344 ms (excluded from above)

ðŸ”§ Model: 7B

======================================================================
Testing MLP: hidden=4096, intermediate=11008, rank=1024
Batch*Seq size: 8192
======================================================================
2:4 mask sparsity: 50.0%

1. Fullrank SwiGLU:
   Time: 31.661 ms

2. Lowrank SwiGLU:
   Time: 12.130 ms
   Speedup vs fullrank: 2.61x

3. Lowrank ReLU2 (no 2:4):
   Time: 11.314 ms
   Speedup vs fullrank: 2.80x

4. Lowrank ReLU2 + 2:4 sparse:
   (Mask application time excluded from measurement)
   Time: 12.234 ms
   Speedup vs fullrank: 2.59x
   Speedup vs lowrank ReLU2: 0.92x
   Mask application time: 0.690 ms (excluded from above)

ðŸ”§ Model: 13B

======================================================================
Testing MLP: hidden=5120, intermediate=13824, rank=1280
Batch*Seq size: 8192
======================================================================
2:4 mask sparsity: 50.0%

1. Fullrank SwiGLU:
   Time: 49.248 ms

2. Lowrank SwiGLU:
   Time: 18.297 ms
   Speedup vs fullrank: 2.69x

3. Lowrank ReLU2 (no 2:4):
   Time: 16.861 ms
   Speedup vs fullrank: 2.92x

4. Lowrank ReLU2 + 2:4 sparse:
   (Mask application time excluded from measurement)
   Time: 17.748 ms
   Speedup vs fullrank: 2.77x
   Speedup vs lowrank ReLU2: 0.95x
   Mask application time: 0.872 ms (excluded from above)

ðŸ”§ Model: 33B

======================================================================
Testing MLP: hidden=6656, intermediate=17920, rank=1664
Batch*Seq size: 8192
======================================================================
2:4 mask sparsity: 50.0%

1. Fullrank SwiGLU:
   Time: 82.069 ms

2. Lowrank SwiGLU:
   Time: 30.606 ms
   Speedup vs fullrank: 2.68x

3. Lowrank ReLU2 (no 2:4):
   Time: 27.806 ms
   Speedup vs fullrank: 2.95x

4. Lowrank ReLU2 + 2:4 sparse:
   (Mask application time excluded from measurement)
   Time: 29.752 ms
   Speedup vs fullrank: 2.76x
   Speedup vs lowrank ReLU2: 0.93x
   Mask application time: 1.120 ms (excluded from above)

ðŸ”§ Model: 65B

======================================================================
Testing MLP: hidden=8192, intermediate=22016, rank=2048
Batch*Seq size: 8192
======================================================================
2:4 mask sparsity: 50.0%

1. Fullrank SwiGLU:
   Time: 123.189 ms

2. Lowrank SwiGLU:
   Time: 44.759 ms
   Speedup vs fullrank: 2.75x

3. Lowrank ReLU2 (no 2:4):
   Time: 40.911 ms
   Speedup vs fullrank: 3.01x

4. Lowrank ReLU2 + 2:4 sparse:
   (Mask application time excluded from measurement)
   Time: 43.123 ms
   Speedup vs fullrank: 2.86x
   Speedup vs lowrank ReLU2: 0.95x
   Mask application time: 1.382 ms (excluded from above)

================================================================================
ðŸ“¦ Testing Batch Size: 32x512 (total tokens: 16384)
================================================================================

ðŸ”§ Model: 60M

======================================================================
Testing MLP: hidden=512, intermediate=1376, rank=128
Batch*Seq size: 16384
======================================================================
2:4 mask sparsity: 50.0%

1. Fullrank SwiGLU:
   Time: 1.469 ms

2. Lowrank SwiGLU:
   Time: 0.734 ms
   Speedup vs fullrank: 2.00x

3. Lowrank ReLU2 (no 2:4):
   Time: 0.739 ms
   Speedup vs fullrank: 1.99x

4. Lowrank ReLU2 + 2:4 sparse:
   (Mask application time excluded from measurement)
   Time: 0.931 ms
   Speedup vs fullrank: 1.58x
   Speedup vs lowrank ReLU2: 0.79x
   Mask application time: 0.175 ms (excluded from above)

ðŸ”§ Model: 130M

======================================================================
Testing MLP: hidden=768, intermediate=2048, rank=256
Batch*Seq size: 16384
======================================================================
2:4 mask sparsity: 50.0%

1. Fullrank SwiGLU:
   Time: 2.757 ms

2. Lowrank SwiGLU:
   Time: 1.617 ms
   Speedup vs fullrank: 1.70x

3. Lowrank ReLU2 (no 2:4):
   Time: 1.586 ms
   Speedup vs fullrank: 1.74x

4. Lowrank ReLU2 + 2:4 sparse:
   (Mask application time excluded from measurement)
   Time: 1.818 ms
   Speedup vs fullrank: 1.52x
   Speedup vs lowrank ReLU2: 0.87x
   Mask application time: 0.259 ms (excluded from above)

ðŸ”§ Model: 350M

======================================================================
Testing MLP: hidden=1024, intermediate=2736, rank=256
Batch*Seq size: 16384
======================================================================
2:4 mask sparsity: 50.0%

1. Fullrank SwiGLU:
   Time: 4.726 ms

2. Lowrank SwiGLU:
   Time: 2.163 ms
   Speedup vs fullrank: 2.19x

3. Lowrank ReLU2 (no 2:4):
   Time: 2.113 ms
   Speedup vs fullrank: 2.24x

4. Lowrank ReLU2 + 2:4 sparse:
   (Mask application time excluded from measurement)
   Time: 2.552 ms
   Speedup vs fullrank: 1.85x
   Speedup vs lowrank ReLU2: 0.83x
   Mask application time: 0.345 ms (excluded from above)

ðŸ”§ Model: 1B

======================================================================
Testing MLP: hidden=2048, intermediate=5461, rank=512
Batch*Seq size: 16384
======================================================================
2:4 mask sparsity: 50.0%

1. Fullrank SwiGLU:
   Time: 22.402 ms

2. Lowrank SwiGLU:
   Time: 8.328 ms
   Speedup vs fullrank: 2.69x

3. Lowrank ReLU2 (no 2:4):
   Time: 7.678 ms
   Speedup vs fullrank: 2.92x

4. Lowrank ReLU2 + 2:4 sparse:
   (Mask application time excluded from measurement)
   Time: 8.506 ms
   Speedup vs fullrank: 2.63x
   Speedup vs lowrank ReLU2: 0.90x
   Mask application time: 0.688 ms (excluded from above)

ðŸ”§ Model: 7B

======================================================================
Testing MLP: hidden=4096, intermediate=11008, rank=1024
Batch*Seq size: 16384
======================================================================
2:4 mask sparsity: 50.0%

1. Fullrank SwiGLU:
   Time: 62.729 ms

2. Lowrank SwiGLU:
   Time: 24.324 ms
   Speedup vs fullrank: 2.58x

3. Lowrank ReLU2 (no 2:4):
   Time: 22.689 ms
   Speedup vs fullrank: 2.76x

4. Lowrank ReLU2 + 2:4 sparse:
   (Mask application time excluded from measurement)
   Time: 23.851 ms
   Speedup vs fullrank: 2.63x
   Speedup vs lowrank ReLU2: 0.95x
   Mask application time: 1.389 ms (excluded from above)

ðŸ”§ Model: 13B

======================================================================
Testing MLP: hidden=5120, intermediate=13824, rank=1280
Batch*Seq size: 16384
======================================================================
2:4 mask sparsity: 50.0%

1. Fullrank SwiGLU:
   Time: 98.126 ms

2. Lowrank SwiGLU:
   Time: 36.352 ms
   Speedup vs fullrank: 2.70x

3. Lowrank ReLU2 (no 2:4):
   Time: 33.691 ms
   Speedup vs fullrank: 2.91x

4. Lowrank ReLU2 + 2:4 sparse:
   (Mask application time excluded from measurement)
   Time: 35.469 ms
   Speedup vs fullrank: 2.77x
   Speedup vs lowrank ReLU2: 0.95x
   Mask application time: 1.743 ms (excluded from above)

ðŸ”§ Model: 33B

======================================================================
Testing MLP: hidden=6656, intermediate=17920, rank=1664
Batch*Seq size: 16384
======================================================================
2:4 mask sparsity: 50.0%

1. Fullrank SwiGLU:
   Time: 163.632 ms

2. Lowrank SwiGLU:
   Time: 60.886 ms
   Speedup vs fullrank: 2.69x

3. Lowrank ReLU2 (no 2:4):
   Time: 55.164 ms
   Speedup vs fullrank: 2.97x

4. Lowrank ReLU2 + 2:4 sparse:
   (Mask application time excluded from measurement)
   Time: 58.553 ms
   Speedup vs fullrank: 2.79x
   Speedup vs lowrank ReLU2: 0.94x
   Mask application time: 2.262 ms (excluded from above)

ðŸ”§ Model: 65B

======================================================================
Testing MLP: hidden=8192, intermediate=22016, rank=2048
Batch*Seq size: 16384
======================================================================
2:4 mask sparsity: 50.0%

1. Fullrank SwiGLU:
   Time: 246.231 ms

2. Lowrank SwiGLU:
   Time: 88.751 ms
   Speedup vs fullrank: 2.77x

3. Lowrank ReLU2 (no 2:4):
   Time: 81.526 ms
   Speedup vs fullrank: 3.02x

4. Lowrank ReLU2 + 2:4 sparse:
   (Mask application time excluded from measurement)
   Time: 85.195 ms
   Speedup vs fullrank: 2.89x
   Speedup vs lowrank ReLU2: 0.96x
   Mask application time: 2.779 ms (excluded from above)

================================================================================
ðŸ“Š BATCH SIZE: 1x512 - Forward time (ms)
================================================================================
Model    Fullrank     LR-SwiGLU    LR-ReLU2     LR-ReLU2+2:4   
-----------------------------------------------------------------
60M      0.081        0.073        0.060        0.163          
130M     0.119        0.095        0.084        0.163          
350M     0.225        0.122        0.085        0.175          
1B       0.851        0.385        0.292        0.415          
7B       2.491        0.952        0.794        0.842          
13B      3.212        1.213        1.097        1.127          
33B      5.218        1.983        1.901        2.179          
65B      8.585        3.079        2.716        3.059          

ðŸ“Š SPEEDUP vs FULLRANK (Batch: 1x512)
Model    LR-SwiGLU    LR-ReLU2     LR-ReLU2+2:4    2:4 vs ReLU2   
--------------------------------------------------------------
60M      1.11        x 1.35        x 0.49           x 0.37           x
130M     1.25        x 1.41        x 0.73           x 0.52           x
350M     1.85        x 2.65        x 1.28           x 0.48           x
1B       2.21        x 2.91        x 2.05           x 0.70           x
7B       2.62        x 3.14        x 2.96           x 0.94           x
13B      2.65        x 2.93        x 2.85           x 0.97           x
33B      2.63        x 2.74        x 2.39           x 0.87           x
65B      2.79        x 3.16        x 2.81           x 0.89           x

================================================================================
ðŸ“Š BATCH SIZE: 4x512 - Forward time (ms)
================================================================================
Model    Fullrank     LR-SwiGLU    LR-ReLU2     LR-ReLU2+2:4   
-----------------------------------------------------------------
60M      0.259        0.144        0.134        0.155          
130M     0.405        0.243        0.222        0.257          
350M     0.709        0.306        0.288        0.376          
1B       2.952        1.159        1.049        1.193          
7B       8.160        3.270        3.008        3.229          
13B      12.317       4.567        4.240        4.416          
33B      20.688       7.780        7.196        7.722          
65B      30.667       11.441       10.527       11.526         

ðŸ“Š SPEEDUP vs FULLRANK (Batch: 4x512)
Model    LR-SwiGLU    LR-ReLU2     LR-ReLU2+2:4    2:4 vs ReLU2   
--------------------------------------------------------------
60M      1.80        x 1.93        x 1.67           x 0.86           x
130M     1.67        x 1.83        x 1.58           x 0.86           x
350M     2.32        x 2.47        x 1.89           x 0.77           x
1B       2.55        x 2.81        x 2.47           x 0.88           x
7B       2.50        x 2.71        x 2.53           x 0.93           x
13B      2.70        x 2.91        x 2.79           x 0.96           x
33B      2.66        x 2.88        x 2.68           x 0.93           x
65B      2.68        x 2.91        x 2.66           x 0.91           x

================================================================================
ðŸ“Š BATCH SIZE: 8x512 - Forward time (ms)
================================================================================
Model    Fullrank     LR-SwiGLU    LR-ReLU2     LR-ReLU2+2:4   
-----------------------------------------------------------------
60M      0.452        0.236        0.216        0.247          
130M     0.881        0.432        0.416        0.486          
350M     1.280        0.569        0.548        0.679          
1B       5.654        2.242        2.035        2.272          
7B       16.077       6.545        6.214        6.358          
13B      24.800       9.371        8.463        8.895          
33B      41.817       15.906       14.091       15.373         
65B      61.920       22.866       20.935       22.245         

ðŸ“Š SPEEDUP vs FULLRANK (Batch: 8x512)
Model    LR-SwiGLU    LR-ReLU2     LR-ReLU2+2:4    2:4 vs ReLU2   
--------------------------------------------------------------
60M      1.91        x 2.09        x 1.83           x 0.88           x
130M     2.04        x 2.12        x 1.81           x 0.86           x
350M     2.25        x 2.34        x 1.89           x 0.81           x
1B       2.52        x 2.78        x 2.49           x 0.90           x
7B       2.46        x 2.59        x 2.53           x 0.98           x
13B      2.65        x 2.93        x 2.79           x 0.95           x
33B      2.63        x 2.97        x 2.72           x 0.92           x
65B      2.71        x 2.96        x 2.78           x 0.94           x

================================================================================
ðŸ“Š BATCH SIZE: 16x512 - Forward time (ms)
================================================================================
Model    Fullrank     LR-SwiGLU    LR-ReLU2     LR-ReLU2+2:4   
-----------------------------------------------------------------
60M      0.740        0.391        0.387        0.477          
130M     1.435        0.828        0.805        0.940          
350M     2.414        1.109        1.087        1.318          
1B       10.925       4.122        3.783        4.271          
7B       31.661       12.130       11.314       12.234         
13B      49.248       18.297       16.861       17.748         
33B      82.069       30.606       27.806       29.752         
65B      123.189      44.759       40.911       43.123         

ðŸ“Š SPEEDUP vs FULLRANK (Batch: 16x512)
Model    LR-SwiGLU    LR-ReLU2     LR-ReLU2+2:4    2:4 vs ReLU2   
--------------------------------------------------------------
60M      1.89        x 1.91        x 1.55           x 0.81           x
130M     1.73        x 1.78        x 1.53           x 0.86           x
350M     2.18        x 2.22        x 1.83           x 0.82           x
1B       2.65        x 2.89        x 2.56           x 0.89           x
7B       2.61        x 2.80        x 2.59           x 0.92           x
13B      2.69        x 2.92        x 2.77           x 0.95           x
33B      2.68        x 2.95        x 2.76           x 0.93           x
65B      2.75        x 3.01        x 2.86           x 0.95           x

================================================================================
ðŸ“Š BATCH SIZE: 32x512 - Forward time (ms)
================================================================================
Model    Fullrank     LR-SwiGLU    LR-ReLU2     LR-ReLU2+2:4   
-----------------------------------------------------------------
60M      1.469        0.734        0.739        0.931          
130M     2.757        1.617        1.586        1.818          
350M     4.726        2.163        2.113        2.552          
1B       22.402       8.328        7.678        8.506          
7B       62.729       24.324       22.689       23.851         
13B      98.126       36.352       33.691       35.469         
33B      163.632      60.886       55.164       58.553         
65B      246.231      88.751       81.526       85.195         

ðŸ“Š SPEEDUP vs FULLRANK (Batch: 32x512)
Model    LR-SwiGLU    LR-ReLU2     LR-ReLU2+2:4    2:4 vs ReLU2   
--------------------------------------------------------------
60M      2.00        x 1.99        x 1.58           x 0.79           x
130M     1.70        x 1.74        x 1.52           x 0.87           x
350M     2.19        x 2.24        x 1.85           x 0.83           x
1B       2.69        x 2.92        x 2.63           x 0.90           x
7B       2.58        x 2.76        x 2.63           x 0.95           x
13B      2.70        x 2.91        x 2.77           x 0.95           x
33B      2.69        x 2.97        x 2.79           x 0.94           x
65B      2.77        x 3.02        x 2.89           x 0.96           x

================================================================================
ðŸŽ¯ 2:4 SPARSE SPEEDUP vs NON-SPARSE RELU2
================================================================================
Model    1x512      4x512      8x512      16x512     32x512    
--------------------------------------------------------------------
60M           0.37x      0.86x      0.88x      0.81x      0.79x
130M          0.52x      0.86x      0.86x      0.86x      0.87x
350M          0.48x      0.77x      0.81x      0.82x      0.83x
1B            0.70x      0.88x      0.90x      0.89x      0.90x
7B            0.94x      0.93x      0.98x      0.92x      0.95x
13B           0.97x      0.96x      0.95x      0.95x      0.95x
33B           0.87x      0.93x      0.92x      0.93x      0.94x
65B           0.89x      0.91x      0.94x      0.95x      0.96x

================================================================================
ðŸ”¥ LoRO ReLU2+2:4 SPEEDUP vs FULLRANK (all batch sizes)
================================================================================
Model    1x512      4x512      8x512      16x512     32x512    
--------------------------------------------------------------------
60M           0.49x      1.67x      1.83x      1.55x      1.58x
130M          0.73x      1.58x      1.81x      1.53x      1.52x
350M          1.28x      1.89x      1.89x      1.83x      1.85x
1B            2.05x      2.47x      2.49x      2.56x      2.63x
7B            2.96x      2.53x      2.53x      2.59x      2.63x
13B           2.85x      2.79x      2.79x      2.77x      2.77x
33B           2.39x      2.68x      2.72x      2.76x      2.79x
65B           2.81x      2.66x      2.78x      2.86x      2.89x

================================================================================
âš¡ LoRO ReLU2+2:4 SPEEDUP vs LoRO SwiGLU (all batch sizes)
================================================================================
Model    1x512      4x512      8x512      16x512     32x512    
--------------------------------------------------------------------
60M           0.44x      0.93x      0.96x      0.82x      0.79x
130M          0.58x      0.94x      0.89x      0.88x      0.89x
350M          0.70x      0.81x      0.84x      0.84x      0.85x
1B            0.93x      0.97x      0.99x      0.97x      0.98x
7B            1.13x      1.01x      1.03x      0.99x      1.02x
13B           1.08x      1.03x      1.05x      1.03x      1.02x
33B           0.91x      1.01x      1.03x      1.03x      1.04x
65B           1.01x      0.99x      1.03x      1.04x      1.04x
