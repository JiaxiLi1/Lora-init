这是我的进行 lowrank 训练以及 activation 2:4训练的代码库，run_c4.py是主函数，需要进入loro_2by4环境并且执行类似 torchrun --nproc_per_node 1 run_c4.py --model_config configs/llama_130m.json --dtype bfloat16 --batch_size 64 --total_batch_size 512 --num_training_steps 20000 --save_every 1000 --eval_every 1000 --lr 0.0001 --scheduler cosine_restart --warmup_steps 2000 --min_lr_ratio 0.1 --cosine_restart_freq 500 --lr_adjust_steps -2000 --weight_decay 0.1 --optimizer cola_adamw --loro_refresh all --loro_refresh_freq 500 --loro_scope all --loro_init xavier --loro_attn_rank 256 --loro_mlp_rank 256 --loro_type loro --loro_freq 500 --loro_lr_scaler -1 --c4_local False --enable_2to4_sparse False --save_ckpt True --attn_2by4 True --mlp_2by4 True --seed 43 --flip_rate True --activation_2by4 True --activation_soft_threshold True --squ_relu relu2 --activation_sparse_method soft_threshold_weights --dynamic_activation_steps 10 --activation_calibration_samples 50 --activation_dense_warmup_steps 0 --dx_direct_sparse 3 --wandb_sparsityrelu True --permute_2by4 False --momentum_reset_steps 200 --more_activation_relu2 True --cola_sparse_method cola_init的命令来进行训练。

我用--optimizer选项来设置不同的用到的lowrank训练方法，--optimizer=adamw表示执行fullrank训练，会用到的lowrank训练方法有：--optimizer=loro_adamw表示执行loro的lowrank训练，loro_adamw是指把attn和mlp里的linear层变成a和b两个lowrank层的乘积，然后使用loro独特的优化器；--optimizer=cola_adamw表示将attn和mlp层的linear层换成a和b乘积的形式但是在ab中间加入一个激活函数；--optimizer=lost_adamw是指将attn和mlp的linear层换成a和b乘积以及一个稀疏矩阵的形式，同时ab之间加入激活函数，最终输出是lowrank输出和column wise的稀疏矩阵的输出的加权和。

这个代码的主要任务其实是进行activation的2:4形式训练，由于我看到有文章说把llama的SwiGLU模块换成普通的ffn形式同时使用relu2激活函数（即去掉gated unit层，只留up和down层），在训练一段时间后，可以利用relu2容易产生高稀疏度的特性，对relu2后的activation进行2:4化，然后进行gpu支持的2:4矩阵乘法来加速计算。这个在/home/rtx3090/code_jiaxi/LORO-main_temp/2503.16672v1.pdf里说了，如果有需要你可以参考这个文章。

所以我想在lowrank训练基础上进行activation2:4训练来加速训练以及推理，--squ_relu relu2表示把mlp换成ffn模块然后使用relu2激活函数，在目前的代码里是使用了自定义的forward和backward函数。然后--activation_sparse_method soft_threshold_weights表示了要对activation进行的2:4方法，目前有naive即直接根据数值大小来hard threshold来把四个值中较小的2个值变成0，然后soft_threshold_weights是指用soft_thresholding的方法，即把四个值里较小的两个值变成0，同时把较大的两个值减去倒数第二大的值，但是在一开始基于模型weight2:4前后的值的mse最小来计算一个scaling值，在2:4后缩放一下weights来让这种soft threshold前后模型weights的值差别不太大。但是由于soft_threshold_weights是参考了对weights做activation 2:4的文章，而我们这里是通过activation来2:4，所以我设计了soft_dynamic这种方式，即soft_thresholding的一个小升级，即通过一组batch的数据的activation前后mse最小来得到scaling值，你可以通过阅读代码明白具体是怎么回事（如果有实现的问题需要告诉我）。然后--dynamic_activation_steps和--activation_calibration_samples分别表示控制soft_dynamic的多少步执行一次调整scaling值以及用多少数据来计算scaling值。--activation_dense_warmup_steps表示在进行activation 2：4训练前先进行多少步的dense训练，比如如果activation_dense_warmup_steps=2000就表示2000步之前都是普通的dense训练，执行普通的forward和backward，2000步之后才是activation 2:4训练。--dx_direct_sparse是表示在activation 2:4时，backward里的稀疏计算是否执行split_gemm（split_gemm也是参考的2503.16672v1.pdf里的方法，2503.16672v1.pdf里有两个创新点，一个是在进入ffn前后执行一次input permutation，这个我用--2by4_permute控制的，还有就是这个split_gemm也就是把95%的部分执行稀疏计算，另外5%执行dense计算），然后dx_direct_sparse=1表示执行正常的split_gemm计算，--dx_direct_sparse=2表示全部执行稀疏计算，不再计算稀疏度和区分95%和5%，--dx_direct_sparse=3表示不执行稀疏计算，就用普通的dense方式传递梯度。--momentum_reset_steps是指每多少步执行一次对adamw里的一阶和二阶量执行一次reset，因为有文章说这样对训练有帮助。--momentum_reset_steps为True时是指不光是在ffn里的relu2后执行activation 2:4，同时在cola和lost的lowrank矩阵间也使用relu2激活函数然后对relu2后的activation使用2:4，相当于增加了activation 2:4的范围

以上就是代码背景，然后修改完你可以直接用我刚才给你的命令来执行测试（请设置一个timeout，因为evaluation时会很消耗时间），不需要设置额外的测试文件，注意你需要先进入loro_2by4环境