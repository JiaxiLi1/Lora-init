#!/usr/bin/env python3

import torch
import torch.nn as nn
from transformers import LlamaConfig, LlamaForCausalLM
from loro_torch.lowrank_module import apply_lowrank_param
from loro_torch.sparse_overlay import apply_sparse_overlay_on_loro

print('ğŸ” è°ƒè¯•LOROæ¨¡å—æ£€æµ‹...')

def debug_loro_detection():
    # åˆ›å»ºä¸€ä¸ªå°çš„LLaMAæ¨¡å‹é…ç½®
    config = LlamaConfig(
        vocab_size=1000,
        hidden_size=64,
        intermediate_size=128,
        num_hidden_layers=2,
        num_attention_heads=4,
        num_key_value_heads=4,
        max_position_embeddings=128,
        initializer_range=0.02,
    )
    
    print("1. åˆ›å»ºåŸå§‹æ¨¡å‹...")
    model = LlamaForCausalLM(config)
    
    # æ£€æŸ¥åŸå§‹æ¨¡å‹ä¸­çš„Linearæ¨¡å—
    print("2. åŸå§‹æ¨¡å‹ä¸­çš„Linearæ¨¡å—:")
    original_linear_count = 0
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            original_linear_count += 1
            if 'q_proj' in name or 'k_proj' in name:
                print(f"   {name}: {type(module)}")
    print(f"   æ€»å…±{original_linear_count}ä¸ªLinearæ¨¡å—")
    
    print("\n3. åº”ç”¨LORO...")
    apply_lowrank_param(
        model,
        config,
        model_type="llama",
        scope="all",  # åº”ç”¨åˆ°æ‰€æœ‰å±‚
        attn_rank=16,
        mlp_rank=32,
        init="xavier",
        verbose=False
    )
    
    # æ£€æŸ¥LOROåº”ç”¨åçš„æ¨¡å—
    print("4. LOROåº”ç”¨åçš„æ¨¡å—æ£€æŸ¥:")
    loro_count = 0
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    
    for name, module in model.named_modules():
        module_name = name.split('.')[-1]
        if module_name in target_modules:
            print(f"   {name}:")
            print(f"     ç±»å‹: {type(module)}")
            print(f"     æœ‰weight_in: {hasattr(module, 'weight_in')}")
            print(f"     æœ‰weight_out: {hasattr(module, 'weight_out')}")
            if hasattr(module, 'weight_in') and hasattr(module, 'weight_out'):
                print(f"     weight_in shape: {module.weight_in.shape}")
                print(f"     weight_out shape: {module.weight_out.shape}")
                loro_count += 1
    
    print(f"\n5. æ‰¾åˆ°{loro_count}ä¸ªLOROæ¨¡å—")
    
    print("\n6. å°è¯•åº”ç”¨sparse overlay...")
    try:
        model = apply_sparse_overlay_on_loro(
            model,
            sparse_init_scale=1.0,
            target_modules=["q_proj", "k_proj", "v_proj"]  # åªæµ‹è¯•å‡ ä¸ªæ¨¡å—
        )
        print("âœ… Sparse overlayåº”ç”¨æˆåŠŸï¼")
    except Exception as e:
        print(f"âŒ Sparse overlayåº”ç”¨å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    # æ£€æŸ¥æœ€ç»ˆçš„æ¨¡å—ç±»å‹
    print("\n7. æœ€ç»ˆæ¨¡å—ç±»å‹æ£€æŸ¥:")
    for name, module in model.named_modules():
        if 'q_proj' in name or 'k_proj' in name or 'v_proj' in name:
            print(f"   {name}: {type(module)}")

if __name__ == '__main__':
    debug_loro_detection() 