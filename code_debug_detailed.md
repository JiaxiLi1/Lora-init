# ActivationSparse2to4LowRankFunction å®Œæ•´ä»£ç é€è¡Œè¯¦è§£

è¿™æ˜¯ä¸€ä»½æå…¶è¯¦ç»†çš„é€è¡Œä»£ç è§£é‡Šæ–‡æ¡£ï¼Œæ—¨åœ¨å¸®åŠ©ç†è§£split_gemmå’Œactivation 2:4ç¨€ç–è®­ç»ƒçš„å…·ä½“å®ç°ï¼Œç‰¹åˆ«æ˜¯ä¸ºäº†å®šä½NaNé—®é¢˜ã€‚

## ç›®å½•
1. [ç±»æ¦‚è¿°](#ç±»æ¦‚è¿°)
2. [Forwardæ–¹æ³•å®Œæ•´é€è¡Œè§£é‡Š](#forwardæ–¹æ³•å®Œæ•´é€è¡Œè§£é‡Š)
3. [Backwardæ–¹æ³•å®Œæ•´é€è¡Œè§£é‡Š](#backwardæ–¹æ³•å®Œæ•´é€è¡Œè§£é‡Š)
4. [æ ¸å¿ƒKernelå‡½æ•°è¯¦ç»†è§£é‡Š](#æ ¸å¿ƒkernelå‡½æ•°è¯¦ç»†è§£é‡Š)
5. [æ½œåœ¨é—®é¢˜åˆ†æ](#æ½œåœ¨é—®é¢˜åˆ†æ)

---

## ç±»æ¦‚è¿°

`ActivationSparse2to4LowRankFunction` æ˜¯ä¸€ä¸ªè‡ªå®šä¹‰çš„ PyTorch autograd Functionï¼Œå®ç°äº†ä½ç§©ï¼ˆLow-Rankï¼‰FFN å±‚ä¸ Activation 2:4 ç¨€ç–æ€§çš„ç»“åˆã€‚ä¸»è¦ç‰¹ç‚¹ï¼š
- ä½¿ç”¨ Split-GEMM ç­–ç•¥ï¼š95% ç¨€ç–åˆ—ç”¨2:4ç¨€ç–è®¡ç®—ï¼Œ5% å¯†é›†åˆ—ç”¨æ ‡å‡†è®¡ç®—
- æ”¯æŒ Dense Warmupï¼šå‰Næ­¥ç”¨å¯†é›†è®¡ç®—ï¼Œä¹‹åæ‰å¯ç”¨ç¨€ç–
- ç¼“å­˜å‰å‘ä¼ æ’­çš„ç¨€ç–æ€§ä¿¡æ¯ä¾›åå‘ä¼ æ’­ä½¿ç”¨

æ–‡ä»¶ä½ç½®ï¼š`/home/rtx3090/code_jiaxi/LORO-main_temp/peft_pretraining/modeling_llama.py`

---

## Forwardæ–¹æ³•å®Œæ•´é€è¡Œè§£é‡Š

### å‡½æ•°ç­¾åå’Œå‚æ•°è®¾ç½® (Lines 292-317)

```python
@staticmethod
@custom_fwd  # Line 291: PyTorchè‡ªå®šä¹‰å‰å‘ä¼ æ’­è£…é¥°å™¨ï¼Œç”¨äºæ··åˆç²¾åº¦è®­ç»ƒ
def forward(ctx, input, weight_in1, weight_out1, weight_in2, weight_out2, 
            bias1=None, bias2=None, sparsity_method="mvue", warmup_steps=None, 
            dx_direct_sparse=1, dynamic_steps=10, calibration_samples=100, 
            enable_permute=True):
```

**å‚æ•°è¯¦è§£ï¼š**
- `ctx`: ä¸Šä¸‹æ–‡å¯¹è±¡ï¼Œç”¨äºä¿å­˜åå‘ä¼ æ’­æ‰€éœ€çš„å¼ é‡
- `input`: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ [batch_size, seq_len, hidden_size]
- `weight_in1, weight_out1`: ç¬¬ä¸€ä¸ªä½ç§©å±‚çš„æƒé‡ï¼Œç›¸å½“äºå°† W1 åˆ†è§£ä¸º weight_in1 @ weight_out1.T
- `weight_in2, weight_out2`: ç¬¬äºŒä¸ªä½ç§©å±‚çš„æƒé‡
- `bias1, bias2`: å¯é€‰çš„åç½®é¡¹
- `sparsity_method`: ç¨€ç–åŒ–æ–¹æ³•ï¼ˆ"naive", "mvue", "soft_threshold_weights", "soft_dynamic"ï¼‰
- `warmup_steps`: Dense warmupçš„æ­¥æ•°ï¼Œå‰Næ­¥ä¸ä½¿ç”¨ç¨€ç–
- `dx_direct_sparse`: åå‘ä¼ æ’­ç¨€ç–ç­–ç•¥ï¼ˆ1=split_gemm, 2=å…¨ç¨€ç–, 3=å…¨å¯†é›†ï¼‰
- `dynamic_steps`: åŠ¨æ€è°ƒæ•´scalingçš„æ­¥æ•°é—´éš”
- `calibration_samples`: ç”¨äºè®¡ç®—scalingçš„æ ·æœ¬æ•°
- `enable_permute`: æ˜¯å¦å¯ç”¨tokenç½®æ¢

```python
# Lines 308-312: ä¿å­˜å‚æ•°åˆ°ä¸Šä¸‹æ–‡
ctx.sparsity_method = sparsity_method  # ç¨€ç–åŒ–æ–¹æ³•
ctx.input_shape = input.shape  # è¾“å…¥å½¢çŠ¶ï¼Œåå‘ä¼ æ’­éœ€è¦
ctx.dx_direct_sparse = int(dx_direct_sparse)  # ç¡®ä¿æ˜¯æ•´æ•°
ctx.dynamic_steps = dynamic_steps
ctx.calibration_samples = calibration_samples

# Lines 315-316: æ›´æ–°å…¨å±€warmupæ­¥æ•°
if warmup_steps is not None:
    ActivationSparse2to4LowRankFunction._warmup_steps = warmup_steps
```

### è¾“å…¥ç½®æ¢ (Lines 318-341)

Tokenç½®æ¢æ˜¯ä¸ºäº†æ‰“ç ´åºåˆ—ä¸­çš„å±€éƒ¨ç›¸å…³æ€§ï¼Œè®©2:4ç¨€ç–æ¨¡å¼æ›´å‡åŒ€ï¼š

```python
batch_size, seq_len, hidden_size = input.shape  # Line 318: è·å–è¾“å…¥ç»´åº¦

if enable_permute:  # Line 321: å¦‚æœå¯ç”¨ç½®æ¢
    perm_key = f"{seq_len}_{input.device}"  # Line 322: åˆ›å»ºç½®æ¢é”®ï¼ˆåºåˆ—é•¿åº¦+è®¾å¤‡ï¼‰
    
    # Lines 324-329: é¦–æ¬¡é‡åˆ°æ­¤åºåˆ—é•¿åº¦æ—¶åˆ›å»ºå›ºå®šçš„éšæœºç½®æ¢
    if perm_key not in ActivationSparse2to4LowRankFunction._token_permutation:
        perm = torch.randperm(seq_len, device=input.device)  # éšæœºç½®æ¢ç´¢å¼•
        inv_perm = torch.argsort(perm)  # é€†ç½®æ¢ç´¢å¼•ï¼ˆç”¨äºæ¢å¤ï¼‰
        # ç¼“å­˜ç½®æ¢æ¨¡å¼ï¼ŒåŒæ ·é•¿åº¦çš„åºåˆ—ä½¿ç”¨ç›¸åŒç½®æ¢
        ActivationSparse2to4LowRankFunction._token_permutation[perm_key] = perm
        ActivationSparse2to4LowRankFunction._inverse_permutation[perm_key] = inv_perm
    
    # Lines 331-332: ä»ç¼“å­˜è·å–ç½®æ¢
    perm = ActivationSparse2to4LowRankFunction._token_permutation[perm_key]
    inv_perm = ActivationSparse2to4LowRankFunction._inverse_permutation[perm_key]
    
    # Line 335: åº”ç”¨ç½®æ¢ - é‡æ–°æ’åˆ—åºåˆ—ç»´åº¦çš„token
    # ä¾‹å¦‚ï¼šå¦‚æœperm=[2,0,1]ï¼Œåˆ™ç¬¬0ä¸ªtokenç§»åˆ°ä½ç½®2ï¼Œç¬¬1ä¸ªç§»åˆ°ä½ç½®0ï¼Œç¬¬2ä¸ªç§»åˆ°ä½ç½®1
    input_permuted = input[:, perm, :]
else:
    # Lines 337-340: ä¸ç½®æ¢ï¼Œç›´æ¥ä½¿ç”¨åŸè¾“å…¥
    input_permuted = input
    perm = None
    inv_perm = None
```

### ç¬¬ä¸€ä¸ªä½ç§©å±‚è®¡ç®— (Lines 342-368)

```python
# Line 344: é‡å¡‘ä¸º2Dç”¨äºçŸ©é˜µä¹˜æ³• 
# [batch_size, seq_len, hidden_size] -> [batch*seq, hidden_size]
input_2d = input_permuted.view(-1, input_permuted.shape[-1])

# Line 347: ç¬¬ä¸€æ­¥ä½ç§©ä¹˜æ³•ï¼šinput @ weight_in1
# input_2d: [batch*seq, hidden_size], weight_in1: [hidden_size, rank1]
# intermediate_1: [batch*seq, rank1]
intermediate_1 = torch.mm(input_2d, weight_in1)

# Lines 349-364: ç¬¬äºŒæ­¥ä½ç§©ä¹˜æ³•ï¼šintermediate_1 @ weight_out1.T
if ctx.dx_direct_sparse != 3:  # éœ€è¦ç¨€ç–æ€§è·Ÿè¸ªç”¨äºsplit-GEMM
    layer_id_y1 = f"lowrank_layer1_{id(ctx)}"  # Line 351: åˆ›å»ºå”¯ä¸€å±‚ID
    
    # Lines 356-359: ä½¿ç”¨èåˆkernelè®¡ç®—GEMMå¹¶è·Ÿè¸ªç¨€ç–æ€§
    # weight_out1: [intermediate_size, rank1], weight_out1.T: [rank1, intermediate_size]
    y1, _ = fused_gemm_forward_with_sparsity(
        intermediate_1,  # [batch*seq, rank1]
        weight_out1.T,   # [rank1, intermediate_size]
        layer_id_y1,     # å±‚IDç”¨äºç¼“å­˜ç¨€ç–æ€§
        activation_relu2=False,  # æ­¤å¤„ä¸åº”ç”¨ReLUÂ²
        sparsity_threshold=0.95  # 95%åˆ—æ ‡è®°ä¸ºç¨€ç–
    )
    ctx.layer_id_y1 = layer_id_y1  # Line 360: ä¿å­˜å±‚IDä¾›åå‘ä¼ æ’­ä½¿ç”¨
else:
    # Lines 362-363: æ ‡å‡†çŸ©é˜µä¹˜æ³•ï¼Œä¸è·Ÿè¸ªç¨€ç–æ€§
    y1 = torch.mm(intermediate_1, weight_out1.T)  # [batch*seq, intermediate_size]
    ctx.layer_id_y1 = None

# Lines 366-367: æ·»åŠ åç½®ï¼ˆå¦‚æœæœ‰ï¼‰
if bias1 is not None:
    y1 = y1 + bias1
```

### ReLUÂ²æ¿€æ´»å’Œç¨€ç–æ€§è·Ÿè¸ª (Lines 369-400)

```python
if ctx.dx_direct_sparse != 3:  # Line 370: éœ€è¦ç¨€ç–æ€§è·Ÿè¸ª
    from triton_relu2_sparsity import relu2_with_sparsity  # Line 371
    from fused_sparsity_ops import sparsity_tracker  # Line 372
    
    # Line 375: è®¡ç®—ReLUÂ²å¹¶åŒæ—¶è·Ÿè¸ªåˆ—ç¨€ç–æ€§ï¼ˆèåˆè®¡ç®—ï¼Œå‡ ä¹æ— é¢å¤–å¼€é”€ï¼‰
    y2, col_sparsity = relu2_with_sparsity(y1)
    # y2 = ReLUÂ²(y1) = max(0, y1)Â²
    # col_sparsity[j] = è¯¥åˆ—ä¸­é›¶å…ƒç´ çš„æ¯”ä¾‹
    
    layer_id_y2 = f"lowrank_layer2_{id(ctx)}"  # Line 378: ç¬¬äºŒå±‚çš„å”¯ä¸€ID
    num_features = col_sparsity.shape[0]  # Line 379: ç‰¹å¾æ•°ï¼ˆintermediate_sizeï¼‰
    num_sparse = int(0.95 * num_features)  # Line 380: 95%çš„åˆ—æ ‡è®°ä¸ºç¨€ç–
    
    # Lines 383-384: ä½¿ç”¨å¿«é€Ÿé˜ˆå€¼åˆ†åŒºæ‰¾å‡ºæœ€ç¨€ç–çš„95%åˆ—
    from triton_cheap_argsort import fast_threshold_partition
    sparse_mask = fast_threshold_partition(col_sparsity, 0.95)
    # sparse_mask[j] = True è¡¨ç¤ºç¬¬jåˆ—æ˜¯ç¨€ç–çš„ï¼ˆç¨€ç–åº¦æœ€é«˜çš„95%ï¼‰
    
    # Line 387: å­˜å‚¨ç¨€ç–æ€§ä¿¡æ¯ä¾›åå‘ä¼ æ’­ä½¿ç”¨
    sparsity_tracker.store_sparsity(layer_id_y2, col_sparsity, sparse_mask)
    ctx.layer_id_y2 = layer_id_y2  # Line 388
else:
    # Lines 391-393: æ ‡å‡†ReLUÂ²è®¡ç®—ï¼Œä¸è·Ÿè¸ªç¨€ç–æ€§
    relu_y1 = F.relu(y1)  # ReLU(y1)
    y2 = relu_y1 * relu_y1  # ReLUÂ²(y1)
    ctx.layer_id_y2 = None

# Lines 397-399: è®°å½•ç¨€ç–æ€§ç»Ÿè®¡ï¼ˆç”¨äºç›‘æ§ï¼Œå¯é€‰ï¼‰
if hasattr(ActivationSparse2to4LowRankFunction, '_wandb_sparsityrelu_enabled') \
   and ActivationSparse2to4LowRankFunction._wandb_sparsityrelu_enabled:
    ActivationSparse2to4LowRankFunction._record_activation_sparsity_static(y2)
```

### Dense Warmupå¤„ç† (Lines 404-420)

```python
# Line 404: æ£€æŸ¥æ˜¯å¦åœ¨warmupé˜¶æ®µ
if ActivationSparse2to4LowRankFunction._training_step < \
   ActivationSparse2to4LowRankFunction._warmup_steps:
    # WarmupæœŸé—´ä½¿ç”¨å¯†é›†è®¡ç®—ï¼Œä¸åº”ç”¨2:4ç¨€ç–
    
    # Line 407: ç¬¬äºŒä¸ªä½ç§©å±‚ç¬¬ä¸€æ­¥ï¼šy2 @ weight_in2
    intermediate_2 = torch.mm(y2, weight_in2)  # [batch*seq, rank2]
    
    # Line 408: ç¬¬äºŒä¸ªä½ç§©å±‚ç¬¬äºŒæ­¥ï¼šintermediate_2 @ weight_out2.T
    y3 = torch.mm(intermediate_2, weight_out2.T)  # [batch*seq, hidden_size]
    
    # Lines 409-410: æ·»åŠ åç½®
    if bias2 is not None:
        y3 = y3 + bias2
    
    # Line 414: warmupæœŸé—´ï¼Œy2_sparseå°±æ˜¯y2ï¼ˆæ— ç¨€ç–åŒ–ï¼‰
    y2_sparse = y2
    
    # Line 415: ä¿å­˜æ‰€æœ‰å¼ é‡ä¾›åå‘ä¼ æ’­
    ctx.save_for_backward(input_permuted, weight_in1, weight_out1, 
                         weight_in2, weight_out2, bias1, bias2, 
                         y1, y2, y2_sparse, intermediate_1, intermediate_2)
    ctx.perm = perm  # Line 416
    ctx.inv_perm = inv_perm  # Line 417
    ctx.is_warmup = True  # Line 418: æ ‡è®°ä¸ºwarmupçŠ¶æ€
```

### ç¨€ç–è®­ç»ƒè®¡ç®— (Lines 421-458)

```python
else:  # éwarmupï¼Œåº”ç”¨2:4ç¨€ç–
    # Lines 424-441: æ ¹æ®sparsity_methodåº”ç”¨2:4ç¨€ç–åŒ–
    if sparsity_method == "naive":
        # Line 425: ç®€å•çš„2:4ç¨€ç–ï¼Œæ¯4ä¸ªå…ƒç´ ä¿ç•™æœ€å¤§çš„2ä¸ª
        y2_sparse = apply_naive_2to4_sparsity(y2)
        
    elif sparsity_method == "mvue":
        # Line 427: MVUEï¼ˆæœ€å°æ–¹å·®æ— åä¼°è®¡ï¼‰æ–¹æ³•
        y2_sparse = apply_mvue_2to4_sparsity(y2)
        
    elif sparsity_method == "soft_threshold_weights":
        # Lines 429-430: è½¯é˜ˆå€¼æ–¹æ³•ï¼ŒåŸºäºæƒé‡è®¡ç®—scaling
        layer_id = f"lowrank_{id(ctx)}_layer2"
        y2_sparse = apply_soft_threshold_weights_2to4_sparsity(
            y2, weight_in2, layer_id, is_lowrank=True, weight_out=weight_out2)
        
    elif sparsity_method == "soft_dynamic":
        # Lines 433-439: åŠ¨æ€è½¯é˜ˆå€¼ï¼Œå®šæœŸæ›´æ–°scaling
        layer_id = getattr(ActivationSoftThresholdManager, '_current_layer_id', 0) % 12
        current_step = getattr(ActivationSparse2to4LowRankFunction, '_global_training_step', 0)
        calibration_samples = getattr(ctx, 'calibration_samples', 100)
        
        y2_sparse = apply_soft_threshold_dynamic_activation_2to4_sparsity(
            y2, layer_id, current_step, dynamic_steps, calibration_samples)
        
        # æ›´æ–°å±‚IDè®¡æ•°å™¨
        ActivationSoftThresholdManager._current_layer_id = \
            getattr(ActivationSoftThresholdManager, '_current_layer_id', 0) + 1
    else:
        raise ValueError(f"Unknown sparsity method: {sparsity_method}")
    
    # Line 445: ä½¿ç”¨fake_fp8_mmè¿›è¡Œç¨€ç–çŸ©é˜µä¹˜æ³•ï¼ˆåˆ©ç”¨GPUçš„2:4ç¨€ç–åŠ é€Ÿï¼‰
    # y2_sparseå·²ç»æ˜¯2:4ç¨€ç–æ ¼å¼
    intermediate_2 = fake_fp8_mm(y2_sparse, weight_in2, torch.float8_e4m3fn)
    
    # Line 448: è®¡ç®—æœ€ç»ˆè¾“å‡º
    y3 = torch.mm(intermediate_2.to(weight_out2.dtype), weight_out2.T)
    
    # Lines 450-451: æ·»åŠ åç½®
    if bias2 is not None:
        y3 = y3 + bias2
    
    # Line 454: ä¿å­˜å¼ é‡ä¾›åå‘ä¼ æ’­
    ctx.save_for_backward(input_permuted, weight_in1, weight_out1, 
                         weight_in2, weight_out2, bias1, bias2, 
                         y1, y2, y2_sparse, intermediate_1, intermediate_2)
    ctx.perm = perm
    ctx.inv_perm = inv_perm
    ctx.is_warmup = False  # Line 457: æ ‡è®°ä¸ºéwarmup
```

### è¾“å‡ºå¤„ç†å’Œé€†ç½®æ¢ (Lines 460-467)

```python
# Line 461: é‡å¡‘å›3Då¼ é‡
# [batch*seq, hidden_size] -> [batch_size, seq_len, hidden_size]
y3_reshaped = y3.view(batch_size, seq_len, hidden_size)

# Lines 462-465: åº”ç”¨é€†ç½®æ¢æ¢å¤åŸå§‹tokené¡ºåº
if enable_permute and inv_perm is not None:
    # inv_permå°†tokenæ¢å¤åˆ°åŸå§‹ä½ç½®
    output = y3_reshaped[:, inv_perm, :]
else:
    output = y3_reshaped

return output  # Line 467
```

---

## Backwardæ–¹æ³•å®Œæ•´é€è¡Œè§£é‡Š

### åˆå§‹åŒ–å’Œæ¢¯åº¦ç½®æ¢ (Lines 471-488)

```python
@staticmethod
@custom_bwd  # Line 470: PyTorchè‡ªå®šä¹‰åå‘ä¼ æ’­è£…é¥°å™¨
def backward(ctx, grad_output):
    # Line 475: æ¢å¤å‰å‘ä¼ æ’­ä¿å­˜çš„å¼ é‡
    input_permuted, weight_in1, weight_out1, weight_in2, weight_out2, \
    bias1, bias2, y1, y2, y2_forward, intermediate_1, intermediate_2 = ctx.saved_tensors
    
    perm = ctx.perm  # Line 476: ç½®æ¢ç´¢å¼•
    inv_perm = ctx.inv_perm  # Line 477: é€†ç½®æ¢ç´¢å¼•
    is_warmup = ctx.is_warmup  # Line 478: æ˜¯å¦åœ¨warmupé˜¶æ®µ
    dx_direct_sparse = int(ctx.dx_direct_sparse)  # Line 479: ç¨€ç–ç­–ç•¥
    
    batch_size, seq_len, hidden_size = grad_output.shape  # Line 481
    
    # Lines 484-488: åº”ç”¨ç½®æ¢åˆ°æ¢¯åº¦ï¼ˆä¸å‰å‘ä¼ æ’­ä¿æŒä¸€è‡´ï¼‰
    if perm is not None:
        grad_output_permuted = grad_output[:, perm, :]  # ç½®æ¢æ¢¯åº¦
    else:
        grad_output_permuted = grad_output
    
    # é‡å¡‘ä¸º2D
    dy3 = grad_output_permuted.view(-1, grad_output_permuted.shape[-1])  # [batch*seq, hidden_size]
```

### ç¬¬äºŒä¸ªä½ç§©å±‚æ¢¯åº¦è®¡ç®— (Lines 490-497)

```python
# Line 494: è®¡ç®—intermediate_2çš„æ¢¯åº¦
# y3 = intermediate_2 @ weight_out2.T
# å› æ­¤ï¼šd_intermediate_2 = dy3 @ weight_out2
d_intermediate_2 = torch.mm(dy3.to(weight_out2.dtype), weight_out2)  # [batch*seq, rank2]

# Line 497: è®¡ç®—y2çš„æ¢¯åº¦
# intermediate_2 = y2 @ weight_in2
# å› æ­¤ï¼šdy2 = d_intermediate_2 @ weight_in2.T
dy2 = torch.mm(d_intermediate_2, weight_in2.T)  # [batch*seq, intermediate_size]
```

### ReLUÂ²æ¢¯åº¦è®¡ç®— (Lines 499-504)

```python
# ReLUÂ²çš„å¯¼æ•°ï¼šd/dx[ReLUÂ²(x)] = 2*ReLU(x)ï¼ˆå½“x>0æ—¶ä¸º2xï¼Œå¦åˆ™ä¸º0ï¼‰
relu_y1 = F.relu(y1)  # Line 503: è®¡ç®—ReLU(y1)
dy1 = 2 * dy2 * relu_y1  # Line 504: åº”ç”¨é“¾å¼æ³•åˆ™
```

### WarmupæœŸé—´çš„æ¢¯åº¦è®¡ç®— (Lines 509-539)

```python
if is_warmup:  # Line 509: Dense warmupï¼Œæ ‡å‡†æ¢¯åº¦è®¡ç®—
    
    if ctx.needs_input_grad[0]:  # Line 511: è¾“å…¥æ¢¯åº¦
        # dx = dy1 @ weight_out1 @ weight_in1.T
        d_intermediate_1 = torch.mm(dy1, weight_out1)  # Line 513
        grad_input_2d = torch.mm(d_intermediate_1, weight_in1.T)  # Line 514
        
        # é‡å¡‘å¹¶åº”ç”¨é€†ç½®æ¢
        grad_input_permuted = grad_input_2d.view(batch_size, seq_len, hidden_size)
        if inv_perm is not None:
            grad_input = grad_input_permuted[:, inv_perm, :]  # Line 517: æ¢å¤åŸå§‹é¡ºåº
        else:
            grad_input = grad_input_permuted
    
    # Line 523: weight_in1æ¢¯åº¦ = input.T @ (dy1 @ weight_out1)
    if ctx.needs_input_grad[1]:
        grad_weight_in1 = torch.mm(
            input_permuted.view(-1, input_permuted.shape[-1]).T, 
            torch.mm(dy1, weight_out1))
    
    # Line 526: weight_out1æ¢¯åº¦ = dy1.T @ intermediate_1
    if ctx.needs_input_grad[2]:
        grad_weight_out1 = torch.mm(dy1.T, intermediate_1.to(dy1.dtype))
    
    # Line 530: weight_in2æ¢¯åº¦ = y2.T @ d_intermediate_2
    if ctx.needs_input_grad[3]:
        grad_weight_in2 = torch.mm(y2.T, d_intermediate_2.to(y2.dtype))
    
    # Line 533: weight_out2æ¢¯åº¦ = dy3.T @ intermediate_2
    if ctx.needs_input_grad[4]:
        grad_weight_out2 = torch.mm(dy3.T, intermediate_2.to(dy3.dtype))
    
    # Lines 536-539: åç½®æ¢¯åº¦ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if ctx.needs_input_grad[5] and bias1 is not None:
        grad_bias1 = dy1.sum(0)  # å¯¹æ‰¹æ¬¡ç»´åº¦æ±‚å’Œ
    if ctx.needs_input_grad[6] and bias2 is not None:
        grad_bias2 = dy3.sum(0)
```

### ç¨€ç–è®­ç»ƒæ¢¯åº¦è®¡ç®— (Lines 540-611)

#### è¾“å…¥æ¢¯åº¦dxè®¡ç®— (Lines 542-562)

```python
if ctx.needs_input_grad[0]:
    if dx_direct_sparse == 3:  # Line 543: å®Œå…¨å¯†é›†è®¡ç®—
        d_intermediate_1 = torch.mm(dy1, weight_out1)
        grad_input_2d = torch.mm(d_intermediate_1.to(weight_in1.dtype), weight_in1.T)
        
    elif dx_direct_sparse == 2:  # Line 546: å…¨éƒ¨ç”¨2:4ç¨€ç–
        dy1_naive_sparse = apply_naive_2to4_sparsity(dy1)  # Line 547: åº”ç”¨2:4ç¨€ç–
        # ä½¿ç”¨fake_fp8_mmè¿›è¡Œç¨€ç–çŸ©é˜µä¹˜æ³•
        d_intermediate_1 = fake_fp8_mm(dy1_naive_sparse, weight_out1, torch.float8_e4m3fn)
        grad_input_2d = torch.mm(d_intermediate_1.to(weight_in1.dtype), weight_in1.T)
        
    else:  # dx_direct_sparse == 1: Split-GEMMï¼ˆä½†å½“å‰å®ç°ä½¿ç”¨naiveï¼‰
        # Lines 552-554: åº”è¯¥ä½¿ç”¨ç¼“å­˜çš„ç¨€ç–æ€§ï¼Œä½†å½“å‰ä½¿ç”¨naiveæ–¹æ³•
        dy1_naive_sparse = apply_naive_2to4_sparsity(dy1)
        d_intermediate_1 = fake_fp8_mm(dy1_naive_sparse, weight_out1, torch.float8_e4m3fn)
        grad_input_2d = torch.mm(d_intermediate_1.to(weight_in1.dtype), weight_in1.T)
        
        # æ³¨é‡Šæ‰çš„æ­£ç¡®å®ç°ï¼ˆåº”è¯¥ä½¿ç”¨ï¼‰ï¼š
        # d_intermediate_1 = compute_split_gemm_lowrank_intermediate(dy1, weight_out1, ctx.layer_id_y1)
        # grad_input_2d = torch.mm(d_intermediate_1.to(weight_in1.dtype), weight_in1.T)
    
    # Lines 558-562: é‡å¡‘å¹¶åº”ç”¨é€†ç½®æ¢
    grad_input_permuted = grad_input_2d.view(batch_size, seq_len, hidden_size)
    if inv_perm is not None:
        grad_input = grad_input_permuted[:, inv_perm, :]
    else:
        grad_input = grad_input_permuted
```

#### ç¬¬ä¸€å±‚æƒé‡æ¢¯åº¦ (Lines 565-590)

```python
# weight_in1æ¢¯åº¦è®¡ç®—
if ctx.needs_input_grad[1]:  # Line 566
    if dx_direct_sparse == 3:  # Line 567: å¯†é›†è®¡ç®—
        d_intermediate_1_for_w_in1 = torch.mm(dy1, weight_out1)
        
    elif dx_direct_sparse == 2:  # Line 569: å…¨ç¨€ç–
        dy1_sparse = apply_naive_2to4_sparsity(dy1)  # Line 570
        d_intermediate_1_for_w_in1 = fake_fp8_mm(dy1_sparse, weight_out1, torch.float8_e4m3fn)
        
    else:  # dx_direct_sparse == 1: Split-GEMM
        # Line 575: ä½¿ç”¨ç¼“å­˜çš„ç¨€ç–æ€§ä¿¡æ¯
        d_intermediate_1_for_w_in1 = compute_split_gemm_lowrank_intermediate(
            dy1, weight_out1, ctx.layer_id_y1)
    
    # Line 576: è®¡ç®—æ¢¯åº¦
    grad_weight_in1 = torch.mm(
        input_permuted.view(-1, input_permuted.shape[-1]).T, 
        d_intermediate_1_for_w_in1.to(input_permuted.dtype))

# weight_out1æ¢¯åº¦è®¡ç®—
if ctx.needs_input_grad[2]:  # Line 578
    if dx_direct_sparse == 3:  # Line 579: å¯†é›†
        grad_weight_out1 = torch.mm(dy1.T, intermediate_1.to(dy1.dtype))
        
    elif dx_direct_sparse == 2:  # Line 581: ç¨€ç–ï¼ˆæ³¨æ„ï¼šdy1_sparseåœ¨ä¸Šé¢å·²è®¡ç®—ï¼‰
        grad_weight_out1 = fake_fp8_mm(dy1_sparse.T, intermediate_1, torch.float8_e4m3fn)
        
    else:  # Line 585: Split-GEMM
        # Line 587: åº”ç”¨ç¼“å­˜çš„ç¨€ç–æ¨¡å¼åˆ°dy1
        dy1_split_gemm = apply_split_gemm_to_dy1(dy1, ctx.layer_id_y1)
        # Line 589: dy1_split_gemmæ˜¯2:4ç¨€ç–çš„
        grad_weight_out1 = fake_fp8_mm(dy1_split_gemm.T, intermediate_1, torch.float8_e4m3fn)
```

#### ç¬¬äºŒå±‚æƒé‡æ¢¯åº¦ (Lines 592-605)

```python
# weight_in2æ¢¯åº¦ï¼ˆè¿™æ˜¯Split-GEMMçš„å…³é”®éƒ¨åˆ†ï¼‰
if ctx.needs_input_grad[3]:  # Line 592
    if dx_direct_sparse == 3:  # Line 593: å¯†é›†
        grad_weight_in2 = torch.mm(y2.T, d_intermediate_2.to(y2.dtype))
        
    elif dx_direct_sparse == 2:  # Line 595: ä½¿ç”¨å‰å‘ä¿å­˜çš„ç¨€ç–y2
        # y2_forwardå·²ç»æ˜¯2:4ç¨€ç–æ ¼å¼
        grad_weight_in2 = fake_fp8_mm(y2_forward.T, d_intermediate_2, torch.float8_e4m3fn)
        
    else:  # dx_direct_sparse == 1: Split-GEMM
        # Line 602: ä½¿ç”¨split-GEMMè®¡ç®—weight_in2æ¢¯åº¦
        grad_weight_in2 = compute_split_gemm_dw2_lowrank(
            y2, d_intermediate_2, y2_forward, weight_in2, ctx.layer_id_y2)

# weight_out2æ¢¯åº¦ï¼ˆæ ‡å‡†è®¡ç®—ï¼‰
if ctx.needs_input_grad[4]:  # Line 604
    grad_weight_out2 = torch.mm(dy3.T, intermediate_2.to(dy3.dtype))

# åç½®æ¢¯åº¦
if ctx.needs_input_grad[5] and bias1 is not None:  # Line 607
    grad_bias1 = dy1.sum(0)
if ctx.needs_input_grad[6] and bias2 is not None:  # Line 610
    grad_bias2 = dy3.sum(0)
```

### è¿”å›æ¢¯åº¦ (Line 614)

```python
# è¿”å›æ‰€æœ‰è¾“å…¥å‚æ•°çš„æ¢¯åº¦ï¼ˆ13ä¸ªï¼Œä¸forwardç­¾ååŒ¹é…ï¼‰
return (grad_input, grad_weight_in1, grad_weight_out1, grad_weight_in2, 
        grad_weight_out2, grad_bias1, grad_bias2, 
        None, None, None, None, None, None)  # å6ä¸ªå‚æ•°æ²¡æœ‰æ¢¯åº¦
```

---

## æ ¸å¿ƒKernelå‡½æ•°è¯¦ç»†è§£é‡Š

### 1. fused_gemm_forward_with_sparsity (fused_sparsity_ops.py)

è¿™ä¸ªå‡½æ•°èåˆäº†GEMMè®¡ç®—å’Œç¨€ç–æ€§è·Ÿè¸ªï¼Œæ˜¯forwardä¸­çš„å…³é”®å‡½æ•°ï¼š

```python
def fused_gemm_forward_with_sparsity(x, weight, layer_id, 
                                     activation_relu2=False, 
                                     compute_2to4=False, 
                                     sparsity_threshold=0.95):
    """
    Line 74-81: æ ¸å¿ƒè®¡ç®—é€»è¾‘
    """
    # Line 76: é€‰æ‹©æ¿€æ´»å‡½æ•°ç±»å‹
    activation = 'relu2' if activation_relu2 else 'none'
    
    # Line 77-80: è°ƒç”¨Triton kernelè¿›è¡Œèåˆè®¡ç®—
    # ä¸€æ¬¡kernelè°ƒç”¨å®Œæˆï¼šçŸ©é˜µä¹˜æ³• + å¯é€‰ReLUÂ² + ç¨€ç–æ€§ç»Ÿè®¡
    y, col_sparsity = triton_matmul_with_sparsity(
        x, weight, 
        activation=activation,
        track_sparsity=True  # åœ¨epilogueä¸­å…è´¹è®¡ç®—ç¨€ç–æ€§
    )
    
    # Line 88: ä½¿ç”¨å¿«é€Ÿåˆ†åŒºæ‰¾å‡ºæœ€ç¨€ç–çš„95%åˆ—
    sparse_mask = fast_threshold_partition(col_sparsity, sparsity_threshold)
    
    # Line 91: ç¼“å­˜ç¨€ç–æ€§ä¿¡æ¯
    sparsity_tracker.store_sparsity(layer_id, col_sparsity, sparse_mask)
    
    return y, y  # è¿”å›è¾“å‡ºï¼ˆç¬¬äºŒä¸ªç”¨äº2:4ç¨€ç–ç‰ˆæœ¬ï¼Œç›®å‰æœªå®ç°ï¼‰
```

### 2. triton_matmul_with_sparsity Kernel å®Œæ•´é€è¡Œè§£é‡Š (triton_fused_gemm.py)

è¿™æ˜¯æœ€é‡è¦çš„Triton kernelï¼Œå®ç°äº†èåˆçš„GEMMå’Œç¨€ç–æ€§è®¡ç®—ã€‚è¿™ä¸ªkernelä¸€æ¬¡å®ŒæˆçŸ©é˜µä¹˜æ³•ã€å¯é€‰çš„æ¿€æ´»å‡½æ•°å’Œç¨€ç–æ€§ç»Ÿè®¡ï¼š

```python
@triton.jit  # Line 11: Triton JITç¼–è¯‘è£…é¥°å™¨ï¼Œå°†Pythonå‡½æ•°ç¼–è¯‘æˆGPU kernel
def matmul_kernel_with_sparsity(
    # å†…å­˜æŒ‡é’ˆå‚æ•°
    a_ptr, b_ptr, c_ptr,  # çŸ©é˜µAã€Bã€Cçš„GPUå†…å­˜æŒ‡é’ˆ
    # çŸ©é˜µç»´åº¦
    M, N, K,              # A[M,K] @ B[K,N] = C[M,N]
    # å†…å­˜å¸ƒå±€å‚æ•°ï¼ˆstrideè¡¨ç¤ºåœ¨è¯¥ç»´åº¦ç§»åŠ¨1ä¸ªå•ä½éœ€è¦è·³è¿‡çš„å…ƒç´ æ•°ï¼‰
    stride_am, stride_ak, # AçŸ©é˜µï¼šstride_am=æ¯è¡Œé—´éš”ï¼Œstride_ak=æ¯åˆ—é—´éš”
    stride_bk, stride_bn, # BçŸ©é˜µï¼šstride_bk=æ¯è¡Œé—´éš”ï¼Œstride_bn=æ¯åˆ—é—´éš”
    stride_cm, stride_cn, # CçŸ©é˜µï¼šstride_cm=æ¯è¡Œé—´éš”ï¼Œstride_cn=æ¯åˆ—é—´éš”
    # ç¨€ç–æ€§è·Ÿè¸ª
    col_nnz_ptr,         # æŒ‡å‘å­˜å‚¨æ¯åˆ—éé›¶å…ƒç´ è®¡æ•°çš„å†…å­˜
    # ç¼–è¯‘æ—¶å¸¸é‡ï¼ˆtl.constexprè¡¨ç¤ºè¿™äº›å€¼åœ¨ç¼–è¯‘æ—¶ç¡®å®šï¼Œå¯ç”¨äºä¼˜åŒ–ï¼‰
    BLOCK_SIZE_M: tl.constexpr,  # Mç»´åº¦çš„å—å¤§å°ï¼ˆé€šå¸¸64æˆ–128ï¼‰
    BLOCK_SIZE_N: tl.constexpr,  # Nç»´åº¦çš„å—å¤§å°
    BLOCK_SIZE_K: tl.constexpr,  # Kç»´åº¦çš„å—å¤§å°ï¼ˆé€šå¸¸32æˆ–64ï¼‰
    ACTIVATION: tl.constexpr,     # æ¿€æ´»å‡½æ•°ç±»å‹ï¼ˆ0=none, 1=relu, 2=reluÂ²ï¼‰
):
```

**kernelå†…éƒ¨é€è¡Œè¯¦è§£ï¼š**

```python
    # Lines 29-34: è®¡ç®—å½“å‰çº¿ç¨‹å—ï¼ˆblockï¼‰è´Ÿè´£å¤„ç†çš„çŸ©é˜µåŒºåŸŸ
    pid = tl.program_id(axis=0)  # è·å–å½“å‰çº¿ç¨‹å—çš„å…¨å±€IDï¼ˆ0åˆ°æ€»å—æ•°-1ï¼‰
    
    # è®¡ç®—æ€»å…±éœ€è¦å¤šå°‘ä¸ªå—æ¥è¦†ç›–æ•´ä¸ªè¾“å‡ºçŸ©é˜µ
    # tl.cdivæ˜¯å‘ä¸Šå–æ•´é™¤æ³•ï¼Œä¾‹å¦‚cdiv(10,3)=4
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)  # Mç»´åº¦éœ€è¦çš„å—æ•°
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)  # Nç»´åº¦éœ€è¦çš„å—æ•°
    
    # å°†ä¸€ç»´çš„å—IDæ˜ å°„åˆ°äºŒç»´ç½‘æ ¼åæ ‡
    # ä¾‹å¦‚ï¼šå¦‚æœnum_pid_n=3ï¼Œpid=7ï¼Œåˆ™pid_m=7//3=2, pid_n=7%3=1
    pid_m = pid // num_pid_n  # å½“å‰å—åœ¨Mç»´åº¦çš„ç´¢å¼•ï¼ˆè¡Œå—ç´¢å¼•ï¼‰
    pid_n = pid % num_pid_n   # å½“å‰å—åœ¨Nç»´åº¦çš„ç´¢å¼•ï¼ˆåˆ—å—ç´¢å¼•ï¼‰
    
    # Lines 37-39: åˆ›å»ºå—å†…æ¯ä¸ªçº¿ç¨‹è´Ÿè´£çš„å…ƒç´ åç§»
    # tl.arange(0, N)åˆ›å»º[0,1,2,...,N-1]çš„å‘é‡
    # è¿™äº›åç§»ç”¨äºè®¡ç®—æ¯ä¸ªçº¿ç¨‹è¦è®¿é—®çš„å†…å­˜åœ°å€
    
    # Mç»´åº¦åç§»ï¼šè®¡ç®—è¿™ä¸ªå—è¦å¤„ç†çš„è¡Œç´¢å¼•
    # ä¾‹å¦‚ï¼šå¦‚æœpid_m=2, BLOCK_SIZE_M=64ï¼Œåˆ™å¤„ç†è¡Œ128-191
    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M
    
    # Nç»´åº¦åç§»ï¼šè®¡ç®—è¿™ä¸ªå—è¦å¤„ç†çš„åˆ—ç´¢å¼•  
    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N
    
    # Kç»´åº¦åç§»ï¼šç”¨äºéå†Kç»´åº¦è¿›è¡Œç´¯åŠ 
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    
    # Lines 41-42: è®¡ç®—åˆå§‹å†…å­˜åœ°å€ï¼ˆæŒ‡é’ˆç®—æœ¯ï¼‰
    # ä½¿ç”¨å¹¿æ’­åˆ›å»º2Dåœ°å€çŸ©é˜µï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå†…å­˜åœ°å€
    
    # AçŸ©é˜µçš„åœ°å€è®¡ç®—ï¼š
    # offs_am[:, None]å°†[BLOCK_SIZE_M]æ‰©å±•ä¸º[BLOCK_SIZE_M, 1]
    # offs_k[None, :]å°†[BLOCK_SIZE_K]æ‰©å±•ä¸º[1, BLOCK_SIZE_K]
    # å¹¿æ’­åå¾—åˆ°[BLOCK_SIZE_M, BLOCK_SIZE_K]çš„åœ°å€çŸ©é˜µ
    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)
    
    # BçŸ©é˜µçš„åœ°å€è®¡ç®—ï¼šç±»ä¼¼åœ°åˆ›å»º[BLOCK_SIZE_K, BLOCK_SIZE_N]çš„åœ°å€çŸ©é˜µ
    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)
    
    # Line 45: åˆå§‹åŒ–ç´¯åŠ å™¨
    # ä½¿ç”¨float32è€Œéfloat16/bfloat16æ˜¯ä¸ºäº†é¿å…ç´¯åŠ è¿‡ç¨‹ä¸­çš„ç²¾åº¦æŸå¤±
    # æœ€åä¼šè½¬æ¢å›éœ€è¦çš„ç²¾åº¦
    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
    
    # Lines 48-58: ä¸»å¾ªç¯ - åˆ†å—çŸ©é˜µä¹˜æ³•çš„æ ¸å¿ƒ
    # å°†Kç»´åº¦åˆ†æˆå¤šä¸ªBLOCK_SIZE_Kå¤§å°çš„å—ï¼Œé€å—ç´¯åŠ 
    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        # Lines 50-51: ä»å…¨å±€å†…å­˜åŠ è½½Aå’ŒBçš„å­å—åˆ°å…±äº«å†…å­˜/å¯„å­˜å™¨
        # maskå‚æ•°ç”¨äºè¾¹ç•Œæ£€æŸ¥ï¼šå½“kæ˜¯æœ€åä¸€å—ä¸”Kä¸èƒ½è¢«BLOCK_SIZE_Kæ•´é™¤æ—¶
        # è¶…å‡ºè¾¹ç•Œçš„å…ƒç´ ç”¨other=0.0å¡«å……ï¼Œä¸å½±å“ç»“æœ
        
        # åŠ è½½Açš„å—ï¼š[BLOCK_SIZE_M, BLOCK_SIZE_K]
        a = tl.load(a_ptrs, 
                   mask=offs_k[None, :] < K - k * BLOCK_SIZE_K,  # è¾¹ç•Œæ£€æŸ¥
                   other=0.0)  # è¶Šç•Œä½ç½®å¡«0
        
        # åŠ è½½Bçš„å—ï¼š[BLOCK_SIZE_K, BLOCK_SIZE_N]
        b = tl.load(b_ptrs, 
                   mask=offs_k[:, None] < K - k * BLOCK_SIZE_K,
                   other=0.0)
        
        # Line 54: æ‰§è¡Œå—çŸ©é˜µä¹˜æ³•å¹¶ç´¯åŠ 
        # tl.dotä¼šè‡ªåŠ¨ä½¿ç”¨Tensor Coreï¼ˆå¦‚æœå¯ç”¨ï¼‰è¿›è¡ŒåŠ é€Ÿ
        # è¿™æ˜¯æ•´ä¸ªkernelæœ€é‡è¦çš„è®¡ç®—
        accumulator += tl.dot(a, b)  # [M_BLOCK, K_BLOCK] @ [K_BLOCK, N_BLOCK]
        
        # Lines 57-58: ç§»åŠ¨æŒ‡é’ˆåˆ°Kç»´åº¦çš„ä¸‹ä¸€ä¸ªå—
        # æ¯æ¬¡è¿­ä»£å¤„ç†Kç»´åº¦çš„BLOCK_SIZE_Kä¸ªå…ƒç´ 
        a_ptrs += BLOCK_SIZE_K * stride_ak  # AçŸ©é˜µæŒ‡é’ˆå‘å³ç§»åŠ¨
        b_ptrs += BLOCK_SIZE_K * stride_bk  # BçŸ©é˜µæŒ‡é’ˆå‘ä¸‹ç§»åŠ¨
    
    # Lines 61-66: åº”ç”¨æ¿€æ´»å‡½æ•°ï¼ˆå¯é€‰çš„åå¤„ç†ï¼‰
    c = accumulator  # ç´¯åŠ ç»“æœ
    
    if ACTIVATION == 1:  # æ ‡å‡†ReLU: max(0, x)
        c = tl.maximum(c, 0.0)
        
    elif ACTIVATION == 2:  # ReLUÂ²: ReLU(x)Â²
        # å…ˆåˆ¤æ–­å“ªäº›å…ƒç´ å¤§äº0
        relu_mask = c > 0
        # å¯¹å¤§äº0çš„å…ƒç´ å¹³æ–¹ï¼Œå°äºç­‰äº0çš„è®¾ä¸º0
        # è¿™æ¯”å…ˆReLUå†å¹³æ–¹æ›´é«˜æ•ˆï¼ˆé¿å…äº†0Â²çš„è®¡ç®—ï¼‰
        c = tl.where(relu_mask, c * c, 0.0)
    
    # Lines 69-73: å°†è®¡ç®—ç»“æœå†™å›å…¨å±€å†…å­˜
    # é‡æ–°è®¡ç®—è¾“å‡ºä½ç½®ï¼ˆå› ä¸ºä¹‹å‰çš„offs_am/offs_bnå¯èƒ½è¢«%M/%Nä¿®æ”¹äº†ï¼‰
    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
    
    # è®¡ç®—è¾“å‡ºåœ°å€çŸ©é˜µ[BLOCK_SIZE_M, BLOCK_SIZE_N]
    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]
    
    # åˆ›å»ºè¾¹ç•Œæ£€æŸ¥maskï¼Œç¡®ä¿ä¸å†™å…¥è¶Šç•Œå†…å­˜
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)
    
    # æ¡ä»¶å­˜å‚¨ï¼šåªå†™å…¥mask=Trueçš„ä½ç½®
    tl.store(c_ptrs, c, mask=c_mask)
    
    # Lines 76-80: ç»Ÿè®¡æ¯åˆ—çš„éé›¶å…ƒç´ æ•°ï¼ˆèåˆçš„ç¨€ç–æ€§è®¡ç®—ï¼‰
    # è¿™æ˜¯å…³é”®ä¼˜åŒ–ï¼šåœ¨GEMMçš„epilogueä¸­"å…è´¹"è®¡ç®—ç¨€ç–æ€§
    if col_nnz_ptr:  # å¦‚æœæä¾›äº†ç¨€ç–æ€§ç»Ÿè®¡æŒ‡é’ˆ
        # åˆ›å»º0/1çŸ©é˜µï¼šéé›¶å…ƒç´ ä¸º1ï¼Œé›¶å…ƒç´ ä¸º0
        nnz = (c != 0).to(tl.int32)
        
        # å¯¹æ¯åˆ—æ±‚å’Œï¼Œå¾—åˆ°æ¯åˆ—çš„éé›¶å…ƒç´ æ•°
        # axis=0è¡¨ç¤ºæ²¿ç€è¡Œç»´åº¦æ±‚å’Œï¼ˆå‹ç¼©è¡Œï¼Œä¿ç•™åˆ—ï¼‰
        col_nnz = tl.sum(nnz, axis=0)  # [BLOCK_SIZE_N]çš„å‘é‡
        
        # è®¡ç®—è¦å†™å…¥çš„å…¨å±€å†…å­˜åœ°å€
        col_nnz_ptrs = col_nnz_ptr + offs_cn
        
        # ä½¿ç”¨åŸå­åŠ æ“ä½œæ›´æ–°å…¨å±€è®¡æ•°å™¨
        # åŸå­æ“ä½œæ˜¯å¿…è¦çš„ï¼Œå› ä¸ºå¤šä¸ªå—å¯èƒ½å¤„ç†åŒä¸€åˆ—çš„ä¸åŒè¡Œ
        # ä¾‹å¦‚ï¼šå—0å¤„ç†åˆ—jçš„è¡Œ0-63ï¼Œå—1å¤„ç†åˆ—jçš„è¡Œ64-127
        tl.atomic_add(col_nnz_ptrs, col_nnz, mask=offs_cn < N)
```

### 3. triton_matmul_with_sparsity åŒ…è£…å‡½æ•°å®Œæ•´è§£é‡Š (triton_fused_gemm.py)

è¿™æ˜¯è°ƒç”¨Triton kernelçš„PythonåŒ…è£…å‡½æ•°ï¼Œè´Ÿè´£å‡†å¤‡æ•°æ®ã€é…ç½®å‚æ•°å’Œè°ƒç”¨kernelï¼š

```python
def triton_matmul_with_sparsity(a, b, activation='none', track_sparsity=True):
    """
    è®¡ç®— C = A @ Bï¼Œå¯é€‰æ¿€æ´»å‡½æ•°å’Œç¨€ç–æ€§è·Ÿè¸ª
    è¿™æ˜¯ç”¨æˆ·æ¥å£å‡½æ•°ï¼Œå°è£…äº†åº•å±‚çš„Triton kernelè°ƒç”¨
    
    Args:
        a: è¾“å…¥çŸ©é˜µ [M, K]
        b: æƒé‡çŸ©é˜µ [K, N]  
        activation: 'none', 'relu', æˆ– 'relu2'
        track_sparsity: æ˜¯å¦è®¡ç®—åˆ—ç¨€ç–æ€§
    
    Returns:
        c: è¾“å‡ºçŸ©é˜µ [M, N]
        col_sparsity: æ¯åˆ—çš„ç¨€ç–ç‡ [N] (å¦‚æœtrack_sparsity=True)
    """
    # Line 98: æ£€æŸ¥çŸ©é˜µç»´åº¦æ˜¯å¦åŒ¹é…
    assert a.shape[1] == b.shape[0], "Matrix dimensions must match"
    
    # Lines 99-100: æå–çŸ©é˜µç»´åº¦
    M, K = a.shape  # Mæ˜¯æ‰¹æ¬¡*åºåˆ—é•¿åº¦ï¼ŒKæ˜¯éšè—ç»´åº¦
    K, N = b.shape  # Næ˜¯è¾“å‡ºç»´åº¦
    
    # Line 103: åˆ†é…è¾“å‡ºå†…å­˜
    # ä½¿ç”¨emptyè€Œä¸æ˜¯zeroså¯ä»¥é¿å…ä¸å¿…è¦çš„åˆå§‹åŒ–å¼€é”€
    c = torch.empty((M, N), device=a.device, dtype=a.dtype)
    
    # Line 106: å‡†å¤‡ç¨€ç–æ€§ç»Ÿè®¡å†…å­˜ï¼ˆå¯é€‰ï¼‰
    # å¦‚æœéœ€è¦è·Ÿè¸ªç¨€ç–æ€§ï¼Œåˆ†é…ä¸€ä¸ªè®¡æ•°å™¨æ•°ç»„
    # æ¯ä¸ªå…ƒç´ è®°å½•å¯¹åº”åˆ—çš„éé›¶å…ƒç´ æ•°é‡
    col_nnz = torch.zeros((N,), device=a.device, dtype=torch.int32) if track_sparsity else None
    
    # Lines 109-110: å°†æ¿€æ´»å‡½æ•°å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•°å­—ä»£ç 
    # Triton kernelä½¿ç”¨æ•°å­—ä»£ç è€Œéå­—ç¬¦ä¸²æ¥é€‰æ‹©æ¿€æ´»å‡½æ•°
    activation_map = {'none': 0, 'relu': 1, 'relu2': 2}
    activation_code = activation_map.get(activation, 0)  # é»˜è®¤ä¸º0ï¼ˆæ— æ¿€æ´»ï¼‰
    
    # Lines 113-115: å®šä¹‰çº¿ç¨‹å—å¤§å°
    # è¿™äº›å€¼ç»è¿‡è°ƒä¼˜ï¼Œå¹³è¡¡äº†å¹¶è¡Œåº¦å’Œå…±äº«å†…å­˜ä½¿ç”¨
    BLOCK_SIZE_M = 64   # Mç»´åº¦çš„å—å¤§å°ï¼ˆé€šå¸¸64æˆ–128ï¼‰
    BLOCK_SIZE_N = 64   # Nç»´åº¦çš„å—å¤§å°
    BLOCK_SIZE_K = 32   # Kç»´åº¦çš„å—å¤§å°ï¼ˆç´¯åŠ ç»´åº¦ï¼Œé€šå¸¸è¾ƒå°ï¼‰
    
    # Lines 118-120: å®šä¹‰kernelå¯åŠ¨ç½‘æ ¼
    # gridæ˜¯ä¸€ä¸ªlambdaå‡½æ•°ï¼Œè¿”å›éœ€è¦çš„çº¿ç¨‹å—æ•°é‡
    # triton.cdivæ˜¯å‘ä¸Šå–æ•´é™¤æ³•ï¼Œç¡®ä¿è¦†ç›–æ‰€æœ‰å…ƒç´ 
    grid = lambda META: (
        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),
        # æ€»çº¿ç¨‹å—æ•° = Mæ–¹å‘å—æ•° * Næ–¹å‘å—æ•°
    )
    
    # Lines 122-133: å¯åŠ¨Triton kernel
    matmul_kernel_with_sparsity[grid](  # [grid]æŒ‡å®šçº¿ç¨‹å—æ•°é‡
        # çŸ©é˜µæŒ‡é’ˆï¼ˆTritonä¼šè‡ªåŠ¨è·å–GPUå†…å­˜åœ°å€ï¼‰
        a, b, c,
        # çŸ©é˜µç»´åº¦
        M, N, K,
        # å†…å­˜å¸ƒå±€å‚æ•°ï¼ˆstrideè¡¨ç¤ºåœ¨è¯¥ç»´åº¦ç§»åŠ¨1éœ€è¦è·³è¿‡çš„å…ƒç´ æ•°ï¼‰
        a.stride(0), a.stride(1),  # AçŸ©é˜µçš„è¡Œæ­¥é•¿å’Œåˆ—æ­¥é•¿
        b.stride(0), b.stride(1),  # BçŸ©é˜µçš„è¡Œæ­¥é•¿å’Œåˆ—æ­¥é•¿  
        c.stride(0), c.stride(1),  # CçŸ©é˜µçš„è¡Œæ­¥é•¿å’Œåˆ—æ­¥é•¿
        # ç¨€ç–æ€§è·Ÿè¸ªæŒ‡é’ˆ
        col_nnz if track_sparsity else None,
        # ç¼–è¯‘æ—¶å¸¸é‡ï¼ˆç”¨äºkernelä¼˜åŒ–ï¼‰
        BLOCK_SIZE_M=BLOCK_SIZE_M,
        BLOCK_SIZE_N=BLOCK_SIZE_N,
        BLOCK_SIZE_K=BLOCK_SIZE_K,
        ACTIVATION=activation_code,
    )
    
    # Lines 136-140: è®¡ç®—ç¨€ç–ç‡ï¼ˆåå¤„ç†ï¼‰
    if track_sparsity:
        # å°†éé›¶è®¡æ•°è½¬æ¢ä¸ºç¨€ç–ç‡
        # ç¨€ç–ç‡ = 1 - (éé›¶å…ƒç´ æ•° / æ€»å…ƒç´ æ•°)
        col_sparsity = 1.0 - (col_nnz.float() / M)
        return c, col_sparsity
    else:
        return c, None
```

**å…³é”®ç‚¹è§£é‡Šï¼š**

1. **å†…å­˜åˆ†é…ç­–ç•¥**ï¼š
   - ä½¿ç”¨`torch.empty`è€Œé`torch.zeros`é¿å…åˆå§‹åŒ–å¼€é”€
   - è¾“å‡ºçŸ©é˜µcåœ¨kernelä¸­ä¼šè¢«å®Œå…¨è¦†ç›–ï¼Œä¸éœ€è¦é¢„åˆå§‹åŒ–

2. **æ­¥é•¿ï¼ˆstrideï¼‰çš„å«ä¹‰**ï¼š
   - `a.stride(0)`ï¼šåœ¨aä¸­ä»ç¬¬iè¡Œç§»åŠ¨åˆ°ç¬¬i+1è¡Œéœ€è¦è·³è¿‡çš„å…ƒç´ æ•°
   - `a.stride(1)`ï¼šåœ¨aä¸­ä»ç¬¬jåˆ—ç§»åŠ¨åˆ°ç¬¬j+1åˆ—éœ€è¦è·³è¿‡çš„å…ƒç´ æ•°
   - å¯¹äºè¿ç»­å­˜å‚¨çš„çŸ©é˜µï¼Œé€šå¸¸stride(0)=åˆ—æ•°ï¼Œstride(1)=1

3. **çº¿ç¨‹å—å¤§å°é€‰æ‹©**ï¼š
   - BLOCK_SIZE_Kè¾ƒå°ï¼ˆ32ï¼‰æ˜¯å› ä¸ºè¿™æ˜¯ç´¯åŠ ç»´åº¦ï¼Œå¤ªå¤§ä¼šå¢åŠ å¯„å­˜å™¨å‹åŠ›
   - BLOCK_SIZE_M/Né€‰æ‹©64æ˜¯å¹³è¡¡å¹¶è¡Œåº¦å’Œèµ„æºä½¿ç”¨çš„ç»“æœ

4. **ç¨€ç–æ€§è®¡ç®—**ï¼š
   - kernelä¸­ä½¿ç”¨åŸå­æ“ä½œç´¯åŠ æ¯åˆ—çš„éé›¶å…ƒç´ æ•°
   - Pythonç«¯å°†è®¡æ•°è½¬æ¢ä¸ºç¨€ç–ç‡ï¼ˆ0åˆ°1ä¹‹é—´ï¼‰

5. **gridè®¡ç®—**ï¼š
   - ä½¿ç”¨lambdaå»¶è¿Ÿè®¡ç®—ï¼Œå…è®¸Tritonåœ¨ç¼–è¯‘æ—¶ä¼˜åŒ–
   - æ€»çº¿ç¨‹å—æ•°å¿…é¡»è¦†ç›–æ•´ä¸ªè¾“å‡ºçŸ©é˜µ

### 4. fast_threshold_partition (triton_cheap_argsort.py)

å¿«é€Ÿæ‰¾å‡ºæœ€ç¨€ç–çš„k%åˆ—ï¼Œé¿å…å®Œæ•´æ’åºï¼Œè¿™æ˜¯ä¼˜åŒ–çš„å…³é”®ï¼š

```python
def fast_threshold_partition(col_sparsity, sparsity_ratio=0.95):
    """
    Line 115-152: ä¸»è¦é€»è¾‘
    æ—¶é—´å¤æ‚åº¦ï¼šO(n) vs å®Œæ•´æ’åºçš„O(n log n)
    """
    N = col_sparsity.shape[0]  # Line 127: åˆ—æ•°
    num_sparse = int(sparsity_ratio * N)  # Line 128: è¦æ ‡è®°ä¸ºç¨€ç–çš„åˆ—æ•°
    
    if num_sparse == 0:  # Line 130-131: è¾¹ç•Œæƒ…å†µ
        return torch.zeros(N, dtype=torch.bool, device=col_sparsity.device)
    
    if num_sparse < N:  # Line 135: éœ€è¦åˆ†åŒº
        # Line 137: ä½¿ç”¨kthvalueæ‰¾ç¬¬kå¤§çš„å€¼ï¼ˆO(n)å¤æ‚åº¦ï¼‰
        # è¿™æ˜¯å…³é”®ä¼˜åŒ–ï¼šä¸éœ€è¦å®Œæ•´æ’åºï¼Œåªéœ€è¦æ‰¾åˆ†ç•Œç‚¹
        kth_val = torch.kthvalue(col_sparsity, N - num_sparse + 1)[0]
        
        # Line 138: åˆ›å»ºæ©ç ï¼Œæ‰€æœ‰>=é˜ˆå€¼çš„åˆ—æ ‡è®°ä¸ºç¨€ç–
        sparse_mask = col_sparsity >= kth_val
        
        # Lines 142-147: å¤„ç†ç›¸ç­‰å€¼ï¼ˆç¡®ä¿æ°å¥½num_sparseä¸ªï¼‰
        # è¿™æ˜¯å¿…è¦çš„ï¼Œå› ä¸ºå¯èƒ½æœ‰å¤šä¸ªåˆ—çš„ç¨€ç–åº¦æ°å¥½ç­‰äºé˜ˆå€¼
        if sparse_mask.sum() > num_sparse:
            # æœ‰å¤ªå¤šç›¸ç­‰çš„å€¼ï¼Œåªä¿ç•™å‰num_sparseä¸ª
            indices = torch.where(sparse_mask)[0]
            sparse_mask.fill_(False)
            sparse_mask[indices[:num_sparse]] = True
    else:  # Line 149-150: æ‰€æœ‰åˆ—éƒ½æ˜¯ç¨€ç–çš„
        sparse_mask = torch.ones(N, dtype=torch.bool, device=col_sparsity.device)
    
    return sparse_mask
```

### 4. split_gemm_2to4_kernel å®Œæ•´é€è¡Œè§£é‡Š (triton_split_gemm_nocopy.py)

è¿™æ˜¯åº”ç”¨2:4ç¨€ç–æ¨¡å¼çš„æ ¸å¿ƒTriton kernelï¼Œç›´æ¥ä¿®æ”¹è¾“å…¥çŸ©é˜µï¼ˆin-placeï¼‰ï¼Œä¹Ÿæ˜¯æœ€å¯èƒ½å‡ºç°NaNçš„åœ°æ–¹ï¼š

```python
@triton.jit
def split_gemm_2to4_kernel(
    a_ptr,              # è¾“å…¥çŸ©é˜µAçš„æŒ‡é’ˆï¼ˆä¼šè¢«åŸåœ°ä¿®æ”¹ï¼‰
    sparse_mask_ptr,    # ç¨€ç–æ©ç æŒ‡é’ˆ
    M, K,               # çŸ©é˜µç»´åº¦
    stride_am, stride_ak,  # æ­¥é•¿
    BLOCK_M: tl.constexpr,  # å—å¤§å°
    BLOCK_K: tl.constexpr,
):
    """
    å¯¹æ ‡è®°ä¸ºç¨€ç–çš„åˆ—åŸåœ°åº”ç”¨2:4ç¨€ç–åŒ–
    è¿™æ˜¯Split-GEMMçš„æ ¸å¿ƒï¼šåªå¯¹95%æœ€ç¨€ç–çš„åˆ—åº”ç”¨2:4æ¨¡å¼
    """
```

**kernelå®Œæ•´é€è¡Œè¯¦è§£ï¼ˆç‰¹åˆ«æ³¨æ„è¾¹ç•Œæ£€æŸ¥å’Œ2:4é€‰æ‹©ç®—æ³•ï¼‰ï¼š**

```python
    # Lines 29-30: è·å–çº¿ç¨‹å—ID
    pid_m = tl.program_id(0)  # Mç»´åº¦çš„å—IDï¼ˆè¡Œæ–¹å‘ï¼‰
    pid_k = tl.program_id(1)  # Kç»´åº¦çš„å—IDï¼ˆåˆ—æ–¹å‘ï¼‰
    
    # Lines 33-34: è®¡ç®—å—çš„èµ·å§‹ä½ç½®
    m_block_start = pid_m * BLOCK_M  # è¿™ä¸ªå—å¤„ç†çš„ç¬¬ä¸€è¡Œ
    k_block_start = pid_k * BLOCK_K  # è¿™ä¸ªå—å¤„ç†çš„ç¬¬ä¸€åˆ—
    
    # Lines 37-99: å¤„ç†å—ä¸­çš„æ¯ä¸€åˆ—
    for k_idx in range(BLOCK_K):  # éå†å—ä¸­çš„æ¯ä¸€åˆ—
        k = k_block_start + k_idx  # Line 38: å…¨å±€åˆ—ç´¢å¼•
        
        if k < K:  # Line 39: è¾¹ç•Œæ£€æŸ¥ï¼Œé˜²æ­¢è¶Šç•Œ
            # Line 41: æ£€æŸ¥è¿™åˆ—æ˜¯å¦éœ€è¦ç¨€ç–åŒ–
            # sparse_maskæ˜¯å¸ƒå°”æ•°ç»„ï¼ŒTrueè¡¨ç¤ºè¯¥åˆ—éœ€è¦2:4ç¨€ç–åŒ–
            is_sparse = tl.load(sparse_mask_ptr + k)
            
            if is_sparse:  # Line 43: å¦‚æœæ˜¯ç¨€ç–åˆ—ï¼Œåº”ç”¨2:4æ¨¡å¼
                # Line 45: ä»¥4ä¸ºç»„å¤„ç†å…ƒç´ ï¼ˆ2:4ç¨€ç–çš„åŸºæœ¬å•ä½ï¼‰
                for m_idx in range(0, BLOCK_M, 4):  # æ¯æ¬¡å¤„ç†4ä¸ªè¿ç»­å…ƒç´ 
                    # Lines 46-49: è®¡ç®—4ä¸ªè¿ç»­å…ƒç´ çš„å…¨å±€ç´¢å¼•
                    m0 = m_block_start + m_idx      # ç¬¬1ä¸ªå…ƒç´ çš„è¡Œç´¢å¼•
                    m1 = m0 + 1                      # ç¬¬2ä¸ªå…ƒç´ çš„è¡Œç´¢å¼•
                    m2 = m0 + 2                      # ç¬¬3ä¸ªå…ƒç´ çš„è¡Œç´¢å¼•
                    m3 = m0 + 3                      # ç¬¬4ä¸ªå…ƒç´ çš„è¡Œç´¢å¼•
                    
                    # Line 52: ğŸ”´ è¾¹ç•Œæ£€æŸ¥ - è¿™é‡Œæœ‰ä¸¥é‡é—®é¢˜ï¼
                    # BUG 1: m0 >= 0æ£€æŸ¥æ˜¯æ— æ„ä¹‰çš„ï¼ˆm0ä½œä¸ºéè´Ÿæ•´æ•°æ€»æ˜¯>=0ï¼‰
                    # BUG 2: åªæ£€æŸ¥m3 < Mï¼Œæ²¡æœ‰æ£€æŸ¥m0, m1, m2æ˜¯å¦åœ¨èŒƒå›´å†…
                    # æ­£ç¡®çš„æ£€æŸ¥åº”è¯¥æ˜¯ï¼šif m0 >= 0 and m3 < M:
                    # æ›´ä¸¥æ ¼åº”è¯¥æ˜¯ï¼šif m0 < M and m1 < M and m2 < M and m3 < M:
                    if m3 < M and m0 >= 0:  # â† é—®é¢˜æ‰€åœ¨ï¼
                        
                        # Lines 54-57: è®¡ç®—4ä¸ªå…ƒç´ çš„å†…å­˜åœ°å€
                        # åœ°å€ = åŸºåœ°å€ + è¡Œåç§» + åˆ—åç§»
                        ptr0 = a_ptr + m0 * stride_am + k * stride_ak  # A[m0, k]çš„åœ°å€
                        ptr1 = a_ptr + m1 * stride_am + k * stride_ak  # A[m1, k]çš„åœ°å€
                        ptr2 = a_ptr + m2 * stride_am + k * stride_ak  # A[m2, k]çš„åœ°å€
                        ptr3 = a_ptr + m3 * stride_am + k * stride_ak  # A[m3, k]çš„åœ°å€
                        
                        # Lines 60-63: åŠ è½½4ä¸ªå€¼
                        val0 = tl.load(ptr0)  # åŠ è½½A[m0, k]
                        val1 = tl.load(ptr1)  # åŠ è½½A[m1, k]
                        val2 = tl.load(ptr2)  # åŠ è½½A[m2, k]
                        val3 = tl.load(ptr3)  # åŠ è½½A[m3, k]
                        
                        # Lines 66-69: è½¬æ¢åˆ°float32è¿›è¡Œæ¯”è¾ƒ
                        # é‡è¦ï¼šé¿å…float16/bfloat16çš„ç²¾åº¦é—®é¢˜
                        val0_f32 = val0.to(tl.float32)
                        val1_f32 = val1.to(tl.float32)
                        val2_f32 = val2.to(tl.float32)
                        val3_f32 = val3.to(tl.float32)
                        
                        # Lines 72-75: è®¡ç®—ç»å¯¹å€¼ï¼ˆç”¨äºæ‰¾æœ€å¤§çš„2ä¸ªï¼‰
                        abs0 = tl.abs(val0_f32)
                        abs1 = tl.abs(val1_f32)
                        abs2 = tl.abs(val2_f32)
                        abs3 = tl.abs(val3_f32)
                        
                        # Lines 78-81: 2:4é€‰æ‹©ç®—æ³• - é€šè¿‡è®¡æ•°ç¡®å®šæ¯ä¸ªå…ƒç´ çš„æ’å
                        # æ ¸å¿ƒæ€æƒ³ï¼šå¦‚æœä¸€ä¸ªå…ƒç´ æ¯”è‡³å°‘2ä¸ªå…¶ä»–å…ƒç´ å¤§ï¼Œå®ƒå°±æ˜¯å‰2å¤§
                        # è¿™é¿å…äº†å®Œæ•´æ’åºï¼Œåªéœ€O(1)çš„æ¯”è¾ƒ
                        
                        # count0: è®¡ç®—abs0æ¯”å¤šå°‘ä¸ªå…¶ä»–å€¼å¤§æˆ–ç›¸ç­‰
                        # ä½¿ç”¨>=è€Œé>æ˜¯ä¸ºäº†å¤„ç†ç›¸ç­‰å€¼çš„æƒ…å†µ
                        count0 = tl.where(abs0 >= abs1, 1, 0) + \  # abs0 >= abs1 ? 1 : 0
                                tl.where(abs0 >= abs2, 1, 0) + \  # abs0 >= abs2 ? 1 : 0  
                                tl.where(abs0 >= abs3, 1, 0)      # abs0 >= abs3 ? 1 : 0
                        # count0çš„å€¼èŒƒå›´æ˜¯[0,3]ï¼Œå¦‚æœcount0>=2ï¼Œè¯´æ˜abs0æ˜¯å‰2å¤§
                        
                        # count1: ç‰¹æ®Šå¤„ç† - ç¬¬ä¸€ä¸ªæ¯”è¾ƒä½¿ç”¨>è€Œé>=
                        # è¿™æ˜¯ä¸ºäº†æ‰“ç ´å¹³å±€ï¼šå½“abs0==abs1æ—¶ï¼Œåªè®©abs0è·å¾—è¿™ä¸€åˆ†
                        # é¿å…ä¸¤ä¸ªç›¸ç­‰çš„å€¼éƒ½è¢«é€‰ä¸­ï¼Œå¯¼è‡´é€‰å‡ºè¶…è¿‡2ä¸ªå…ƒç´ 
                        count1 = tl.where(abs1 > abs0, 1, 0) + \   # ä¸¥æ ¼å¤§äºï¼ˆæ‰“ç ´å¹³å±€ï¼‰
                                tl.where(abs1 >= abs2, 1, 0) + \  # å¤§äºç­‰äº
                                tl.where(abs1 >= abs3, 1, 0)      # å¤§äºç­‰äº
                        
                        count2 = tl.where(abs2 > abs0, 1, 0) + \
                                tl.where(abs2 > abs1, 1, 0) + \
                                tl.where(abs2 >= abs3, 1, 0)
                        
                        count3 = tl.where(abs3 > abs0, 1, 0) + \
                                tl.where(abs3 > abs1, 1, 0) + \
                                tl.where(abs3 > abs2, 1, 0)
                        
                        # Lines 84-87: æ ¹æ®è®¡æ•°ç¡®å®šå“ªäº›å…ƒç´ ä¿ç•™ï¼ˆå®ç°2:4ç¨€ç–ï¼‰
                        # å¦‚æœcount >= 2ï¼Œè¯´æ˜è¯¥å…ƒç´ è‡³å°‘æ¯”å…¶ä»–2ä¸ªå…ƒç´ å¤§
                        # å³è¯¥å…ƒç´ æ’åœ¨å‰2ä½ï¼Œåº”è¯¥ä¿ç•™
                        keep0 = count0 >= 2  # Trueè¡¨ç¤ºval0æ˜¯å‰2å¤§çš„å€¼
                        keep1 = count1 >= 2  # Trueè¡¨ç¤ºval1æ˜¯å‰2å¤§çš„å€¼
                        keep2 = count2 >= 2  # Trueè¡¨ç¤ºval2æ˜¯å‰2å¤§çš„å€¼  
                        keep3 = count3 >= 2  # Trueè¡¨ç¤ºval3æ˜¯å‰2å¤§çš„å€¼
                        # æœ€ç»ˆæ°å¥½æœ‰2ä¸ªkeepå€¼ä¸ºTrueï¼Œ2ä¸ªä¸ºFalse
                        
                        # Lines 90-93: åº”ç”¨2:4ç¨€ç–æ¨¡å¼ - ä¿ç•™å‰2å¤§ï¼Œå…¶ä½™ç½®é›¶
                        # tl.where(condition, true_val, false_val)æ˜¯Tritonçš„æ¡ä»¶é€‰æ‹©
                        # è¿™å®ç°äº†2:4ç¨€ç–ï¼šæ¯4ä¸ªå…ƒç´ ä¸­åªä¿ç•™2ä¸ªéé›¶å€¼
                        result0 = tl.where(keep0, val0, 0.0)  # keep0 ? val0 : 0
                        result1 = tl.where(keep1, val1, 0.0)  # keep1 ? val1 : 0
                        result2 = tl.where(keep2, val2, 0.0)  # keep2 ? val2 : 0
                        result3 = tl.where(keep3, val3, 0.0)  # keep3 ? val3 : 0
                        
                        # Lines 96-99: åŸåœ°å†™å›ç»“æœï¼ˆIN-PLACEä¿®æ”¹ï¼‰
                        # âš ï¸ é‡è¦ï¼šè¿™ç›´æ¥ä¿®æ”¹äº†è¾“å…¥çŸ©é˜µa_ptræŒ‡å‘çš„å†…å­˜ï¼
                        # è¿™å°±æ˜¯"zero-copy"çš„å«ä¹‰ - ä¸å¤åˆ¶æ•°æ®ï¼Œç›´æ¥ä¿®æ”¹
                        tl.store(ptr0, result0)  # å°†ç¨€ç–åŒ–åçš„å€¼å†™å›åŸä½ç½®
                        tl.store(ptr1, result1)
                        tl.store(ptr2, result2)
                        tl.store(ptr3, result3)
```

### 5. compute_split_gemm_dw2_lowrank é—®é¢˜åˆ†æ

è¿™ä¸ªå‡½æ•°æ˜¯backwardä¸­è®¡ç®—weight_in2æ¢¯åº¦çš„å…³é”®ï¼Œä½†å½“å‰å®ç°æœ‰ä¸¥é‡é—®é¢˜ï¼š

```python
def compute_split_gemm_dw2_lowrank(y2, d_intermediate_2, y2_forward, weight_in2, layer_id):
    """
    è®¡ç®—ä½ç§©å±‚çš„ weight_in2 æ¢¯åº¦ä½¿ç”¨ Split-GEMM ç­–ç•¥
    grad_weight_in2 = y2.T @ d_intermediate_2ï¼Œä½†ä½¿ç”¨95%/5%ç‰¹å¾åˆ†å‰²
    
    å½“å‰é—®é¢˜ï¼šå‡½æ•°åç§°æš—ç¤ºåº”è¯¥ä½¿ç”¨split-GEMMï¼Œä½†å®é™…æ²¡æœ‰ï¼
    """
    # Lines 77-79: dtypeä¸€è‡´æ€§å¤„ç†
    if y2.dtype != d_intermediate_2.dtype:
        y2 = y2.to(d_intermediate_2.dtype)
    
    # Line 81: è·å–ç¼“å­˜çš„ç¨€ç–æ€§
    col_sparsity, sparse_mask = sparsity_tracker.get_sparsity(layer_id)
    # é—®é¢˜ï¼šè·å–äº†ç¨€ç–æ€§ä¿¡æ¯ä½†å®Œå…¨æ²¡æœ‰ä½¿ç”¨ï¼
    
    # Line 89: è½¬ç½®y2
    y2_t = y2.t()  # [intermediate_size, batch*seq]
    
    # Line 94: é—®é¢˜æ‰€åœ¨ - åªæ˜¯æ ‡å‡†çŸ©é˜µä¹˜æ³•ï¼Œæ²¡æœ‰split-GEMMï¼
    result = torch.mm(y2_t, d_intermediate_2)
    # åº”è¯¥ï¼š
    # 1. æ ¹æ®sparse_maskå°†y2_tçš„è¡Œï¼ˆåŸy2çš„åˆ—ï¼‰åˆ†ä¸ºç¨€ç–å’Œå¯†é›†
    # 2. å¯¹ç¨€ç–è¡Œåº”ç”¨2:4ç¨€ç–åŒ–å¹¶ç”¨fake_fp8_mm
    # 3. å¯¹å¯†é›†è¡Œç”¨æ ‡å‡†çŸ©é˜µä¹˜æ³•
    # 4. åˆå¹¶ç»“æœ
    
    return result  # [intermediate_size, rank2]
```

---

## è¡¥å……ï¼šå…¶ä»–å…³é”®å‡½æ•°çš„è¯¦ç»†è§£é‡Š

### 6. fake_fp8_mm å‡½æ•°è§£é‡Š

è¿™ä¸ªå‡½æ•°æ¨¡æ‹Ÿ FP8 çŸ©é˜µä¹˜æ³•ï¼Œå®é™…ä½¿ç”¨ Triton çš„ç¨€ç–çŸ©é˜µä¹˜æ³•ï¼š

```python
def fake_fp8_mm(a, b, dtype):
    """
    æ¨¡æ‹Ÿ FP8 çŸ©é˜µä¹˜æ³•ï¼Œä½†å®é™…ä½¿ç”¨ float16 è®¡ç®—
    ç”¨äº2:4ç¨€ç–çŸ©é˜µçš„é«˜æ•ˆè®¡ç®—
    """
    # Lines 34-35: ä¿å­˜åŸå§‹æ•°æ®ç±»å‹
    original_dtype_a = a.dtype
    original_dtype_b = b.dtype
    
    # Lines 38-39: è½¬æ¢åˆ° float16 ä»¥å…¼å®¹ Triton
    # Triton çš„ç¨€ç– matmul kernel åªæ”¯æŒ float16
    # è¿™é‡Œå¤„ç† bfloat16 â†’ float16 çš„è½¬æ¢
    a = a.to(torch.float16).contiguous()  # ç¡®ä¿è¿ç»­å­˜å‚¨
    b = b.to(torch.float16).contiguous()
    
    # Line 40: è°ƒç”¨ Triton çš„ç¨€ç– matmul kernel
    # c_dtype=torch.float32 ä½¿ç”¨ float32 ç´¯åŠ å™¨ä¿è¯ç²¾åº¦
    output = matmul(a, b, c_dtype=torch.float32)
    
    # Lines 42-46: æ ¹æ®è¾“å…¥ç±»å‹è½¬æ¢è¾“å‡º
    if original_dtype_a == torch.bfloat16 or original_dtype_b == torch.bfloat16:
        output = output.to(torch.bfloat16)
    elif original_dtype_a == torch.float32 or original_dtype_b == torch.float32:
        output = output.to(torch.float32)
    else:
        output = output.to(torch.float16)
    
    return output
```

**å…³é”®ç‚¹ï¼š**
- è¿™ä¸ªå‡½æ•°æ˜¯2:4ç¨€ç–è®¡ç®—çš„æ ¸å¿ƒï¼Œé€šè¿‡Tritonçš„ç¨€ç– kernelåŠ é€Ÿ
- å¿…é¡»å¤„ç†dtypeè½¬æ¢ï¼Œå› ä¸ºTriton kernelåªæ”¯æŒfloat16
- ä½¿ç”¨float32ç´¯åŠ å™¨é¿å…ç²¾åº¦æŸå¤±

### 7. apply_soft_threshold_dynamic_activation_2to4_sparsity å‡½æ•°è§£é‡Š

è¿™æ˜¯åŠ¨æ€è½¯é˜ˆå€¼2:4ç¨€ç–åŒ–çš„å®ç°ï¼š

```python
def apply_soft_threshold_dynamic_activation_2to4_sparsity(
    input_tensor, layer_id=0, current_step=0, 
    dynamic_steps=10, calibration_samples=100
):
    """
    åº”ç”¨åŠ¨æ€è½¯é˜ˆå€¼2:4ç¨€ç–åŒ–
    æ ¹æ®æ¿€æ´»å€¼åŠ¨æ€è°ƒæ•´ç¼©æ”¾å› å­ï¼Œæœ€å°åŒ–ç¨€ç–åŒ–å‰åçš„MSE
    """
    # Line 2380: ä¿å­˜åŸå§‹æ•°æ®ç±»å‹
    original_dtype = input_tensor.dtype
    
    # Line 2383: è½¬æ¢åˆ°float16ä»¥å…¼å®¹Triton kernel
    input_temp = input_tensor.to(torch.float16).contiguous()
    
    # Line 2386: åº”ç”¨è½¯é˜ˆå€¼2:4ç¨€ç–åŒ–
    # soft_threshold24_triton æ˜¯ä¸€ä¸ªTriton kernel
    # å®ƒä¿ç•™æ¯4ä¸ªå…ƒç´ ä¸­æœ€å¤§çš„2ä¸ªï¼Œå¹¶å‡å»ç¬¬äºŒå¤§çš„å€¼
    output_temp = soft_threshold24_triton(input_temp)
    
    # Lines 2389-2393: è½¬å›åŸå§‹ç²¾åº¦
    if original_dtype == torch.bfloat16:
        output = output_temp.to(torch.bfloat16)
    elif original_dtype == torch.float32:
        output = output_temp.to(torch.float32)
    else:
        output = output_temp  # ä¿æŒfloat16
    
    # Lines 2396-2434: åŠ¨æ€ç¼©æ”¾å› å­è®¡ç®—
    # æ¯dynamic_stepsæ­¥é‡æ–°è®¡ç®—ç¼©æ”¾å› å­
    if current_step % dynamic_steps == 0:
        # è·å–æˆ–åˆ›å»ºç®¡ç†å™¨
        manager = ActivationSoftThresholdManager.get_instance()
        
        # æ”¶é›†calibration_samplesä¸ªæ ·æœ¬
        if layer_id not in manager.activation_samples:
            manager.activation_samples[layer_id] = []
        
        samples = manager.activation_samples[layer_id]
        if len(samples) < calibration_samples:
            # æ·»åŠ å½“å‰æ¿€æ´»å€¼ä½œä¸ºæ ·æœ¬
            samples.append(input_tensor.detach().clone())
        
        # å½“æ”¶é›†åˆ°è¶³å¤Ÿæ ·æœ¬åï¼Œè®¡ç®—æœ€ä½³ç¼©æ”¾å› å­
        if len(samples) == calibration_samples:
            # åˆå¹¶æ‰€æœ‰æ ·æœ¬
            all_samples = torch.cat(samples, dim=0)
            
            # åº”ç”¨è½¯é˜ˆå€¼ç¨€ç–åŒ–åˆ°æ‰€æœ‰æ ·æœ¬
            all_samples_sparse = soft_threshold24_triton(
                all_samples.to(torch.float16).contiguous()
            ).to(all_samples.dtype)
            
            # è®¡ç®—æœ€ä½³ç¼©æ”¾å› å­ï¼ˆæœ€å°åŒ–MSEï¼‰
            # scale = (X^T @ X_sparse) / (X_sparse^T @ X_sparse)
            numerator = (all_samples * all_samples_sparse).sum()
            denominator = (all_samples_sparse ** 2).sum()
            
            if denominator > 0:
                scale = numerator / denominator
                # é™åˆ¶ç¼©æ”¾å› å­èŒƒå›´ [0.5, 2.0]
                scale = torch.clamp(scale, 0.5, 2.0)
            else:
                scale = 1.0
            
            # ä¿å­˜ç¼©æ”¾å› å­
            manager.scales[layer_id] = scale.item()
            
            # æ¸…ç©ºæ ·æœ¬ä»¥èŠ‚çœå†…å­˜
            samples.clear()
    
    # Line 2437: åº”ç”¨ç¼©æ”¾å› å­
    manager = ActivationSoftThresholdManager.get_instance()
    scale = manager.scales.get(layer_id, 1.0)
    output = output * scale
    
    return output
```

### 8. apply_naive_2to4_sparsity å‡½æ•°è§£é‡Š

è¿™ä¸ªå‡½æ•°å®ç°åŸºæœ¬çš„2:4ç¨€ç–åŒ–ï¼Œé€šè¿‡è°ƒç”¨Triton kernelå®Œæˆï¼š

```python
def apply_naive_2to4_sparsity(input_tensor):
    """
    ä½¿ç”¨ Triton å®ç°çš„ naive 2:4 ç¨€ç–åŒ–
    å¯¹æ¯ä¸ªè¡Œç‹¬ç«‹åº”ç”¨2:4ç¨€ç–æ¨¡å¼
    """
    # Line 2092: æ£€æŸ¥è¾“å…¥æ˜¯äºŒç»´å¼ é‡
    assert input_tensor.dim() == 2, "apply_naive_2to4_sparsity expects a 2D tensor [M, N]"
    
    # Line 2093: ä¿å­˜åŸå§‹æ•°æ®ç±»å‹
    original_dtype = input_tensor.dtype
    
    # Line 2095: è½¬æ¢åˆ°float16ä»¥åŒ¹é…Triton kernelçš„è¦æ±‚
    # Triton kernelé€šå¸¸å¯¹float16ä¼˜åŒ–æœ€å¥½
    input_temp = input_tensor.to(torch.float16).contiguous()
    
    # Line 2096: è°ƒç”¨Triton kernelæ‰§è¡Œ2:4ç¨€ç–åŒ–
    # naive24_tritonæ˜¯ä¸€ä¸ªé¢„ç¼–è¯‘çš„Triton kernel
    output_temp = naive24_triton(input_temp)
    
    # Lines 2098-2102: è½¬å›åŸå§‹ç²¾åº¦
    if original_dtype == torch.bfloat16:
        return output_temp.to(torch.bfloat16)
    elif original_dtype == torch.float32:
        return output_temp.to(torch.float32)
    else:
        return output_temp  # ä¿æŒfloat16
```

### 9. compute_split_gemm_lowrank_intermediate å‡½æ•°è§£é‡Š

è¿™ä¸ªå‡½æ•°åœ¨backwardä¸­è®¡ç®—ä¸­é—´æ¢¯åº¦ï¼Œä½¿ç”¨ç¼“å­˜çš„ç¨€ç–æ€§ä¿¡æ¯ï¼š

```python
def compute_split_gemm_lowrank_intermediate_nocopy(dy1, weight_out1, layer_id):
    """
    é›¶æ‹·è´ç‰ˆæœ¬çš„split-GEMMä¸­é—´è®¡ç®—
    è®¡ç®—: dy1 @ weight_out1 ä½¿ç”¨split-GEMMç­–ç•¥
    """
    from fused_sparsity_ops import sparsity_tracker
    
    # Line 218: ä»ç¼“å­˜è·å–forward passè®¡ç®—çš„ç¨€ç–æ€§ä¿¡æ¯
    col_sparsity, sparse_mask = sparsity_tracker.get_sparsity(layer_id)
    
    # Lines 221-241: æ£€æŸ¥ç¨€ç–æ€§ä¿¡æ¯æ˜¯å¦å­˜åœ¨
    if sparse_mask is None:
        # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°è¯¦ç»†é”™è¯¯
        print(f"ERROR: No cached sparsity found for layer_id={layer_id}")
        print(f"Available layer_ids: {list(sparsity_tracker.forward_masks.keys())}")
        
        # å°è¯•æ‰¾åˆ°ç±»ä¼¼çš„layer_idï¼ˆå¤„ç†å‘½åä¸ä¸€è‡´é—®é¢˜ï¼‰
        for stored_id in sparsity_tracker.forward_masks.keys():
            if 'lowrank_layer1' in stored_id:
                print(f"Found similar layer_id: {stored_id}")
                # å°è¯•ä½¿ç”¨æ‰¾åˆ°çš„ç¨€ç–æ€§
                col_sparsity, sparse_mask = sparsity_tracker.get_sparsity(stored_id)
                if sparse_mask is not None:
                    return split_gemm_nocopy(dy1, weight_out1, sparse_mask)
        
        # å¦‚æœä»æœªæ‰¾åˆ°ï¼ŒæŠ›å‡ºé”™è¯¯
        raise RuntimeError(f"Failed to find cached sparsity for layer_id={layer_id}")
    
    # Line 243: è°ƒç”¨é›¶æ‹·è´çš„split-GEMMå®ç°
    return split_gemm_nocopy(dy1, weight_out1, sparse_mask)
```

### 10. split_gemm_nocopy å‡½æ•°è§£é‡Š

é›¶æ‹·è´çš„Split-GEMMæ ¸å¿ƒå®ç°ï¼š

```python
def split_gemm_nocopy(dy1, weight, sparse_mask):
    """
    é›¶æ‹·è´ Split-GEMM å®ç°
    ç­–ç•¥:
    1. å…‹éš†dy1ï¼ˆä¸ºäº†ä¿ç•™åŸå§‹æ•°æ®ï¼‰
    2. å¯¹ç¨€ç–åˆ—åŸåœ°åº”ç”¨2:4ç¨€ç–
    3. ä½¿ç”¨å•ä¸ªGEMMå¤„ç†æ··åˆç¨€ç–/å¯†é›†æ•°æ®
    """
    M, K = dy1.shape
    _, N = weight.shape
    
    # Lines 115-118: ç¡®ä¿å¼ é‡è¿ç»­å­˜å‚¨ï¼ˆTriton kernelè¦æ±‚ï¼‰
    if not dy1.is_contiguous():
        dy1 = dy1.contiguous()
    if not weight.is_contiguous():
        weight = weight.contiguous()
    
    # Line 121: å…‹éš†è¾“å…¥ï¼ˆå¿…è¦çš„ï¼Œä¸ºäº†ä¿ç•™åŸå§‹æ•°æ®ç”¨äºå…¶ä»–è®¡ç®—ï¼‰
    dy1_work = dy1.clone()
    
    # Lines 125-161: åŸåœ°åº”ç”¨2:4ç¨€ç–åˆ°ç¨€ç–åˆ—
    if sparse_mask is not None and sparse_mask.numel() > 0 and sparse_mask.any():
        # ç¡®ä¿æ©ç åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        sparse_mask = sparse_mask.to(dy1.device)
        sparse_mask_int = sparse_mask.to(torch.int32)  # Tritonéœ€è¦int32
        
        # é…ç½®çº¿ç¨‹å—å¤§å°
        BLOCK_M = 128
        BLOCK_K = 32
        
        # è®¡ç®—ç½‘æ ¼å¤§å°
        grid = (
            triton.cdiv(M, BLOCK_M),  # Mç»´åº¦çš„å—æ•°
            triton.cdiv(K, BLOCK_K),  # Kç»´åº¦çš„å—æ•°
        )
        
        # å¯åŠ¨kernelåŸåœ°ä¿®æ”¹dy1_work
        try:
            split_gemm_2to4_kernel[grid](
                dy1_work,          # å°†è¢«åŸåœ°ä¿®æ”¹
                sparse_mask_int,   # ç¨€ç–åˆ—æ©ç 
                M, K,
                dy1_work.stride(0), dy1_work.stride(1),
                BLOCK_M, BLOCK_K,
                num_warps=4,       # æ€§èƒ½è°ƒä¼˜å‚æ•°
                num_stages=2,      # æµæ°´çº¿çº§æ•°
            )
            # åŒæ­¥ç¡®ä¿kernelå®Œæˆ
            torch.cuda.synchronize()
        except RuntimeError as e:
            # è¯¦ç»†é”™è¯¯ä¿¡æ¯ç”¨äºè°ƒè¯•
            print(f"Kernel execution failed with shape [{M}, {K}], grid {grid}")
            raise e
    
    # Line 166: ç°åœ¨dy1_workæœ‰ç¨€ç–åˆ—çš„2:4ç¨€ç–ï¼Œå¯†é›†åˆ—ä¿æŒåŸæ ·
    # ä½¿ç”¨æ ‡å‡†matmulå¤„ç†æ··åˆç¨€ç–/å¯†é›†çŸ©é˜µ
    # æ³¨æ„ï¼šfake_fp8_mméœ€è¦å®Œå…¨ç¨€ç–çŸ©é˜µï¼Œæ‰€ä»¥è¿™é‡Œä¸èƒ½ç”¨
    result = torch.mm(dy1_work, weight)
    
    return result
```

### 11. soft_threshold24_triton å‡½æ•°å’Œå†…æ ¸è§£é‡Š

è¿™æ˜¯å®ç°è½¯é˜ˆå€¼2:4ç¨€ç–åŒ–çš„æ ¸å¿ƒå‡½æ•°ï¼š

#### Python åŒ…è£…å‡½æ•°
```python
def soft_threshold24_triton(dense):
    """
    å¯¹çŸ©é˜µåº”ç”¨è½¯é˜ˆå€¼2:4ç¨€ç–åŒ–
    ä¿ç•™æ¯4ä¸ªå…ƒç´ ä¸­æœ€å¤§çš„2ä¸ªï¼Œå¹¶å‡å»ç¬¬äºŒå¤§çš„å€¼ï¼ˆè½¯é˜ˆå€¼ï¼‰
    """
    m, k = dense.shape
    device = dense.device
    
    # Line 202-203: åˆå§‹åŒ–è¾“å‡ºçŸ©é˜µå’Œæ©ç 
    sparse = torch.zeros_like(dense)  # ç¨€ç–åŒ–åçš„çŸ©é˜µ
    mask = torch.zeros_like(dense)    # æ ‡è®°å“ªäº›å…ƒç´ éé›¶
    
    # Lines 205-211: æ ¹æ®å†…å­˜å¸ƒå±€é€‰æ‹©å¤„ç†æ–¹å¼
    row_stride, col_stride = dense.stride()
    if row_stride > col_stride:
        # è¡Œä¸»åºï¼ˆrow-majorï¼‰å¸ƒå±€
        array_layout = 'row'
        # æ¯ä¸ªçº¿ç¨‹å¤„ç†ä¸€è¡Œçš„å¤šä¸ª4å…ƒç´ ç»„
        grid = lambda META: (m, triton.cdiv(k, 4 * META['BLOCK_SIZE']))
    else:
        # åˆ—ä¸»åºï¼ˆcolumn-majorï¼‰å¸ƒå±€
        array_layout = 'col'
        # æ¯ä¸ªçº¿ç¨‹å¤„ç†å¤šè¡Œçš„ä¸€ä¸ª4å…ƒç´ ç»„
        grid = lambda META: (triton.cdiv(m, META['BLOCK_SIZE']), k // 4)
    
    # Lines 213-226: å¯åŠ¨Triton kernel
    func = _soft_threshold24_triton
    func[grid](
        dense,    # è¾“å…¥çŸ©é˜µ
        sparse,   # è¾“å‡ºç¨€ç–çŸ©é˜µ
        mask,     # è¾“å‡ºæ©ç 
        dense.stride(0), sparse.stride(0), mask.stride(0),  # è¡Œæ­¥é•¿
        dense.stride(1), sparse.stride(1), mask.stride(1),  # åˆ—æ­¥é•¿
        m, k,     # çŸ©é˜µç»´åº¦
        BLOCK_SIZE=1024,       # å—å¤§å°
        ARRAY_LAYOUT=array_layout  # å†…å­˜å¸ƒå±€
    )
    return sparse, mask
```

#### Triton Kernel å®ç°
```python
@triton.jit
def _soft_threshold24_triton(
    dense_ptr, sparse_ptr, mask_ptr,
    dense_row_stride, sparse_row_stride, mask_row_stride,
    dense_col_stride, sparse_col_stride, mask_col_stride,
    m, k,
    BLOCK_SIZE: tl.constexpr,
    ARRAY_LAYOUT: tl.constexpr
):
    # Lines 172-179: æ ¹æ®å¸ƒå±€è®¡ç®—çº¿ç¨‹å¤„ç†çš„ä½ç½®
    if ARRAY_LAYOUT == 'row':
        row_idx = tl.program_id(0)  # æ¯ä¸ªçº¿ç¨‹å¤„ç†ä¸€è¡Œ
        # åˆ—ç´¢å¼•ï¼šæ¯ä¸ªçº¿ç¨‹å¤„ç†BLOCK_SIZEä¸ª4å…ƒç´ ç»„
        col_idx = tl.program_id(1) * 4 * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE) * 4
        mask = col_idx < k  # è¾¹ç•Œæ£€æŸ¥
    elif ARRAY_LAYOUT == 'col':
        # è¡Œç´¢å¼•ï¼šæ¯ä¸ªçº¿ç¨‹å¤„ç†BLOCK_SIZEè¡Œ
        row_idx = tl.arange(0, BLOCK_SIZE) + tl.program_id(0) * BLOCK_SIZE
        col_idx = tl.program_id(1) * 4  # æ¯ä¸ªçº¿ç¨‹å¤„ç†ä¸€ä¸ª4å…ƒç´ ç»„
        mask = row_idx < m  # è¾¹ç•Œæ£€æŸ¥
    
    # Lines 180-183: åŠ è½½4ä¸ªè¿ç»­å…ƒç´ 
    dense_40 = tl.load(dense_ptr + row_idx * dense_row_stride + (col_idx + 0) * dense_col_stride, mask=mask)
    dense_41 = tl.load(dense_ptr + row_idx * dense_row_stride + (col_idx + 1) * dense_col_stride, mask=mask)
    dense_42 = tl.load(dense_ptr + row_idx * dense_row_stride + (col_idx + 2) * dense_col_stride, mask=mask)
    dense_43 = tl.load(dense_ptr + row_idx * dense_row_stride + (col_idx + 3) * dense_col_stride, mask=mask)
    
    # Line 185: è°ƒç”¨è½¯é˜ˆå€¼å‡½æ•°å¤„ç†4ä¸ªå…ƒç´ 
    dense_40, dense_41, dense_42, dense_43, m0, m1, m2, m3 = _soft_threshold(
        dense_40, dense_41, dense_42, dense_43
    )
    
    # Lines 187-195: å­˜å‚¨ç»“æœï¼ˆåªå­˜å‚¨éé›¶å…ƒç´ ï¼‰
    # ç¨€ç–çŸ©é˜µï¼šåªåœ¨maskä¸ºtrueçš„ä½ç½®å­˜å‚¨
    tl.store(sparse_ptr + row_idx * sparse_row_stride + (col_idx + 0) * sparse_col_stride, 
             dense_40, mask=mask & m0)
    tl.store(sparse_ptr + row_idx * sparse_row_stride + (col_idx + 1) * sparse_col_stride, 
             dense_41, mask=mask & m1)
    # ... ç±»ä¼¼åœ°å­˜å‚¨dense_42å’Œdense_43
    
    # æ©ç çŸ©é˜µï¼šæ ‡è®°å“ªäº›ä½ç½®éé›¶
    tl.store(mask_ptr + row_idx * mask_row_stride + (col_idx + 0) * mask_col_stride, 
             m0, mask=mask & m0)
    # ... ç±»ä¼¼åœ°å­˜å‚¨m1, m2, m3
```

#### _soft_threshold æ ¸å¿ƒç®—æ³•
```python
@triton.jit
def _soft_threshold(a0, a1, a2, a3):
    """
    å¯¹4ä¸ªå…ƒç´ ä¸­é€‰æ‹©2ä¸ªæœ€å¤§çš„ï¼Œå¹¶å‡å»ç¬¬äºŒå¤§çš„å€¼ï¼ˆè½¯é˜ˆå€¼ï¼‰
    è¿™æ˜¯è½¯é˜ˆå€¼2:4ç¨€ç–åŒ–çš„æ ¸å¿ƒç®—æ³•
    """
    # Lines 75-76: è®¡ç®—æ‰€æœ‰æˆå¯¹æ¯”è¾ƒ
    # è¿™é‡Œä½¿ç”¨äº†6ä¸ªæ¯”è¾ƒæ¥ç¡®å®š4ä¸ªå…ƒç´ çš„æ’åº
    x1 = tl.abs(a0) > tl.abs(a1)  # |a0| > |a1|
    x2 = tl.abs(a0) > tl.abs(a2)  # |a0| > |a2|
    x3 = tl.abs(a0) > tl.abs(a3)  # |a0| > |a3|
    x4 = tl.abs(a1) > tl.abs(a2)  # |a1| > |a2|
    x5 = tl.abs(a1) > tl.abs(a3)  # |a1| > |a3|
    x6 = tl.abs(a2) > tl.abs(a3)  # |a2| > |a3|
    
    # Line 77: é€šè¿‡é€»è¾‘è¿ç®—ç¡®å®šæ¯ä¸ªå…ƒç´ æ˜¯å¦åœ¨å‰2å¤§ä¸­
    # è¿™æ˜¯ä¸€ä¸ªéå¸¸å·§å¦™çš„ä½è¿ç®—æŠ€å·§ï¼Œé€šè¿‡6ä¸ªæ¯”è¾ƒç»“æœå¾—åˆ°æ¯ä¸ªå…ƒç´ çš„æ’å
    m0 = x2 & x3 | x1 & x2 | x1 & x3  # a0æ˜¯å‰2å¤§ï¼Ÿ
    m1 = ~x1 & x5 | x4 & x5 | ~x1 & x4  # a1æ˜¯å‰2å¤§ï¼Ÿ
    m2 = ~x2 & ~x4 | ~x2 & x6 | ~x4 & x6  # a2æ˜¯å‰2å¤§ï¼Ÿ
    m3 = ~x3 & ~x5 | ~x3 & ~x6 | ~x5 & ~x6  # a3æ˜¯å‰2å¤§ï¼Ÿ
    
    # Lines 83-84: è®¡ç®—é˜ˆå€¼ï¼ˆç¬¬äºŒå¤§çš„å€¼ï¼‰
    # ä½¿ç”¨min-maxç®—æ³•æ‰¾åˆ°ç¬¬äºŒå¤§çš„å€¼
    threshold = tl.minimum(
        tl.maximum(tl.minimum(tl.abs(a0), tl.abs(a1)), 
                   tl.minimum(tl.abs(a2), tl.abs(a3))),
        tl.minimum(tl.maximum(tl.abs(a0), tl.abs(a1)), 
                   tl.maximum(tl.abs(a2), tl.abs(a3)))
    )
    
    # Lines 86-89: åº”ç”¨è½¯é˜ˆå€¼
    # å¯¹äºæ­£å€¼å‡å»é˜ˆå€¼ï¼Œè´Ÿå€¼åŠ ä¸Šé˜ˆå€¼
    # è¿™æ ·ä¿ç•™å‰2å¤§çš„å€¼ï¼Œä½†å‡å°äº†å®ƒä»¬çš„å¹…åº¦
    s0 = tl.where(a0 > 0, a0 - threshold, a0 + threshold)
    s1 = tl.where(a1 > 0, a1 - threshold, a1 + threshold)
    s2 = tl.where(a2 > 0, a2 - threshold, a2 + threshold)
    s3 = tl.where(a3 > 0, a3 - threshold, a3 + threshold)
    
    # Line 90: è¿”å›è½¯é˜ˆå€¼åçš„å€¼å’Œæ©ç 
    return s0, s1, s2, s3, m0, m1, m2, m3
```

### 12. matmul å‡½æ•°è§£é‡Š

è¿™æ˜¯ç¨€ç–çŸ©é˜µä¹˜æ³•çš„æ ¸å¿ƒå®ç°ï¼š

#### Python åŒ…è£…å‡½æ•°
```python
def matmul(a, b, c_dtype=torch.float16, activation=""):
    """
    ç¨€ç–çŸ©é˜µä¹˜æ³•ï¼Œæ”¯æŒ2:4ç¨€ç–æ¨¡å¼
    è¿™æ˜¯fake_fp8_mmå†…éƒ¨è°ƒç”¨çš„æ ¸å¿ƒå‡½æ•°
    """
    # Lines 311-314: æ£€æŸ¥è¾“å…¥å’Œè·å–ç»´åº¦
    assert a.shape[1] == b.shape[0], "Incompatible dimensions"
    assert a.is_contiguous(), "Matrix A must be contiguous"
    M, K = a.shape
    K, N = b.shape
    
    # Line 316: åˆ†é…è¾“å‡ºå†…å­˜
    c = torch.empty((M, N), device=a.device, dtype=c_dtype)
    
    # Line 318: è®¡ç®—çº¿ç¨‹ç½‘æ ¼
    grid = lambda META: (
        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),
    )
    
    # Lines 319-326: å¯åŠ¨kernel
    matmul_kernel[grid](
        a, b, c,  # çŸ©é˜µæŒ‡é’ˆ
        M, N, K,  # ç»´åº¦
        a.stride(0), a.stride(1),  # Açš„æ­¥é•¿
        b.stride(0), b.stride(1),  # Bçš„æ­¥é•¿
        c.stride(0), c.stride(1),  # Cçš„æ­¥é•¿
        ACTIVATION=activation       # æ¿€æ´»å‡½æ•°
    )
    return c
```

#### matmul_kernel Tritonå®ç°
```python
@triton.jit
def matmul_kernel(
    a_ptr, b_ptr, c_ptr,
    M, N, K,
    stride_am, stride_ak,
    stride_bk, stride_bn,
    stride_cm, stride_cn,
    BLOCK_SIZE_M: tl.constexpr, 
    BLOCK_SIZE_N: tl.constexpr, 
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    ACTIVATION: tl.constexpr
):
    """
    ç¨€ç–çŸ©é˜µä¹˜æ³•kernel
    æ”¯æŒ2:4ç¨€ç–æ¨¡å¼çš„é«˜æ•ˆè®¡ç®—
    """
    # è·å–å½“å‰çº¿ç¨‹å—çš„ID
    pid = tl.program_id(axis=0)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
    num_pid_in_group = GROUP_SIZE_M * num_pid_n
    group_id = pid // num_pid_in_group
    first_pid_m = group_id * GROUP_SIZE_M
    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)
    
    # ä½¿ç”¨swizzleä¼˜åŒ–å†…å­˜è®¿é—®æ¨¡å¼
    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)
    pid_n = (pid % num_pid_in_group) // group_size_m
    
    # åˆ›å»ºæŒ‡é’ˆå’Œåç§»
    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    
    # åˆå§‹åŒ–æŒ‡é’ˆ
    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)
    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)
    
    # ç´¯åŠ å™¨
    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
    
    # ä¸»å¾ªç¯ - åˆ†å—çŸ©é˜µä¹˜æ³•
    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        # åŠ è½½å—
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        
        # çŸ©é˜µä¹˜æ³•ï¼ˆåˆ©ç”¨Tensor CoreåŠ é€Ÿ2:4ç¨€ç–ï¼‰
        accumulator = tl.dot(a, b, accumulator)
        
        # æ›´æ–°æŒ‡é’ˆ
        a_ptrs += BLOCK_SIZE_K * stride_ak
        b_ptrs += BLOCK_SIZE_K * stride_bk
    
    # åº”ç”¨æ¿€æ´»å‡½æ•°ï¼ˆå¦‚æœæœ‰ï¼‰
    c = accumulator.to(c_ptr.dtype.element_ty)
    if ACTIVATION == "leaky_relu":
        c = leaky_relu(c)
    
    # å†™å›ç»“æœ
    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)
    tl.store(c_ptrs, c, mask=c_mask)
```

**å…³é”®ç‚¹è§£é‡Šï¼š**

1. **soft_threshold24_triton**ï¼š
   - å®ç°è½¯é˜ˆå€¼2:4ç¨€ç–åŒ–
   - ä¿ç•™æ¯4ä¸ªå…ƒç´ ä¸­æœ€å¤§çš„2ä¸ª
   - å‡å»ç¬¬äºŒå¤§å€¼ä»¥å‡å°å¹…åº¦ï¼ˆè½¯é˜ˆå€¼ï¼‰
   - è¿™ç§æ–¹æ³•æ¯”ç¡¬é˜ˆå€¼æ›´å¹³æ»‘ï¼Œå‡å°‘æ¢¯åº¦çªå˜

2. **matmul**ï¼š
   - æ”¯æŒ2:4ç¨€ç–æ¨¡å¼çš„çŸ©é˜µä¹˜æ³•
   - ä½¿ç”¨Tensor CoreåŠ é€Ÿ
   - é€šè¿‡swizzleä¼˜åŒ–å†…å­˜è®¿é—®æ¨¡å¼
   - è¿™æ˜¯fake_fp8_mmå†…éƒ¨è°ƒç”¨çš„æ ¸å¿ƒå‡½æ•°

3. **_soft_thresholdç®—æ³•**ï¼š
   - ä½¿ç”¨ä½è¿ç®—å·§å¦™åœ°ç¡®å®š4ä¸ªå…ƒç´ çš„æ’åº
   - é€šè¿‡6ä¸ªæ¯”è¾ƒå³å¯ç¡®å®šå‰2å¤§çš„å…ƒç´ 
   - ä½¿ç”¨min-maxç®—æ³•å¿«é€Ÿæ‰¾åˆ°ç¬¬äºŒå¤§å€¼

### 13. relu2_with_sparsity å‡½æ•°è§£é‡Š

è¿™ä¸ªå‡½æ•°å®ç°äº†ReLUÂ²æ¿€æ´»å’Œç¨€ç–æ€§è·Ÿè¸ªï¼š

```python
def relu2_with_sparsity(x):
    """
    è®¡ç®— ReLUÂ²(x) å¹¶è·Ÿè¸ªåˆ—ç¨€ç–æ€§
    """
    M, N = x.shape
    
    # Line 73: åˆ†é…è¾“å‡ºå†…å­˜
    y = torch.empty_like(x)
    
    # Line 76: åˆ†é…ç¨€ç–æ€§è®¡æ•°å™¨
    col_nnz = torch.zeros(N, device=x.device, dtype=torch.int32)
    
    # Lines 79-80: å®šä¹‰å—å¤§å°
    BLOCK_SIZE_M = 128
    BLOCK_SIZE_N = 128
    
    # Lines 83-85: è®¡ç®—ç½‘æ ¼å¤§å°
    grid = (
        triton.cdiv(M, BLOCK_SIZE_M),  # Mæ–¹å‘çš„å—æ•°
        triton.cdiv(N, BLOCK_SIZE_N),  # Næ–¹å‘çš„å—æ•°
    )
    
    # Lines 88-98: å¯åŠ¨kernel
    relu2_with_sparsity_kernel[grid](
        x, y,
        M, N,
        x.stride(0), x.stride(1),
        y.stride(0), y.stride(1),
        col_nnz,
        BLOCK_SIZE_M=BLOCK_SIZE_M,
        BLOCK_SIZE_N=BLOCK_SIZE_N,
    )
    
    # Line 101: è®¡ç®—ç¨€ç–ç‡
    col_sparsity = 1.0 - (col_nnz.float() / M)
    
    return y, col_sparsity
```

### 12. ActivationSparse2to4LowRankFunction backward ä¸­çš„å…¶ä»–å…³é”®å‡½æ•°

#### compute_split_gemm_dw_nocopy å‡½æ•°

```python
def compute_split_gemm_dw_nocopy(activation, grad_output, layer_id, transpose_result=False):
    """
    è®¡ç®—æƒé‡æ¢¯åº¦ï¼Œä½¿ç”¨é›¶æ‹·è´çš„split-GEMM
    è®¡ç®—: activation.T @ grad_output
    """
    from fused_sparsity_ops import sparsity_tracker
    
    # Line 274: è·å–ç¼“å­˜çš„ç¨€ç–æ€§
    col_sparsity, sparse_mask = sparsity_tracker.get_sparsity(layer_id)
    
    if sparse_mask is None:
        # æ²¡æœ‰ç¨€ç–æ€§ä¿¡æ¯ï¼Œä½¿ç”¨æ ‡å‡†matmul
        result = torch.mm(activation.T, grad_output)
        return result.T if transpose_result else result
    
    # Lines 282-287: è·å–ç»´åº¦å¹¶åˆå§‹åŒ–æ¢¯åº¦
    batch_size, in_features = activation.shape
    batch_size2, out_features = grad_output.shape
    assert batch_size == batch_size2, "Batch size mismatch"
    
    grad_weight = torch.zeros(in_features, out_features, 
                             device=activation.device, 
                             dtype=activation.dtype)
    
    dense_mask = ~sparse_mask
    
    # Lines 292-304: ç¨€ç–éƒ¨åˆ†å¤„ç†
    if sparse_mask is not None and sparse_mask.any():
        # æå–ç¨€ç–åˆ—
        activation_sparse = activation[:, sparse_mask]
        # åº”ç”¨2:4ç¨€ç–åŒ–
        activation_sparse_2to4 = apply_feature_wise_2to4(activation_sparse)
        # ä½¿ç”¨ç¨€ç–çŸ©é˜µä¹˜æ³•
        result = fake_fp8_mm(
            activation_sparse_2to4.T, 
            grad_output, 
            torch.float8_e4m3fn
        )
        # å°†ç»“æœæ”¾å›å¯¹åº”ä½ç½®
        grad_weight[sparse_mask, :] = result.to(grad_weight.dtype)
    
    # Lines 307-309: å¯†é›†éƒ¨åˆ†å¤„ç†
    if dense_mask.any():
        activation_dense = activation[:, dense_mask]
        grad_weight[dense_mask, :] = torch.mm(activation_dense.T, grad_output)
    
    # Line 312: å¯é€‰è½¬ç½®
    return grad_weight.T if transpose_result else grad_weight
```

#### apply_split_gemm_to_dy1_nocopy å‡½æ•°

```python
def apply_split_gemm_to_dy1_nocopy(dy1, layer_id):
    """
    å¯¹dy1åº”ç”¨split-GEMMç¨€ç–åŒ–
    """
    from fused_sparsity_ops import sparsity_tracker
    
    # Line 252: è·å–ç¼“å­˜çš„ç¨€ç–æ€§
    col_sparsity, sparse_mask = sparsity_tracker.get_sparsity(layer_id)
    
    # Line 255: è°ƒç”¨ç¨€ç–åŒ–å‡½æ•°
    return apply_split_gemm_sparsity_nocopy(dy1, sparse_mask)
```

## æ½œåœ¨é—®é¢˜åˆ†æ

åŸºäºè¯¦ç»†çš„ä»£ç åˆ†æï¼Œä»¥ä¸‹æ˜¯æœ€å¯èƒ½å¯¼è‡´NaNçš„é—®é¢˜ï¼š

### 1. ğŸ”´ compute_split_gemm_dw2_lowrankæœªå®ç°Split-GEMMï¼ˆä¸¥é‡ï¼‰
- **ä½ç½®**: modeling_llama.py:94
- **é—®é¢˜**: å‡½æ•°è·å–äº†ç¨€ç–æ€§ä¿¡æ¯ä½†æœªä½¿ç”¨ï¼Œåªåšæ ‡å‡†çŸ©é˜µä¹˜æ³•
- **å½±å“**: æœªèƒ½åˆ©ç”¨ç¨€ç–æ€§åŠ é€Ÿï¼Œæ¢¯åº¦è®¡ç®—å¯èƒ½ä¸ä¸€è‡´
- **NaNé£é™©**: é«˜ - æ¢¯åº¦ä¸ä¸€è‡´å¯èƒ½å¯¼è‡´ä¼˜åŒ–å™¨çŠ¶æ€å¼‚å¸¸
- **ä¿®å¤å»ºè®®**: å®ç°çœŸæ­£çš„split-GEMMé€»è¾‘

### 2. ğŸ”´ split_gemm_2to4_kernelè¾¹ç•Œæ£€æŸ¥ä¸å®Œæ•´ï¼ˆä¸¥é‡ï¼‰
- **ä½ç½®**: triton_split_gemm_nocopy.py:52
- **é—®é¢˜**: `if m3 < M and m0 >= 0`æ£€æŸ¥ä¸å……åˆ†
  - `m0 >= 0`æ£€æŸ¥å†—ä½™ï¼ˆm0æ€»æ˜¯éè´Ÿï¼‰
  - æœªæ£€æŸ¥m1, m2æ˜¯å¦åœ¨èŒƒå›´å†…
- **NaNé£é™©**: é«˜ - å¯èƒ½è®¿é—®è¶Šç•Œå†…å­˜ï¼Œè¯»å–åƒåœ¾å€¼
- **ä¿®å¤å»ºè®®**: 
```python
# å½“å‰æœ‰é—®é¢˜çš„æ£€æŸ¥
if m3 < M and m0 >= 0:  

# åº”è¯¥æ”¹ä¸º
if m0 < M and m1 < M and m2 < M and m3 < M:
```

### 3. ğŸŸ¡ Backwardä¸­dxè®¡ç®—æœªä½¿ç”¨æ­£ç¡®çš„Split-GEMMï¼ˆä¸­ç­‰ï¼‰
- **ä½ç½®**: modeling_llama.py:552-554
- **é—®é¢˜**: æ³¨é‡Šæ‰äº†æ­£ç¡®çš„split-GEMMå®ç°ï¼Œä½¿ç”¨naiveæ–¹æ³•
- **å½±å“**: æœªå……åˆ†åˆ©ç”¨å‰å‘ä¼ æ’­ç¼“å­˜çš„ç¨€ç–æ€§ä¿¡æ¯
- **ä¿®å¤**: å¯ç”¨æ³¨é‡Šæ‰çš„ä»£ç 

### 4. ğŸŸ¡ æ•°æ®ç±»å‹è½¬æ¢é¢‘ç¹ï¼ˆä¸­ç­‰ï¼‰
- **å¤šå¤„ä½ç½®**: ä¾‹å¦‚Lines 494, 576ç­‰
- **é£é™©**: float16/bfloat16ç²¾åº¦æŸå¤±å¯èƒ½ç´¯ç§¯
- **å»ºè®®**: 
  - ç»Ÿä¸€ä½¿ç”¨float32è¿›è¡Œå…³é”®è®¡ç®—
  - å‡å°‘ä¸å¿…è¦çš„dtypeè½¬æ¢

### 5. ğŸŸ¡ fake_fp8_mmä½¿ç”¨ä¸å½“ï¼ˆä¸­ç­‰ï¼‰
- **ä½ç½®**: å¤šå¤„backwardè®¡ç®—
- **é—®é¢˜**: fake_fp8_mmæœŸæœ›å®Œå…¨ç¨€ç–çŸ©é˜µï¼Œä½†split-GEMMäº§ç”Ÿæ··åˆç¨€ç–/å¯†é›†çŸ©é˜µ
- **é£é™©**: å¯èƒ½å¯¼è‡´è®¡ç®—é”™è¯¯
- **å»ºè®®**: ç¡®è®¤fake_fp8_mmçš„è¾“å…¥è¦æ±‚

### 6. ğŸ”µ ç¨€ç–æ€§ç¼“å­˜ä¸€è‡´æ€§ï¼ˆä½é£é™©ä½†é‡è¦ï¼‰
- **é—®é¢˜**: layer_idç”Ÿæˆå¿…é¡»åœ¨forwardå’Œbackwardä¸­å®Œå…¨ä¸€è‡´
- **é£é™©**: å¦‚æœIDä¸åŒ¹é…ï¼Œbackwardæ‰¾ä¸åˆ°ç¼“å­˜çš„ç¨€ç–æ€§ä¿¡æ¯
- **è°ƒè¯•æ–¹æ³•**: æ·»åŠ æ—¥å¿—éªŒè¯layer_idä¸€è‡´æ€§

### å…³é”®é—®é¢˜æ€»ç»“

æ ¹æ®è¯¦ç»†çš„ä»£ç åˆ†æï¼Œä»¥ä¸‹é—®é¢˜æœ€å¯èƒ½å¯¼è‡´NaNï¼š

1. **split_gemm_2to4_kernelçš„è¾¹ç•Œæ£€æŸ¥é—®é¢˜**ï¼ˆæœ€ä¸¥é‡ï¼‰
   - ä½ç½®ï¼štriton_split_gemm_nocopy.py:52
   - åªæ£€æŸ¥m3 < Mï¼Œä½†m0, m1, m2å¯èƒ½è¶Šç•Œ
   - å¯èƒ½è®¿é—®æ— æ•ˆå†…å­˜ï¼Œè¯»å–åƒåœ¾å€¼å¯¼è‡´NaN

2. **compute_split_gemm_dw2_lowrankæœªå®ç°split-GEMM**
   - ä½ç½®ï¼špeft_pretraining/modeling_llama.py:94
   - è·å–äº†ç¨€ç–æ€§ä¿¡æ¯ä½†æœªä½¿ç”¨
   - å¯èƒ½å¯¼è‡´æ¢¯åº¦è®¡ç®—ä¸ä¸€è‡´

3. **layer_idä¸åŒ¹é…é—®é¢˜**
   - forwardå’Œbackwardä¸­çš„layer_idç”Ÿæˆé€»è¾‘å¿…é¡»å®Œå…¨ä¸€è‡´
   - å¦‚æœä¸åŒ¹é…ï¼Œbackwardæ‰¾ä¸åˆ°ç¼“å­˜çš„ç¨€ç–æ€§ä¿¡æ¯

### è°ƒè¯•å»ºè®®

1. **æ·»åŠ NaNæ£€æŸ¥ç‚¹**ï¼š
```python
def check_nan(tensor, name):
    if torch.isnan(tensor).any():
        print(f"NaN detected in {name}")
        print(f"Shape: {tensor.shape}, dtype: {tensor.dtype}")
        raise RuntimeError(f"NaN in {name}")
    return tensor

# åœ¨å…³é”®ä½ç½®æ·»åŠ 
y2 = check_nan(y2, "y2 after ReLU2")
intermediate_2 = check_nan(intermediate_2, "intermediate_2")
```

2. **ä¿®å¤è¾¹ç•Œæ£€æŸ¥**ï¼ˆæœ€ç´§æ€¥ï¼‰
3. **å®ç°æ­£ç¡®çš„compute_split_gemm_dw2_lowrank**
4. **éªŒè¯ç¨€ç–æ€§ç¼“å­˜**ï¼š
```python
print(f"Forward layer_id: {layer_id}")
print(f"Cached sparsity available: {sparse_mask is not None}")
```

5. **è€ƒè™‘æš‚æ—¶ä½¿ç”¨float32è¿›è¡Œè°ƒè¯•**

è¿™äº›é—®é¢˜ä¸­ï¼Œè¾¹ç•Œæ£€æŸ¥å’Œcompute_split_gemm_dw2_lowrankçš„é—®é¢˜æœ€å¯èƒ½ç›´æ¥å¯¼è‡´NaNã€‚å»ºè®®ä¼˜å…ˆä¿®å¤è¿™ä¸¤ä¸ªé—®é¢˜ã€‚

